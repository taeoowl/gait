{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dc5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error # mse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from dataloader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5a4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        _, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        _, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "    \n",
    "        inputs_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        out_lstm = self.reg_module1(inputs_concat)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83facf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "dataset = Gait_Dataset_Salted(file_path)\n",
    "val_percent = 0.2\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e009bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8209fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bbbae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/1000, Train Loss : 17843.514242, Valid Loss 17913.157878\n",
      "Epoch : 2/1000, Train Loss : 17781.104004, Valid Loss 17854.750651\n",
      "Epoch : 3/1000, Train Loss : 17651.265869, Valid Loss 17553.068685\n",
      "Epoch : 4/1000, Train Loss : 17006.748820, Valid Loss 16404.546224\n",
      "Epoch : 5/1000, Train Loss : 15074.377401, Valid Loss 13396.409505\n",
      "Epoch : 6/1000, Train Loss : 10882.086792, Valid Loss 7934.862386\n",
      "Epoch : 7/1000, Train Loss : 4946.141968, Valid Loss 2148.812703\n",
      "Epoch : 8/1000, Train Loss : 883.482466, Valid Loss 292.360769\n",
      "Epoch : 9/1000, Train Loss : 319.906573, Valid Loss 296.552882\n",
      "Epoch : 10/1000, Train Loss : 311.939814, Valid Loss 288.987289\n",
      "Epoch : 11/1000, Train Loss : 308.403503, Valid Loss 288.523125\n",
      "Epoch : 12/1000, Train Loss : 308.735537, Valid Loss 288.258649\n",
      "Epoch : 13/1000, Train Loss : 307.852030, Valid Loss 288.429888\n",
      "Epoch : 14/1000, Train Loss : 307.999543, Valid Loss 288.560135\n",
      "Epoch : 15/1000, Train Loss : 308.453004, Valid Loss 288.235163\n",
      "Epoch : 16/1000, Train Loss : 307.261364, Valid Loss 288.770933\n",
      "Epoch : 17/1000, Train Loss : 308.200371, Valid Loss 288.277237\n",
      "Epoch : 18/1000, Train Loss : 308.679255, Valid Loss 288.336118\n",
      "Epoch : 19/1000, Train Loss : 308.590227, Valid Loss 288.459188\n",
      "Epoch : 20/1000, Train Loss : 307.582924, Valid Loss 288.257014\n",
      "Epoch : 21/1000, Train Loss : 308.130633, Valid Loss 286.342333\n",
      "Epoch : 22/1000, Train Loss : 305.869092, Valid Loss 285.689659\n",
      "Epoch : 23/1000, Train Loss : 302.321045, Valid Loss 277.681783\n",
      "Epoch : 24/1000, Train Loss : 292.682086, Valid Loss 264.459178\n",
      "Epoch : 25/1000, Train Loss : 277.694619, Valid Loss 243.918454\n",
      "Epoch : 26/1000, Train Loss : 255.185553, Valid Loss 224.619560\n",
      "Epoch : 27/1000, Train Loss : 237.612009, Valid Loss 207.424805\n",
      "Epoch : 28/1000, Train Loss : 217.431426, Valid Loss 189.051397\n",
      "Epoch : 29/1000, Train Loss : 198.106850, Valid Loss 173.571505\n",
      "Epoch : 30/1000, Train Loss : 180.266414, Valid Loss 158.613838\n",
      "Epoch : 31/1000, Train Loss : 164.500933, Valid Loss 147.998093\n",
      "Epoch : 32/1000, Train Loss : 147.434918, Valid Loss 132.280121\n",
      "Epoch : 33/1000, Train Loss : 129.772780, Valid Loss 118.043727\n",
      "Epoch : 34/1000, Train Loss : 116.864358, Valid Loss 106.990368\n",
      "Epoch : 35/1000, Train Loss : 105.028154, Valid Loss 95.917783\n",
      "Epoch : 36/1000, Train Loss : 96.696970, Valid Loss 85.990768\n",
      "Epoch : 37/1000, Train Loss : 84.278853, Valid Loss 76.557254\n",
      "Epoch : 38/1000, Train Loss : 78.136450, Valid Loss 70.748987\n",
      "Epoch : 39/1000, Train Loss : 68.434832, Valid Loss 61.808634\n",
      "Epoch : 40/1000, Train Loss : 65.429411, Valid Loss 60.602514\n",
      "Epoch : 41/1000, Train Loss : 64.564807, Valid Loss 66.452574\n",
      "Epoch : 42/1000, Train Loss : 57.840191, Valid Loss 51.000632\n",
      "Epoch : 43/1000, Train Loss : 53.084057, Valid Loss 48.341124\n",
      "Epoch : 44/1000, Train Loss : 50.040562, Valid Loss 47.590830\n",
      "Epoch : 45/1000, Train Loss : 48.822088, Valid Loss 43.916021\n",
      "Epoch : 46/1000, Train Loss : 50.043172, Valid Loss 47.336099\n",
      "Epoch : 47/1000, Train Loss : 45.639516, Valid Loss 45.607934\n",
      "Epoch : 48/1000, Train Loss : 43.511014, Valid Loss 40.883346\n",
      "Epoch : 49/1000, Train Loss : 42.151493, Valid Loss 39.993495\n",
      "Epoch : 50/1000, Train Loss : 40.115810, Valid Loss 36.621509\n",
      "Epoch : 51/1000, Train Loss : 38.794347, Valid Loss 36.521660\n",
      "Epoch : 52/1000, Train Loss : 37.812372, Valid Loss 33.675094\n",
      "Epoch : 53/1000, Train Loss : 37.371310, Valid Loss 34.544440\n",
      "Epoch : 54/1000, Train Loss : 36.633053, Valid Loss 35.067569\n",
      "Epoch : 55/1000, Train Loss : 35.842364, Valid Loss 32.233124\n",
      "Epoch : 56/1000, Train Loss : 34.415571, Valid Loss 32.972446\n",
      "Epoch : 57/1000, Train Loss : 34.140608, Valid Loss 29.899604\n",
      "Epoch : 58/1000, Train Loss : 34.646745, Valid Loss 29.854607\n",
      "Epoch : 59/1000, Train Loss : 32.333131, Valid Loss 30.050598\n",
      "Epoch : 60/1000, Train Loss : 34.622204, Valid Loss 32.761988\n",
      "Epoch : 61/1000, Train Loss : 33.485426, Valid Loss 27.678962\n",
      "Epoch : 62/1000, Train Loss : 31.504400, Valid Loss 28.165390\n",
      "Epoch : 63/1000, Train Loss : 31.306825, Valid Loss 29.490173\n",
      "Epoch : 64/1000, Train Loss : 30.822862, Valid Loss 26.295947\n",
      "Epoch : 65/1000, Train Loss : 29.708347, Valid Loss 32.817964\n",
      "Epoch : 66/1000, Train Loss : 31.038166, Valid Loss 26.398112\n",
      "Epoch : 67/1000, Train Loss : 28.249908, Valid Loss 25.141088\n",
      "Epoch : 68/1000, Train Loss : 28.504787, Valid Loss 29.717306\n",
      "Epoch : 69/1000, Train Loss : 28.294072, Valid Loss 25.243572\n",
      "Epoch : 70/1000, Train Loss : 28.515166, Valid Loss 25.181201\n",
      "Epoch : 71/1000, Train Loss : 28.858172, Valid Loss 24.311253\n",
      "Epoch : 72/1000, Train Loss : 27.542558, Valid Loss 24.332544\n",
      "Epoch : 73/1000, Train Loss : 27.427003, Valid Loss 27.866123\n",
      "Epoch : 74/1000, Train Loss : 28.086558, Valid Loss 25.023280\n",
      "Epoch : 75/1000, Train Loss : 26.563338, Valid Loss 22.693613\n",
      "Epoch : 76/1000, Train Loss : 27.845648, Valid Loss 26.646356\n",
      "Epoch : 77/1000, Train Loss : 25.893018, Valid Loss 22.655527\n",
      "Epoch : 78/1000, Train Loss : 25.094994, Valid Loss 22.272407\n",
      "Epoch : 79/1000, Train Loss : 25.853182, Valid Loss 21.720087\n",
      "Epoch : 80/1000, Train Loss : 25.085400, Valid Loss 21.932677\n",
      "Epoch : 81/1000, Train Loss : 24.771824, Valid Loss 22.939910\n",
      "Epoch : 82/1000, Train Loss : 25.408765, Valid Loss 29.155055\n",
      "Epoch : 83/1000, Train Loss : 25.653440, Valid Loss 22.747577\n",
      "Epoch : 84/1000, Train Loss : 25.152513, Valid Loss 24.370082\n",
      "Epoch : 85/1000, Train Loss : 25.099023, Valid Loss 20.334743\n",
      "Epoch : 86/1000, Train Loss : 23.485633, Valid Loss 20.901164\n",
      "Epoch : 87/1000, Train Loss : 24.836944, Valid Loss 21.054389\n",
      "Epoch : 88/1000, Train Loss : 23.148195, Valid Loss 20.071027\n",
      "Epoch : 89/1000, Train Loss : 23.671365, Valid Loss 19.657749\n",
      "Epoch : 90/1000, Train Loss : 23.354452, Valid Loss 19.834956\n",
      "Epoch : 91/1000, Train Loss : 22.717995, Valid Loss 19.382779\n",
      "Epoch : 92/1000, Train Loss : 23.119852, Valid Loss 19.663121\n",
      "Epoch : 93/1000, Train Loss : 22.901944, Valid Loss 20.445313\n",
      "Epoch : 94/1000, Train Loss : 22.220571, Valid Loss 19.005280\n",
      "Epoch : 95/1000, Train Loss : 22.695912, Valid Loss 21.541999\n",
      "Epoch : 96/1000, Train Loss : 22.657689, Valid Loss 19.944343\n",
      "Epoch : 97/1000, Train Loss : 22.424244, Valid Loss 19.335995\n",
      "Epoch : 98/1000, Train Loss : 21.470868, Valid Loss 18.980736\n",
      "Epoch : 99/1000, Train Loss : 21.523084, Valid Loss 18.383335\n",
      "Epoch : 100/1000, Train Loss : 22.528361, Valid Loss 19.671696\n",
      "Epoch : 101/1000, Train Loss : 20.951099, Valid Loss 18.463417\n",
      "Epoch : 102/1000, Train Loss : 20.948848, Valid Loss 19.105022\n",
      "Epoch : 103/1000, Train Loss : 21.504065, Valid Loss 26.761004\n",
      "Epoch : 104/1000, Train Loss : 22.422228, Valid Loss 18.794472\n",
      "Epoch : 105/1000, Train Loss : 22.705292, Valid Loss 32.704083\n",
      "Epoch : 106/1000, Train Loss : 24.666452, Valid Loss 18.108572\n",
      "Epoch : 107/1000, Train Loss : 21.392289, Valid Loss 19.118303\n",
      "Epoch : 108/1000, Train Loss : 20.404023, Valid Loss 19.292109\n",
      "Epoch : 109/1000, Train Loss : 20.560968, Valid Loss 19.842497\n",
      "Epoch : 110/1000, Train Loss : 20.577387, Valid Loss 17.794527\n",
      "Epoch : 111/1000, Train Loss : 20.164251, Valid Loss 18.740468\n",
      "Epoch : 112/1000, Train Loss : 20.094865, Valid Loss 17.881794\n",
      "Epoch : 113/1000, Train Loss : 21.298650, Valid Loss 17.975980\n",
      "Epoch : 114/1000, Train Loss : 19.903093, Valid Loss 16.930526\n",
      "Epoch : 115/1000, Train Loss : 19.590301, Valid Loss 17.642852\n",
      "Epoch : 116/1000, Train Loss : 19.358356, Valid Loss 19.320543\n",
      "Epoch : 117/1000, Train Loss : 19.614848, Valid Loss 19.075205\n",
      "Epoch : 118/1000, Train Loss : 22.619106, Valid Loss 16.915071\n",
      "Epoch : 119/1000, Train Loss : 20.176005, Valid Loss 16.745519\n",
      "Epoch : 120/1000, Train Loss : 20.143759, Valid Loss 16.774089\n",
      "Epoch : 121/1000, Train Loss : 20.367036, Valid Loss 24.061598\n",
      "Epoch : 122/1000, Train Loss : 21.085528, Valid Loss 22.516737\n",
      "Epoch : 123/1000, Train Loss : 20.112171, Valid Loss 17.118930\n",
      "Epoch : 124/1000, Train Loss : 18.366126, Valid Loss 16.701179\n",
      "Epoch : 125/1000, Train Loss : 18.204817, Valid Loss 16.377702\n",
      "Epoch : 126/1000, Train Loss : 17.960950, Valid Loss 16.390732\n",
      "Epoch : 127/1000, Train Loss : 18.756424, Valid Loss 16.089848\n",
      "Epoch : 128/1000, Train Loss : 18.371270, Valid Loss 17.049444\n",
      "Epoch : 129/1000, Train Loss : 18.637961, Valid Loss 16.111328\n",
      "Epoch : 130/1000, Train Loss : 18.053066, Valid Loss 16.417494\n",
      "Epoch : 131/1000, Train Loss : 17.975486, Valid Loss 15.740292\n",
      "Epoch : 132/1000, Train Loss : 17.621386, Valid Loss 15.982948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 133/1000, Train Loss : 18.365688, Valid Loss 17.241023\n",
      "Epoch : 134/1000, Train Loss : 17.608108, Valid Loss 17.924470\n",
      "Epoch : 135/1000, Train Loss : 18.295034, Valid Loss 19.855055\n",
      "Epoch : 136/1000, Train Loss : 17.951356, Valid Loss 15.586173\n",
      "Epoch : 137/1000, Train Loss : 17.452861, Valid Loss 17.763008\n",
      "Epoch : 138/1000, Train Loss : 19.200506, Valid Loss 16.247705\n",
      "Epoch : 139/1000, Train Loss : 17.878322, Valid Loss 15.831816\n",
      "Epoch : 140/1000, Train Loss : 16.838532, Valid Loss 15.987575\n",
      "Epoch : 141/1000, Train Loss : 16.967364, Valid Loss 16.042442\n",
      "Epoch : 142/1000, Train Loss : 17.753614, Valid Loss 15.596292\n",
      "Epoch : 143/1000, Train Loss : 17.448234, Valid Loss 16.144645\n",
      "Epoch : 144/1000, Train Loss : 16.891270, Valid Loss 14.996863\n",
      "Epoch : 145/1000, Train Loss : 17.288381, Valid Loss 16.601041\n",
      "Epoch : 146/1000, Train Loss : 16.908607, Valid Loss 15.533703\n",
      "Epoch : 147/1000, Train Loss : 17.599434, Valid Loss 24.289843\n",
      "Epoch : 148/1000, Train Loss : 18.575021, Valid Loss 15.491556\n",
      "Epoch : 149/1000, Train Loss : 17.556290, Valid Loss 15.209496\n",
      "Epoch : 150/1000, Train Loss : 17.282867, Valid Loss 15.042624\n",
      "Epoch : 151/1000, Train Loss : 17.163201, Valid Loss 14.938390\n",
      "Epoch : 152/1000, Train Loss : 16.056690, Valid Loss 15.921424\n",
      "Epoch : 153/1000, Train Loss : 16.248588, Valid Loss 17.368229\n",
      "Epoch : 154/1000, Train Loss : 16.739692, Valid Loss 16.407121\n",
      "Epoch : 155/1000, Train Loss : 16.339669, Valid Loss 16.217168\n",
      "Epoch : 156/1000, Train Loss : 18.566249, Valid Loss 15.282604\n",
      "Epoch : 157/1000, Train Loss : 15.854729, Valid Loss 16.141882\n",
      "Epoch : 158/1000, Train Loss : 17.764762, Valid Loss 16.552591\n",
      "Epoch : 159/1000, Train Loss : 16.563327, Valid Loss 16.171592\n",
      "Epoch : 160/1000, Train Loss : 16.954766, Valid Loss 15.186687\n",
      "Epoch : 161/1000, Train Loss : 15.656585, Valid Loss 14.379762\n",
      "Epoch : 162/1000, Train Loss : 15.767786, Valid Loss 14.973719\n",
      "Epoch : 163/1000, Train Loss : 16.119216, Valid Loss 15.717718\n",
      "Epoch : 164/1000, Train Loss : 16.577718, Valid Loss 14.522161\n",
      "Epoch : 165/1000, Train Loss : 15.705806, Valid Loss 14.481647\n",
      "Epoch : 166/1000, Train Loss : 15.658679, Valid Loss 14.441790\n",
      "Epoch : 167/1000, Train Loss : 17.092854, Valid Loss 14.298916\n",
      "Epoch : 168/1000, Train Loss : 16.624391, Valid Loss 16.316360\n",
      "Epoch : 169/1000, Train Loss : 16.671636, Valid Loss 14.270343\n",
      "Epoch : 170/1000, Train Loss : 15.068913, Valid Loss 16.087000\n",
      "Epoch : 171/1000, Train Loss : 15.600141, Valid Loss 15.053700\n",
      "Epoch : 172/1000, Train Loss : 16.134728, Valid Loss 15.131095\n",
      "Epoch : 173/1000, Train Loss : 15.178340, Valid Loss 14.377809\n",
      "Epoch : 174/1000, Train Loss : 15.064808, Valid Loss 14.139671\n",
      "Epoch : 175/1000, Train Loss : 14.973925, Valid Loss 16.673121\n",
      "Epoch : 176/1000, Train Loss : 15.035401, Valid Loss 13.964984\n",
      "Epoch : 177/1000, Train Loss : 14.855961, Valid Loss 13.936106\n",
      "Epoch : 178/1000, Train Loss : 15.380667, Valid Loss 14.068524\n",
      "Epoch : 179/1000, Train Loss : 14.416104, Valid Loss 14.176821\n",
      "Epoch : 180/1000, Train Loss : 14.587479, Valid Loss 14.211168\n",
      "Epoch : 181/1000, Train Loss : 17.627227, Valid Loss 14.739815\n",
      "Epoch : 182/1000, Train Loss : 16.545564, Valid Loss 14.366480\n",
      "Epoch : 183/1000, Train Loss : 14.860470, Valid Loss 15.088983\n",
      "Epoch : 184/1000, Train Loss : 15.481383, Valid Loss 15.267159\n",
      "Epoch : 185/1000, Train Loss : 16.534404, Valid Loss 13.980600\n",
      "Epoch : 186/1000, Train Loss : 15.026344, Valid Loss 14.387915\n",
      "Epoch : 187/1000, Train Loss : 15.037091, Valid Loss 13.829873\n",
      "Epoch : 188/1000, Train Loss : 14.294355, Valid Loss 15.197784\n",
      "Epoch : 189/1000, Train Loss : 15.527529, Valid Loss 13.456767\n",
      "Epoch : 190/1000, Train Loss : 14.633022, Valid Loss 14.583751\n",
      "Epoch : 191/1000, Train Loss : 14.687393, Valid Loss 16.382203\n",
      "Epoch : 192/1000, Train Loss : 14.941549, Valid Loss 15.133196\n",
      "Epoch : 193/1000, Train Loss : 15.904267, Valid Loss 13.572936\n",
      "Epoch : 194/1000, Train Loss : 17.461550, Valid Loss 15.588743\n",
      "Epoch : 195/1000, Train Loss : 14.458968, Valid Loss 13.431210\n",
      "Epoch : 196/1000, Train Loss : 15.609234, Valid Loss 14.668965\n",
      "Epoch : 197/1000, Train Loss : 14.686551, Valid Loss 13.400716\n",
      "Epoch : 198/1000, Train Loss : 14.115045, Valid Loss 14.065347\n",
      "Epoch : 199/1000, Train Loss : 13.629204, Valid Loss 13.627306\n",
      "Epoch : 200/1000, Train Loss : 13.964131, Valid Loss 13.156524\n",
      "Epoch : 201/1000, Train Loss : 13.751608, Valid Loss 14.292572\n",
      "Epoch : 202/1000, Train Loss : 14.066326, Valid Loss 13.848471\n",
      "Epoch : 203/1000, Train Loss : 13.827464, Valid Loss 13.332922\n",
      "Epoch : 204/1000, Train Loss : 15.256511, Valid Loss 13.130330\n",
      "Epoch : 205/1000, Train Loss : 13.859767, Valid Loss 13.086405\n",
      "Epoch : 206/1000, Train Loss : 13.882302, Valid Loss 13.549439\n",
      "Epoch : 207/1000, Train Loss : 13.642694, Valid Loss 13.948467\n",
      "Epoch : 208/1000, Train Loss : 13.445591, Valid Loss 14.421422\n",
      "Epoch : 209/1000, Train Loss : 15.196489, Valid Loss 12.974798\n",
      "Epoch : 210/1000, Train Loss : 14.967162, Valid Loss 13.119376\n",
      "Epoch : 211/1000, Train Loss : 16.073719, Valid Loss 14.735324\n",
      "Epoch : 212/1000, Train Loss : 14.449489, Valid Loss 13.489377\n",
      "Epoch : 213/1000, Train Loss : 13.017044, Valid Loss 13.070783\n",
      "Epoch : 214/1000, Train Loss : 14.019276, Valid Loss 13.205770\n",
      "Epoch : 215/1000, Train Loss : 13.221284, Valid Loss 13.331748\n",
      "Epoch : 216/1000, Train Loss : 13.457563, Valid Loss 17.322408\n",
      "Epoch : 217/1000, Train Loss : 14.524365, Valid Loss 13.308187\n",
      "Epoch : 218/1000, Train Loss : 13.565573, Valid Loss 13.588014\n",
      "Epoch : 219/1000, Train Loss : 12.902512, Valid Loss 13.556373\n",
      "Epoch : 220/1000, Train Loss : 13.539362, Valid Loss 12.761940\n",
      "Epoch : 221/1000, Train Loss : 12.970653, Valid Loss 14.395831\n",
      "Epoch : 222/1000, Train Loss : 13.023839, Valid Loss 12.932933\n",
      "Epoch : 223/1000, Train Loss : 13.188772, Valid Loss 13.813952\n",
      "Epoch : 224/1000, Train Loss : 13.071688, Valid Loss 12.367921\n",
      "Epoch : 225/1000, Train Loss : 12.670834, Valid Loss 12.886877\n",
      "Epoch : 226/1000, Train Loss : 13.450743, Valid Loss 15.559809\n",
      "Epoch : 227/1000, Train Loss : 13.530481, Valid Loss 12.539884\n",
      "Epoch : 228/1000, Train Loss : 12.406240, Valid Loss 12.743496\n",
      "Epoch : 229/1000, Train Loss : 13.157401, Valid Loss 14.332305\n",
      "Epoch : 230/1000, Train Loss : 13.600987, Valid Loss 13.191121\n",
      "Epoch : 231/1000, Train Loss : 13.788889, Valid Loss 12.362151\n",
      "Epoch : 232/1000, Train Loss : 12.772491, Valid Loss 12.443885\n",
      "Epoch : 233/1000, Train Loss : 12.148200, Valid Loss 16.939752\n",
      "Epoch : 234/1000, Train Loss : 13.892772, Valid Loss 12.695276\n",
      "Epoch : 235/1000, Train Loss : 12.430450, Valid Loss 15.330488\n",
      "Epoch : 236/1000, Train Loss : 13.103355, Valid Loss 11.940355\n",
      "Epoch : 237/1000, Train Loss : 12.744861, Valid Loss 12.733131\n",
      "Epoch : 238/1000, Train Loss : 12.098046, Valid Loss 13.304084\n",
      "Epoch : 239/1000, Train Loss : 12.565413, Valid Loss 15.948782\n",
      "Epoch : 240/1000, Train Loss : 13.962200, Valid Loss 13.106096\n",
      "Epoch : 241/1000, Train Loss : 12.435018, Valid Loss 13.171641\n",
      "Epoch : 242/1000, Train Loss : 12.411205, Valid Loss 12.467111\n",
      "Epoch : 243/1000, Train Loss : 11.591081, Valid Loss 12.895913\n",
      "Epoch : 244/1000, Train Loss : 12.165156, Valid Loss 12.255777\n",
      "Epoch : 245/1000, Train Loss : 12.203776, Valid Loss 11.952047\n",
      "Epoch : 246/1000, Train Loss : 12.742897, Valid Loss 13.184075\n",
      "Epoch : 247/1000, Train Loss : 11.917947, Valid Loss 12.487648\n",
      "Epoch : 248/1000, Train Loss : 12.080364, Valid Loss 14.040494\n",
      "Epoch : 249/1000, Train Loss : 12.833808, Valid Loss 14.164436\n",
      "Epoch : 250/1000, Train Loss : 12.758250, Valid Loss 11.970007\n",
      "Epoch : 251/1000, Train Loss : 12.201308, Valid Loss 12.521079\n",
      "Epoch : 252/1000, Train Loss : 11.735062, Valid Loss 12.147226\n",
      "Epoch : 253/1000, Train Loss : 13.101164, Valid Loss 11.803128\n",
      "Epoch : 254/1000, Train Loss : 12.309267, Valid Loss 12.666415\n",
      "Epoch : 255/1000, Train Loss : 12.524131, Valid Loss 12.042674\n",
      "Epoch : 256/1000, Train Loss : 11.366735, Valid Loss 12.623227\n",
      "Epoch : 257/1000, Train Loss : 11.855225, Valid Loss 12.266684\n",
      "Epoch : 258/1000, Train Loss : 11.511764, Valid Loss 11.821356\n",
      "Epoch : 259/1000, Train Loss : 11.586391, Valid Loss 11.543710\n",
      "Epoch : 260/1000, Train Loss : 11.846100, Valid Loss 11.885037\n",
      "Epoch : 261/1000, Train Loss : 11.779304, Valid Loss 11.500313\n",
      "Epoch : 262/1000, Train Loss : 11.266904, Valid Loss 11.748309\n",
      "Epoch : 263/1000, Train Loss : 11.310546, Valid Loss 12.213874\n",
      "Epoch : 264/1000, Train Loss : 11.878917, Valid Loss 13.444628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 265/1000, Train Loss : 13.743089, Valid Loss 17.239477\n",
      "Epoch : 266/1000, Train Loss : 13.222885, Valid Loss 14.431031\n",
      "Epoch : 267/1000, Train Loss : 11.328628, Valid Loss 14.229660\n",
      "Epoch : 268/1000, Train Loss : 11.822112, Valid Loss 12.475748\n",
      "Epoch : 269/1000, Train Loss : 11.229802, Valid Loss 16.664233\n",
      "Epoch : 270/1000, Train Loss : 12.396696, Valid Loss 13.034094\n",
      "Epoch : 271/1000, Train Loss : 12.053738, Valid Loss 11.507158\n",
      "Epoch : 272/1000, Train Loss : 11.142594, Valid Loss 11.810723\n",
      "Epoch : 273/1000, Train Loss : 11.681785, Valid Loss 15.219431\n",
      "Epoch : 274/1000, Train Loss : 13.004852, Valid Loss 12.658012\n",
      "Epoch : 275/1000, Train Loss : 11.152963, Valid Loss 11.470185\n",
      "Epoch : 276/1000, Train Loss : 11.253933, Valid Loss 11.685269\n",
      "Epoch : 277/1000, Train Loss : 11.782667, Valid Loss 11.695464\n",
      "Epoch : 278/1000, Train Loss : 12.026556, Valid Loss 15.063349\n",
      "Epoch : 279/1000, Train Loss : 11.187093, Valid Loss 11.255485\n",
      "Epoch : 280/1000, Train Loss : 10.852469, Valid Loss 12.367218\n",
      "Epoch : 281/1000, Train Loss : 10.501032, Valid Loss 12.357975\n",
      "Epoch : 282/1000, Train Loss : 11.118969, Valid Loss 11.686699\n",
      "Epoch : 283/1000, Train Loss : 11.493293, Valid Loss 11.551944\n",
      "Epoch : 284/1000, Train Loss : 11.284271, Valid Loss 11.990373\n",
      "Epoch : 285/1000, Train Loss : 11.745943, Valid Loss 11.219385\n",
      "Epoch : 286/1000, Train Loss : 10.660520, Valid Loss 11.518356\n",
      "Epoch : 287/1000, Train Loss : 12.115760, Valid Loss 16.149766\n",
      "Epoch : 288/1000, Train Loss : 11.794033, Valid Loss 11.561410\n",
      "Epoch : 289/1000, Train Loss : 10.476920, Valid Loss 11.581720\n",
      "Epoch : 290/1000, Train Loss : 10.388347, Valid Loss 11.201083\n",
      "Epoch : 291/1000, Train Loss : 10.966730, Valid Loss 13.803102\n",
      "Epoch : 292/1000, Train Loss : 11.210032, Valid Loss 12.017152\n",
      "Epoch : 293/1000, Train Loss : 11.003853, Valid Loss 11.651579\n",
      "Epoch : 294/1000, Train Loss : 10.480386, Valid Loss 10.905530\n",
      "Epoch : 295/1000, Train Loss : 10.521018, Valid Loss 11.064546\n",
      "Epoch : 296/1000, Train Loss : 10.200125, Valid Loss 11.293185\n",
      "Epoch : 297/1000, Train Loss : 11.481046, Valid Loss 13.124843\n",
      "Epoch : 298/1000, Train Loss : 10.530681, Valid Loss 12.182703\n",
      "Epoch : 299/1000, Train Loss : 10.491574, Valid Loss 11.310010\n",
      "Epoch : 300/1000, Train Loss : 10.027741, Valid Loss 11.044856\n",
      "Epoch : 301/1000, Train Loss : 10.113996, Valid Loss 11.100313\n",
      "Epoch : 302/1000, Train Loss : 10.322343, Valid Loss 11.753013\n",
      "Epoch : 303/1000, Train Loss : 10.579245, Valid Loss 13.271403\n",
      "Epoch : 304/1000, Train Loss : 11.961469, Valid Loss 16.411194\n",
      "Epoch : 305/1000, Train Loss : 11.422005, Valid Loss 11.018265\n",
      "Epoch : 306/1000, Train Loss : 10.686977, Valid Loss 13.718863\n",
      "Epoch : 307/1000, Train Loss : 12.393770, Valid Loss 11.626498\n",
      "Epoch : 308/1000, Train Loss : 10.722520, Valid Loss 11.465265\n",
      "Epoch : 309/1000, Train Loss : 10.390518, Valid Loss 12.402599\n",
      "Epoch : 310/1000, Train Loss : 10.180392, Valid Loss 10.831340\n",
      "Epoch : 311/1000, Train Loss : 10.275809, Valid Loss 10.885788\n",
      "Epoch : 312/1000, Train Loss : 11.169132, Valid Loss 11.584878\n",
      "Epoch : 313/1000, Train Loss : 11.461036, Valid Loss 11.347526\n",
      "Epoch : 314/1000, Train Loss : 9.916404, Valid Loss 11.559680\n",
      "Epoch : 315/1000, Train Loss : 10.438300, Valid Loss 13.195081\n",
      "Epoch : 316/1000, Train Loss : 10.135327, Valid Loss 11.763950\n",
      "Epoch : 317/1000, Train Loss : 10.172489, Valid Loss 11.232315\n",
      "Epoch : 318/1000, Train Loss : 9.787811, Valid Loss 11.736034\n",
      "Epoch : 319/1000, Train Loss : 10.390116, Valid Loss 12.527637\n",
      "Epoch : 320/1000, Train Loss : 9.995638, Valid Loss 10.851125\n",
      "Epoch : 321/1000, Train Loss : 10.331913, Valid Loss 14.104334\n",
      "Epoch : 322/1000, Train Loss : 10.221690, Valid Loss 10.799973\n",
      "Epoch : 323/1000, Train Loss : 9.833228, Valid Loss 10.852362\n",
      "Epoch : 324/1000, Train Loss : 10.905029, Valid Loss 15.272735\n",
      "Epoch : 325/1000, Train Loss : 10.550363, Valid Loss 11.035201\n",
      "Epoch : 326/1000, Train Loss : 9.942902, Valid Loss 12.157939\n",
      "Epoch : 327/1000, Train Loss : 10.079906, Valid Loss 10.906641\n",
      "Epoch : 328/1000, Train Loss : 9.587708, Valid Loss 11.428397\n",
      "Epoch : 329/1000, Train Loss : 10.672222, Valid Loss 14.447818\n",
      "Epoch : 330/1000, Train Loss : 10.105814, Valid Loss 10.689183\n",
      "Epoch : 331/1000, Train Loss : 10.291197, Valid Loss 10.551374\n",
      "Epoch : 332/1000, Train Loss : 10.968169, Valid Loss 11.511082\n",
      "Epoch : 333/1000, Train Loss : 9.997011, Valid Loss 13.122874\n",
      "Epoch : 334/1000, Train Loss : 10.244012, Valid Loss 10.892278\n",
      "Epoch : 335/1000, Train Loss : 9.586550, Valid Loss 11.504744\n",
      "Epoch : 336/1000, Train Loss : 10.306776, Valid Loss 11.065715\n",
      "Epoch : 337/1000, Train Loss : 9.307654, Valid Loss 10.581711\n",
      "Epoch : 338/1000, Train Loss : 9.435664, Valid Loss 10.523763\n",
      "Epoch : 339/1000, Train Loss : 9.294167, Valid Loss 11.505391\n",
      "Epoch : 340/1000, Train Loss : 9.998741, Valid Loss 14.067482\n",
      "Epoch : 341/1000, Train Loss : 10.174836, Valid Loss 11.368637\n",
      "Epoch : 342/1000, Train Loss : 10.319492, Valid Loss 11.165692\n",
      "Epoch : 343/1000, Train Loss : 9.625061, Valid Loss 12.645249\n",
      "Epoch : 344/1000, Train Loss : 10.292224, Valid Loss 10.490792\n",
      "Epoch : 345/1000, Train Loss : 9.470129, Valid Loss 10.316347\n",
      "Epoch : 346/1000, Train Loss : 9.383833, Valid Loss 11.466021\n",
      "Epoch : 347/1000, Train Loss : 9.133297, Valid Loss 10.812968\n",
      "Epoch : 348/1000, Train Loss : 9.307883, Valid Loss 11.379984\n",
      "Epoch : 349/1000, Train Loss : 9.461954, Valid Loss 10.391013\n",
      "Epoch : 350/1000, Train Loss : 9.161769, Valid Loss 10.424240\n",
      "Epoch : 351/1000, Train Loss : 9.690211, Valid Loss 10.566037\n",
      "Epoch : 352/1000, Train Loss : 9.826047, Valid Loss 11.377653\n",
      "Epoch : 353/1000, Train Loss : 10.008597, Valid Loss 10.423192\n",
      "Epoch : 354/1000, Train Loss : 9.339972, Valid Loss 10.908623\n",
      "Epoch : 355/1000, Train Loss : 8.922709, Valid Loss 11.283024\n",
      "Epoch : 356/1000, Train Loss : 10.732101, Valid Loss 11.607920\n",
      "Epoch : 357/1000, Train Loss : 10.384745, Valid Loss 11.025164\n",
      "Epoch : 358/1000, Train Loss : 8.962102, Valid Loss 10.732119\n",
      "Epoch : 359/1000, Train Loss : 9.401850, Valid Loss 13.504600\n",
      "Epoch : 360/1000, Train Loss : 9.539386, Valid Loss 10.513751\n",
      "Epoch : 361/1000, Train Loss : 9.296991, Valid Loss 13.992857\n",
      "Epoch : 362/1000, Train Loss : 12.155530, Valid Loss 12.406279\n",
      "Epoch : 363/1000, Train Loss : 8.990467, Valid Loss 10.252409\n",
      "Epoch : 364/1000, Train Loss : 9.451184, Valid Loss 10.491141\n",
      "Epoch : 365/1000, Train Loss : 8.930020, Valid Loss 10.533983\n",
      "Epoch : 366/1000, Train Loss : 10.174993, Valid Loss 12.213820\n",
      "Epoch : 367/1000, Train Loss : 9.927522, Valid Loss 16.031841\n",
      "Epoch : 368/1000, Train Loss : 10.343829, Valid Loss 10.763488\n",
      "Epoch : 369/1000, Train Loss : 9.233796, Valid Loss 10.316852\n",
      "Epoch : 370/1000, Train Loss : 8.908383, Valid Loss 10.583333\n",
      "Epoch : 371/1000, Train Loss : 8.942910, Valid Loss 11.027260\n",
      "Epoch : 372/1000, Train Loss : 9.300078, Valid Loss 11.544959\n",
      "Epoch : 373/1000, Train Loss : 10.662156, Valid Loss 10.305649\n",
      "Epoch : 374/1000, Train Loss : 9.172495, Valid Loss 10.911069\n",
      "Epoch : 375/1000, Train Loss : 8.726670, Valid Loss 10.091430\n",
      "Epoch : 376/1000, Train Loss : 8.320962, Valid Loss 11.771450\n",
      "Epoch : 377/1000, Train Loss : 9.131476, Valid Loss 10.755315\n",
      "Epoch : 378/1000, Train Loss : 8.384886, Valid Loss 10.117259\n",
      "Epoch : 379/1000, Train Loss : 8.488107, Valid Loss 11.174271\n",
      "Epoch : 380/1000, Train Loss : 8.481082, Valid Loss 10.045753\n",
      "Epoch : 381/1000, Train Loss : 8.423213, Valid Loss 10.212915\n",
      "Epoch : 382/1000, Train Loss : 8.313012, Valid Loss 10.024934\n",
      "Epoch : 383/1000, Train Loss : 8.342791, Valid Loss 11.361705\n",
      "Epoch : 384/1000, Train Loss : 8.960483, Valid Loss 10.127668\n",
      "Epoch : 385/1000, Train Loss : 8.402626, Valid Loss 10.076425\n",
      "Epoch : 386/1000, Train Loss : 8.516892, Valid Loss 10.653136\n",
      "Epoch : 387/1000, Train Loss : 9.158366, Valid Loss 10.728131\n",
      "Epoch : 388/1000, Train Loss : 8.667865, Valid Loss 10.143885\n",
      "Epoch : 389/1000, Train Loss : 9.230861, Valid Loss 11.671153\n",
      "Epoch : 390/1000, Train Loss : 8.510675, Valid Loss 10.804937\n",
      "Epoch : 391/1000, Train Loss : 9.284489, Valid Loss 10.186359\n",
      "Epoch : 392/1000, Train Loss : 10.052047, Valid Loss 12.825722\n",
      "Epoch : 393/1000, Train Loss : 8.821262, Valid Loss 10.163813\n",
      "Epoch : 394/1000, Train Loss : 9.050081, Valid Loss 10.174982\n",
      "Epoch : 395/1000, Train Loss : 8.240169, Valid Loss 10.006950\n",
      "Epoch : 396/1000, Train Loss : 9.022861, Valid Loss 13.364494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 397/1000, Train Loss : 8.998206, Valid Loss 10.498788\n",
      "Epoch : 398/1000, Train Loss : 8.981791, Valid Loss 10.742903\n",
      "Epoch : 399/1000, Train Loss : 8.369495, Valid Loss 10.142751\n",
      "Epoch : 400/1000, Train Loss : 9.381063, Valid Loss 11.612102\n",
      "Epoch : 401/1000, Train Loss : 8.138059, Valid Loss 10.558767\n",
      "Epoch : 402/1000, Train Loss : 8.434222, Valid Loss 10.169509\n",
      "Epoch : 403/1000, Train Loss : 8.849316, Valid Loss 10.266486\n",
      "Epoch : 404/1000, Train Loss : 8.864819, Valid Loss 9.903698\n",
      "Epoch : 405/1000, Train Loss : 8.557919, Valid Loss 13.526413\n",
      "Epoch : 406/1000, Train Loss : 10.634432, Valid Loss 9.995327\n",
      "Epoch : 407/1000, Train Loss : 10.207454, Valid Loss 9.965683\n",
      "Epoch : 408/1000, Train Loss : 8.200171, Valid Loss 10.197138\n",
      "Epoch : 409/1000, Train Loss : 8.730258, Valid Loss 10.323763\n",
      "Epoch : 410/1000, Train Loss : 8.910158, Valid Loss 9.946690\n",
      "Epoch : 411/1000, Train Loss : 8.785566, Valid Loss 11.856901\n",
      "Epoch : 412/1000, Train Loss : 8.443433, Valid Loss 10.598417\n",
      "Epoch : 413/1000, Train Loss : 7.900977, Valid Loss 10.316129\n",
      "Epoch : 414/1000, Train Loss : 8.141586, Valid Loss 11.512198\n",
      "Epoch : 415/1000, Train Loss : 9.311601, Valid Loss 9.718712\n",
      "Epoch : 416/1000, Train Loss : 9.467798, Valid Loss 10.159721\n",
      "Epoch : 417/1000, Train Loss : 8.403942, Valid Loss 11.450851\n",
      "Epoch : 418/1000, Train Loss : 8.323648, Valid Loss 10.795500\n",
      "Epoch : 419/1000, Train Loss : 7.984755, Valid Loss 11.242560\n",
      "Epoch : 420/1000, Train Loss : 7.664581, Valid Loss 10.658967\n",
      "Epoch : 421/1000, Train Loss : 7.786452, Valid Loss 10.185531\n",
      "Epoch : 422/1000, Train Loss : 8.000606, Valid Loss 10.352584\n",
      "Epoch : 423/1000, Train Loss : 8.180907, Valid Loss 10.234019\n",
      "Epoch : 424/1000, Train Loss : 7.851030, Valid Loss 9.694616\n",
      "Epoch : 425/1000, Train Loss : 7.855320, Valid Loss 10.185488\n",
      "Epoch : 426/1000, Train Loss : 8.150149, Valid Loss 11.199707\n",
      "Epoch : 427/1000, Train Loss : 8.970037, Valid Loss 10.044805\n",
      "Epoch : 428/1000, Train Loss : 7.973933, Valid Loss 9.685901\n",
      "Epoch : 429/1000, Train Loss : 7.942465, Valid Loss 9.973253\n",
      "Epoch : 430/1000, Train Loss : 7.884765, Valid Loss 9.897694\n",
      "Epoch : 431/1000, Train Loss : 8.593278, Valid Loss 10.053529\n",
      "Epoch : 432/1000, Train Loss : 7.892223, Valid Loss 9.841310\n",
      "Epoch : 433/1000, Train Loss : 7.830819, Valid Loss 10.168576\n",
      "Epoch : 434/1000, Train Loss : 8.439036, Valid Loss 10.854250\n",
      "Epoch : 435/1000, Train Loss : 8.530712, Valid Loss 10.638987\n",
      "Epoch : 436/1000, Train Loss : 7.597035, Valid Loss 9.559844\n",
      "Epoch : 437/1000, Train Loss : 8.327869, Valid Loss 9.554019\n",
      "Epoch : 438/1000, Train Loss : 7.845198, Valid Loss 10.076958\n",
      "Epoch : 439/1000, Train Loss : 8.267992, Valid Loss 10.066393\n",
      "Epoch : 440/1000, Train Loss : 8.626719, Valid Loss 10.633670\n",
      "Epoch : 441/1000, Train Loss : 7.664592, Valid Loss 11.170149\n",
      "Epoch : 442/1000, Train Loss : 7.550357, Valid Loss 10.130642\n",
      "Epoch : 443/1000, Train Loss : 7.802758, Valid Loss 11.606026\n",
      "Epoch : 444/1000, Train Loss : 8.349129, Valid Loss 9.798265\n",
      "Epoch : 445/1000, Train Loss : 7.793160, Valid Loss 10.720495\n",
      "Epoch : 446/1000, Train Loss : 7.501236, Valid Loss 10.254951\n",
      "Epoch : 447/1000, Train Loss : 7.313163, Valid Loss 9.778977\n",
      "Epoch : 448/1000, Train Loss : 8.729258, Valid Loss 13.428965\n",
      "Epoch : 449/1000, Train Loss : 7.963182, Valid Loss 9.728632\n",
      "Epoch : 450/1000, Train Loss : 7.710870, Valid Loss 11.072642\n",
      "Epoch : 451/1000, Train Loss : 7.628969, Valid Loss 10.125694\n",
      "Epoch : 452/1000, Train Loss : 7.217774, Valid Loss 10.221190\n",
      "Epoch : 453/1000, Train Loss : 7.699352, Valid Loss 9.931691\n",
      "Epoch : 454/1000, Train Loss : 9.348067, Valid Loss 12.614213\n",
      "Epoch : 455/1000, Train Loss : 9.062417, Valid Loss 9.597960\n",
      "Epoch : 456/1000, Train Loss : 7.402882, Valid Loss 9.636736\n",
      "Epoch : 457/1000, Train Loss : 7.221409, Valid Loss 11.213273\n",
      "Epoch : 458/1000, Train Loss : 8.306513, Valid Loss 9.549770\n",
      "Epoch : 459/1000, Train Loss : 8.142586, Valid Loss 12.873693\n",
      "Epoch : 460/1000, Train Loss : 9.517127, Valid Loss 9.481661\n",
      "Epoch : 461/1000, Train Loss : 8.540721, Valid Loss 9.638753\n",
      "Epoch : 462/1000, Train Loss : 7.607255, Valid Loss 9.827663\n",
      "Epoch : 463/1000, Train Loss : 7.739964, Valid Loss 10.050215\n",
      "Epoch : 464/1000, Train Loss : 7.431230, Valid Loss 9.355492\n",
      "Epoch : 465/1000, Train Loss : 7.116772, Valid Loss 9.502712\n",
      "Epoch : 466/1000, Train Loss : 6.987060, Valid Loss 11.713939\n",
      "Epoch : 467/1000, Train Loss : 7.699942, Valid Loss 10.463069\n",
      "Epoch : 468/1000, Train Loss : 7.001265, Valid Loss 11.017730\n",
      "Epoch : 469/1000, Train Loss : 8.513532, Valid Loss 9.917274\n",
      "Epoch : 470/1000, Train Loss : 8.356479, Valid Loss 9.780086\n",
      "Epoch : 471/1000, Train Loss : 8.201687, Valid Loss 12.067117\n",
      "Epoch : 472/1000, Train Loss : 7.823727, Valid Loss 9.503661\n",
      "Epoch : 473/1000, Train Loss : 7.206896, Valid Loss 9.288851\n",
      "Epoch : 474/1000, Train Loss : 7.351801, Valid Loss 9.637863\n",
      "Epoch : 475/1000, Train Loss : 7.008927, Valid Loss 9.611345\n",
      "Epoch : 476/1000, Train Loss : 7.425321, Valid Loss 10.123401\n",
      "Epoch : 477/1000, Train Loss : 6.811308, Valid Loss 9.484379\n",
      "Epoch : 478/1000, Train Loss : 8.064234, Valid Loss 10.416560\n",
      "Epoch : 479/1000, Train Loss : 7.552576, Valid Loss 9.620455\n",
      "Epoch : 480/1000, Train Loss : 7.503249, Valid Loss 9.269984\n",
      "Epoch : 481/1000, Train Loss : 7.135758, Valid Loss 9.421930\n",
      "Epoch : 482/1000, Train Loss : 6.900491, Valid Loss 9.339644\n",
      "Epoch : 483/1000, Train Loss : 7.109982, Valid Loss 9.819770\n",
      "Epoch : 484/1000, Train Loss : 7.136405, Valid Loss 9.404644\n",
      "Epoch : 485/1000, Train Loss : 7.129974, Valid Loss 10.670332\n",
      "Epoch : 486/1000, Train Loss : 6.659930, Valid Loss 10.422136\n",
      "Epoch : 487/1000, Train Loss : 7.060968, Valid Loss 12.413129\n",
      "Epoch : 488/1000, Train Loss : 10.272109, Valid Loss 13.589787\n",
      "Epoch : 489/1000, Train Loss : 8.439723, Valid Loss 10.279452\n",
      "Epoch : 490/1000, Train Loss : 6.869395, Valid Loss 9.475696\n",
      "Epoch : 491/1000, Train Loss : 6.830036, Valid Loss 10.087071\n",
      "Epoch : 492/1000, Train Loss : 7.159984, Valid Loss 9.222531\n",
      "Epoch : 493/1000, Train Loss : 6.887658, Valid Loss 10.232312\n",
      "Epoch : 494/1000, Train Loss : 7.194992, Valid Loss 9.738789\n",
      "Epoch : 495/1000, Train Loss : 7.024312, Valid Loss 10.221650\n",
      "Epoch : 496/1000, Train Loss : 7.159490, Valid Loss 9.174102\n",
      "Epoch : 497/1000, Train Loss : 7.156080, Valid Loss 9.338710\n",
      "Epoch : 498/1000, Train Loss : 6.934198, Valid Loss 9.685607\n",
      "Epoch : 499/1000, Train Loss : 6.590447, Valid Loss 9.300070\n",
      "Epoch : 500/1000, Train Loss : 6.588566, Valid Loss 9.934611\n",
      "Epoch : 501/1000, Train Loss : 6.957098, Valid Loss 9.981367\n",
      "Epoch : 502/1000, Train Loss : 6.759194, Valid Loss 9.258043\n",
      "Epoch : 503/1000, Train Loss : 6.962623, Valid Loss 9.237437\n",
      "Epoch : 504/1000, Train Loss : 7.353726, Valid Loss 9.571897\n",
      "Epoch : 505/1000, Train Loss : 7.842619, Valid Loss 10.442110\n",
      "Epoch : 506/1000, Train Loss : 6.831438, Valid Loss 10.383547\n",
      "Epoch : 507/1000, Train Loss : 6.524836, Valid Loss 9.030080\n",
      "Epoch : 508/1000, Train Loss : 7.092635, Valid Loss 9.568855\n",
      "Epoch : 509/1000, Train Loss : 6.374040, Valid Loss 9.449788\n",
      "Epoch : 510/1000, Train Loss : 6.584487, Valid Loss 9.446453\n",
      "Epoch : 511/1000, Train Loss : 6.647864, Valid Loss 11.059689\n",
      "Epoch : 512/1000, Train Loss : 7.977481, Valid Loss 9.988662\n",
      "Epoch : 513/1000, Train Loss : 6.562813, Valid Loss 9.170693\n",
      "Epoch : 514/1000, Train Loss : 6.656049, Valid Loss 9.783149\n",
      "Epoch : 515/1000, Train Loss : 6.598450, Valid Loss 10.091307\n",
      "Epoch : 516/1000, Train Loss : 6.512279, Valid Loss 9.725238\n",
      "Epoch : 517/1000, Train Loss : 7.038717, Valid Loss 9.748553\n",
      "Epoch : 518/1000, Train Loss : 7.176921, Valid Loss 9.116154\n",
      "Epoch : 519/1000, Train Loss : 6.726293, Valid Loss 11.289041\n",
      "Epoch : 520/1000, Train Loss : 7.078332, Valid Loss 11.738999\n",
      "Epoch : 521/1000, Train Loss : 6.992276, Valid Loss 9.455906\n",
      "Epoch : 522/1000, Train Loss : 6.327865, Valid Loss 9.534504\n",
      "Epoch : 523/1000, Train Loss : 6.282640, Valid Loss 9.858018\n",
      "Epoch : 524/1000, Train Loss : 7.169256, Valid Loss 9.696018\n",
      "Epoch : 525/1000, Train Loss : 6.627410, Valid Loss 9.202301\n",
      "Epoch : 526/1000, Train Loss : 6.640841, Valid Loss 9.551272\n",
      "Epoch : 527/1000, Train Loss : 7.288750, Valid Loss 9.991242\n",
      "Epoch : 528/1000, Train Loss : 6.497089, Valid Loss 9.767472\n",
      "Epoch : 529/1000, Train Loss : 6.089726, Valid Loss 10.079865\n",
      "Epoch : 530/1000, Train Loss : 7.157272, Valid Loss 9.825294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 531/1000, Train Loss : 6.067839, Valid Loss 10.202530\n",
      "Epoch : 532/1000, Train Loss : 6.645296, Valid Loss 10.316754\n",
      "Epoch : 533/1000, Train Loss : 7.235519, Valid Loss 9.773104\n",
      "Epoch : 534/1000, Train Loss : 7.767439, Valid Loss 9.910338\n",
      "Epoch : 535/1000, Train Loss : 6.402451, Valid Loss 8.870997\n",
      "Epoch : 536/1000, Train Loss : 6.598183, Valid Loss 9.661762\n",
      "Epoch : 537/1000, Train Loss : 6.273912, Valid Loss 9.449165\n",
      "Epoch : 538/1000, Train Loss : 6.416787, Valid Loss 9.479396\n",
      "Epoch : 539/1000, Train Loss : 6.243819, Valid Loss 9.230173\n",
      "Epoch : 540/1000, Train Loss : 6.513553, Valid Loss 9.525838\n",
      "Epoch : 541/1000, Train Loss : 6.119587, Valid Loss 9.192241\n",
      "Epoch : 542/1000, Train Loss : 5.912576, Valid Loss 8.953144\n",
      "Epoch : 543/1000, Train Loss : 6.174207, Valid Loss 10.226153\n",
      "Epoch : 544/1000, Train Loss : 6.307659, Valid Loss 9.313653\n",
      "Epoch : 545/1000, Train Loss : 6.239843, Valid Loss 9.064045\n",
      "Epoch : 546/1000, Train Loss : 6.129379, Valid Loss 9.900979\n",
      "Epoch : 547/1000, Train Loss : 6.210410, Valid Loss 8.860173\n",
      "Epoch : 548/1000, Train Loss : 6.673760, Valid Loss 9.353701\n",
      "Epoch : 549/1000, Train Loss : 6.071969, Valid Loss 10.091017\n",
      "Epoch : 550/1000, Train Loss : 6.408854, Valid Loss 8.949735\n",
      "Epoch : 551/1000, Train Loss : 6.168115, Valid Loss 11.710008\n",
      "Epoch : 552/1000, Train Loss : 6.974860, Valid Loss 9.023040\n",
      "Epoch : 553/1000, Train Loss : 5.854898, Valid Loss 8.798534\n",
      "Epoch : 554/1000, Train Loss : 6.257228, Valid Loss 9.051735\n",
      "Epoch : 555/1000, Train Loss : 6.183766, Valid Loss 8.839439\n",
      "Epoch : 556/1000, Train Loss : 6.719879, Valid Loss 8.920090\n",
      "Epoch : 557/1000, Train Loss : 6.027083, Valid Loss 8.860501\n",
      "Epoch : 558/1000, Train Loss : 6.184453, Valid Loss 11.524439\n",
      "Epoch : 559/1000, Train Loss : 6.435874, Valid Loss 10.045583\n",
      "Epoch : 560/1000, Train Loss : 6.744363, Valid Loss 8.733136\n",
      "Epoch : 561/1000, Train Loss : 6.104312, Valid Loss 8.995665\n",
      "Epoch : 562/1000, Train Loss : 5.875542, Valid Loss 9.436712\n",
      "Epoch : 563/1000, Train Loss : 5.889820, Valid Loss 8.996860\n",
      "Epoch : 564/1000, Train Loss : 5.951173, Valid Loss 9.759630\n",
      "Epoch : 565/1000, Train Loss : 6.750050, Valid Loss 9.197958\n",
      "Epoch : 566/1000, Train Loss : 5.857333, Valid Loss 8.796166\n",
      "Epoch : 567/1000, Train Loss : 5.935935, Valid Loss 9.088243\n",
      "Epoch : 568/1000, Train Loss : 5.602275, Valid Loss 8.997746\n",
      "Epoch : 569/1000, Train Loss : 5.790499, Valid Loss 11.859898\n",
      "Epoch : 570/1000, Train Loss : 5.932041, Valid Loss 9.958210\n",
      "Epoch : 571/1000, Train Loss : 6.253647, Valid Loss 9.825282\n",
      "Epoch : 572/1000, Train Loss : 5.738666, Valid Loss 10.649050\n",
      "Epoch : 573/1000, Train Loss : 6.397147, Valid Loss 9.026479\n",
      "Epoch : 574/1000, Train Loss : 5.711191, Valid Loss 8.926620\n",
      "Epoch : 575/1000, Train Loss : 5.987516, Valid Loss 9.336287\n",
      "Epoch : 576/1000, Train Loss : 5.912021, Valid Loss 10.415505\n",
      "Epoch : 577/1000, Train Loss : 6.426735, Valid Loss 9.071325\n",
      "Epoch : 578/1000, Train Loss : 5.704933, Valid Loss 9.774888\n",
      "Epoch : 579/1000, Train Loss : 5.824892, Valid Loss 8.969112\n",
      "Epoch : 580/1000, Train Loss : 5.931689, Valid Loss 9.418473\n",
      "Epoch : 581/1000, Train Loss : 6.023963, Valid Loss 9.243878\n",
      "Epoch : 582/1000, Train Loss : 6.318782, Valid Loss 9.098060\n",
      "Epoch : 583/1000, Train Loss : 5.937645, Valid Loss 10.394879\n",
      "Epoch : 584/1000, Train Loss : 6.134543, Valid Loss 9.381195\n",
      "Epoch : 585/1000, Train Loss : 6.356589, Valid Loss 8.695075\n",
      "Epoch : 586/1000, Train Loss : 5.424426, Valid Loss 8.614384\n",
      "Epoch : 587/1000, Train Loss : 5.595858, Valid Loss 8.816595\n",
      "Epoch : 588/1000, Train Loss : 5.492399, Valid Loss 8.867211\n",
      "Epoch : 589/1000, Train Loss : 5.519990, Valid Loss 9.133170\n",
      "Epoch : 590/1000, Train Loss : 5.576449, Valid Loss 8.970051\n",
      "Epoch : 591/1000, Train Loss : 5.661326, Valid Loss 12.218556\n",
      "Epoch : 592/1000, Train Loss : 6.569690, Valid Loss 8.788733\n",
      "Epoch : 593/1000, Train Loss : 6.006733, Valid Loss 9.170893\n",
      "Epoch : 594/1000, Train Loss : 5.940059, Valid Loss 8.941535\n",
      "Epoch : 595/1000, Train Loss : 5.609167, Valid Loss 9.023133\n",
      "Epoch : 596/1000, Train Loss : 5.316507, Valid Loss 8.689739\n",
      "Epoch : 597/1000, Train Loss : 5.377599, Valid Loss 9.233349\n",
      "Epoch : 598/1000, Train Loss : 5.919267, Valid Loss 10.241393\n",
      "Epoch : 599/1000, Train Loss : 6.243891, Valid Loss 8.397880\n",
      "Epoch : 600/1000, Train Loss : 5.630972, Valid Loss 9.414594\n",
      "Epoch : 601/1000, Train Loss : 5.971659, Valid Loss 8.636171\n",
      "Epoch : 602/1000, Train Loss : 7.253558, Valid Loss 11.460500\n",
      "Epoch : 603/1000, Train Loss : 5.753094, Valid Loss 8.719580\n",
      "Epoch : 604/1000, Train Loss : 6.394850, Valid Loss 8.893714\n",
      "Epoch : 605/1000, Train Loss : 6.428613, Valid Loss 9.196352\n",
      "Epoch : 606/1000, Train Loss : 6.186727, Valid Loss 10.167496\n",
      "Epoch : 607/1000, Train Loss : 5.543441, Valid Loss 8.483307\n",
      "Epoch : 608/1000, Train Loss : 5.407167, Valid Loss 9.512214\n",
      "Epoch : 609/1000, Train Loss : 5.514163, Valid Loss 8.853835\n",
      "Epoch : 610/1000, Train Loss : 6.016054, Valid Loss 9.032087\n",
      "Epoch : 611/1000, Train Loss : 5.855695, Valid Loss 8.585739\n",
      "Epoch : 612/1000, Train Loss : 5.132112, Valid Loss 9.542555\n",
      "Epoch : 613/1000, Train Loss : 5.977392, Valid Loss 8.409163\n",
      "Epoch : 614/1000, Train Loss : 5.785920, Valid Loss 8.489056\n",
      "Epoch : 615/1000, Train Loss : 5.373836, Valid Loss 8.509365\n",
      "Epoch : 616/1000, Train Loss : 6.708936, Valid Loss 10.576490\n",
      "Epoch : 617/1000, Train Loss : 5.607784, Valid Loss 9.066614\n",
      "Epoch : 618/1000, Train Loss : 5.169178, Valid Loss 10.159266\n",
      "Epoch : 619/1000, Train Loss : 5.173012, Valid Loss 10.246305\n",
      "Epoch : 620/1000, Train Loss : 6.287447, Valid Loss 9.761440\n",
      "Epoch : 621/1000, Train Loss : 6.110152, Valid Loss 8.487300\n",
      "Epoch : 622/1000, Train Loss : 5.296219, Valid Loss 9.173758\n",
      "Epoch : 623/1000, Train Loss : 5.246849, Valid Loss 9.911182\n",
      "Epoch : 624/1000, Train Loss : 5.136345, Valid Loss 9.574319\n",
      "Epoch : 625/1000, Train Loss : 5.108309, Valid Loss 8.516494\n",
      "Epoch : 626/1000, Train Loss : 4.956240, Valid Loss 8.361310\n",
      "Epoch : 627/1000, Train Loss : 5.197549, Valid Loss 8.850084\n",
      "Epoch : 628/1000, Train Loss : 5.244439, Valid Loss 8.689062\n",
      "Epoch : 629/1000, Train Loss : 5.393668, Valid Loss 8.825708\n",
      "Epoch : 630/1000, Train Loss : 5.155957, Valid Loss 9.278624\n",
      "Epoch : 631/1000, Train Loss : 5.146478, Valid Loss 9.083498\n",
      "Epoch : 632/1000, Train Loss : 5.026529, Valid Loss 9.517007\n",
      "Epoch : 633/1000, Train Loss : 5.535410, Valid Loss 9.334981\n",
      "Epoch : 634/1000, Train Loss : 5.715132, Valid Loss 8.359683\n",
      "Epoch : 635/1000, Train Loss : 5.238610, Valid Loss 8.841306\n",
      "Epoch : 636/1000, Train Loss : 5.356145, Valid Loss 8.266465\n",
      "Epoch : 637/1000, Train Loss : 5.852454, Valid Loss 9.975480\n",
      "Epoch : 638/1000, Train Loss : 5.163151, Valid Loss 8.428287\n",
      "Epoch : 639/1000, Train Loss : 4.961983, Valid Loss 8.494600\n",
      "Epoch : 640/1000, Train Loss : 5.203304, Valid Loss 8.452947\n",
      "Epoch : 641/1000, Train Loss : 5.622949, Valid Loss 9.174706\n",
      "Epoch : 642/1000, Train Loss : 5.227435, Valid Loss 8.582795\n",
      "Epoch : 643/1000, Train Loss : 5.386118, Valid Loss 8.542913\n",
      "Epoch : 644/1000, Train Loss : 5.599926, Valid Loss 8.485005\n",
      "Epoch : 645/1000, Train Loss : 5.181769, Valid Loss 8.723159\n",
      "Epoch : 646/1000, Train Loss : 5.336887, Valid Loss 8.659953\n",
      "Epoch : 647/1000, Train Loss : 5.871830, Valid Loss 9.569252\n",
      "Epoch : 648/1000, Train Loss : 5.449656, Valid Loss 8.261682\n",
      "Epoch : 649/1000, Train Loss : 5.016318, Valid Loss 8.982621\n",
      "Epoch : 650/1000, Train Loss : 4.850614, Valid Loss 8.440465\n",
      "Epoch : 651/1000, Train Loss : 4.648077, Valid Loss 8.830302\n",
      "Epoch : 652/1000, Train Loss : 5.059050, Valid Loss 8.844540\n",
      "Epoch : 653/1000, Train Loss : 5.613012, Valid Loss 8.557858\n",
      "Epoch : 654/1000, Train Loss : 5.317391, Valid Loss 9.002546\n",
      "Epoch : 655/1000, Train Loss : 6.581569, Valid Loss 10.570431\n",
      "Epoch : 656/1000, Train Loss : 5.676460, Valid Loss 8.389646\n",
      "Epoch : 657/1000, Train Loss : 4.969037, Valid Loss 8.988471\n",
      "Epoch : 658/1000, Train Loss : 5.143476, Valid Loss 8.150564\n",
      "Epoch : 659/1000, Train Loss : 4.801190, Valid Loss 9.035133\n",
      "Epoch : 660/1000, Train Loss : 4.888728, Valid Loss 8.903725\n",
      "Epoch : 661/1000, Train Loss : 5.374516, Valid Loss 9.157329\n",
      "Epoch : 662/1000, Train Loss : 5.424044, Valid Loss 11.238467\n",
      "Epoch : 663/1000, Train Loss : 5.383717, Valid Loss 8.337140\n",
      "Epoch : 664/1000, Train Loss : 4.883989, Valid Loss 8.040128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 665/1000, Train Loss : 5.398625, Valid Loss 9.214578\n",
      "Epoch : 666/1000, Train Loss : 4.937365, Valid Loss 8.887685\n",
      "Epoch : 667/1000, Train Loss : 5.259057, Valid Loss 11.087280\n",
      "Epoch : 668/1000, Train Loss : 5.258602, Valid Loss 8.849310\n",
      "Epoch : 669/1000, Train Loss : 4.963677, Valid Loss 8.678467\n",
      "Epoch : 670/1000, Train Loss : 5.053885, Valid Loss 8.440699\n",
      "Epoch : 671/1000, Train Loss : 4.936166, Valid Loss 8.500221\n",
      "Epoch : 672/1000, Train Loss : 4.714136, Valid Loss 8.369712\n",
      "Epoch : 673/1000, Train Loss : 5.838861, Valid Loss 8.533288\n",
      "Epoch : 674/1000, Train Loss : 5.987300, Valid Loss 8.433028\n",
      "Epoch : 675/1000, Train Loss : 4.699981, Valid Loss 8.960471\n",
      "Epoch : 676/1000, Train Loss : 4.601401, Valid Loss 8.076967\n",
      "Epoch : 677/1000, Train Loss : 5.976278, Valid Loss 11.369517\n",
      "Epoch : 678/1000, Train Loss : 5.290850, Valid Loss 8.276173\n",
      "Epoch : 679/1000, Train Loss : 4.714567, Valid Loss 8.026692\n",
      "Epoch : 680/1000, Train Loss : 5.161360, Valid Loss 8.700176\n",
      "Epoch : 681/1000, Train Loss : 5.270580, Valid Loss 8.303333\n",
      "Epoch : 682/1000, Train Loss : 4.812429, Valid Loss 8.671289\n",
      "Epoch : 683/1000, Train Loss : 4.691504, Valid Loss 9.052642\n",
      "Epoch : 684/1000, Train Loss : 4.849713, Valid Loss 8.519612\n",
      "Epoch : 685/1000, Train Loss : 4.504872, Valid Loss 8.343409\n",
      "Epoch : 686/1000, Train Loss : 5.014726, Valid Loss 9.504905\n",
      "Epoch : 687/1000, Train Loss : 5.284214, Valid Loss 8.339753\n",
      "Epoch : 688/1000, Train Loss : 5.085561, Valid Loss 10.073363\n",
      "Epoch : 689/1000, Train Loss : 5.289255, Valid Loss 8.222924\n",
      "Epoch : 690/1000, Train Loss : 4.651681, Valid Loss 9.042076\n",
      "Epoch : 691/1000, Train Loss : 4.780958, Valid Loss 8.154717\n",
      "Epoch : 692/1000, Train Loss : 5.021666, Valid Loss 8.740470\n",
      "Epoch : 693/1000, Train Loss : 4.842472, Valid Loss 8.696135\n",
      "Epoch : 694/1000, Train Loss : 4.542076, Valid Loss 9.092884\n",
      "Epoch : 695/1000, Train Loss : 4.928712, Valid Loss 8.237354\n",
      "Epoch : 696/1000, Train Loss : 4.439450, Valid Loss 11.052050\n",
      "Epoch : 697/1000, Train Loss : 4.833043, Valid Loss 8.424581\n",
      "Epoch : 698/1000, Train Loss : 4.787165, Valid Loss 8.724253\n",
      "Epoch : 699/1000, Train Loss : 4.814211, Valid Loss 8.025268\n",
      "Epoch : 700/1000, Train Loss : 4.510670, Valid Loss 8.182898\n",
      "Epoch : 701/1000, Train Loss : 5.503092, Valid Loss 9.028974\n",
      "Epoch : 702/1000, Train Loss : 5.286195, Valid Loss 8.335769\n",
      "Epoch : 703/1000, Train Loss : 4.364777, Valid Loss 8.120896\n",
      "Epoch : 704/1000, Train Loss : 4.932214, Valid Loss 8.286029\n",
      "Epoch : 705/1000, Train Loss : 4.772721, Valid Loss 7.847899\n",
      "Epoch : 706/1000, Train Loss : 4.339158, Valid Loss 8.250848\n",
      "Epoch : 707/1000, Train Loss : 4.409633, Valid Loss 8.232687\n",
      "Epoch : 708/1000, Train Loss : 4.647927, Valid Loss 8.180275\n",
      "Epoch : 709/1000, Train Loss : 4.498700, Valid Loss 8.263160\n",
      "Epoch : 710/1000, Train Loss : 4.634581, Valid Loss 9.027797\n",
      "Epoch : 711/1000, Train Loss : 4.689109, Valid Loss 9.472438\n",
      "Epoch : 712/1000, Train Loss : 5.861839, Valid Loss 10.571118\n",
      "Epoch : 713/1000, Train Loss : 4.827489, Valid Loss 9.164354\n",
      "Epoch : 714/1000, Train Loss : 5.202712, Valid Loss 9.981480\n",
      "Epoch : 715/1000, Train Loss : 4.435032, Valid Loss 8.961515\n",
      "Epoch : 716/1000, Train Loss : 4.800013, Valid Loss 8.411093\n",
      "Epoch : 717/1000, Train Loss : 4.404533, Valid Loss 8.161354\n",
      "Epoch : 718/1000, Train Loss : 5.009413, Valid Loss 9.352826\n",
      "Epoch : 719/1000, Train Loss : 6.363257, Valid Loss 8.967273\n",
      "Epoch : 720/1000, Train Loss : 4.992504, Valid Loss 9.378152\n",
      "Epoch : 721/1000, Train Loss : 4.433767, Valid Loss 7.973547\n",
      "Epoch : 722/1000, Train Loss : 4.276537, Valid Loss 7.994667\n",
      "Epoch : 723/1000, Train Loss : 4.512607, Valid Loss 7.740502\n",
      "Epoch : 724/1000, Train Loss : 4.368342, Valid Loss 8.338340\n",
      "Epoch : 725/1000, Train Loss : 4.760863, Valid Loss 9.968457\n",
      "Epoch : 726/1000, Train Loss : 5.742629, Valid Loss 8.677766\n",
      "Epoch : 727/1000, Train Loss : 4.397966, Valid Loss 7.912851\n",
      "Epoch : 728/1000, Train Loss : 4.125750, Valid Loss 8.414944\n",
      "Epoch : 729/1000, Train Loss : 4.210364, Valid Loss 7.771120\n",
      "Epoch : 730/1000, Train Loss : 4.521937, Valid Loss 8.408578\n",
      "Epoch : 731/1000, Train Loss : 4.860788, Valid Loss 8.455322\n",
      "Epoch : 732/1000, Train Loss : 4.301573, Valid Loss 8.185229\n",
      "Epoch : 733/1000, Train Loss : 4.845590, Valid Loss 8.559194\n",
      "Epoch : 734/1000, Train Loss : 5.531916, Valid Loss 8.388646\n",
      "Epoch : 735/1000, Train Loss : 5.081345, Valid Loss 8.024030\n",
      "Epoch : 736/1000, Train Loss : 4.531233, Valid Loss 8.240188\n",
      "Epoch : 737/1000, Train Loss : 4.360436, Valid Loss 10.031455\n",
      "Epoch : 738/1000, Train Loss : 4.722685, Valid Loss 8.154899\n",
      "Epoch : 739/1000, Train Loss : 4.074559, Valid Loss 8.277470\n",
      "Epoch : 740/1000, Train Loss : 4.100692, Valid Loss 8.011532\n",
      "Epoch : 741/1000, Train Loss : 4.016210, Valid Loss 7.972020\n",
      "Epoch : 742/1000, Train Loss : 4.440796, Valid Loss 8.356624\n",
      "Epoch : 743/1000, Train Loss : 4.219672, Valid Loss 7.784147\n",
      "Epoch : 744/1000, Train Loss : 4.172121, Valid Loss 9.140712\n",
      "Epoch : 745/1000, Train Loss : 5.214460, Valid Loss 8.281087\n",
      "Epoch : 746/1000, Train Loss : 4.198113, Valid Loss 8.270511\n",
      "Epoch : 747/1000, Train Loss : 4.530030, Valid Loss 8.383442\n",
      "Epoch : 748/1000, Train Loss : 4.112889, Valid Loss 7.926473\n",
      "Epoch : 749/1000, Train Loss : 4.120802, Valid Loss 8.854122\n",
      "Epoch : 750/1000, Train Loss : 4.509774, Valid Loss 8.044955\n",
      "Epoch : 751/1000, Train Loss : 4.476047, Valid Loss 8.378345\n",
      "Epoch : 752/1000, Train Loss : 4.139401, Valid Loss 7.735328\n",
      "Epoch : 753/1000, Train Loss : 4.006469, Valid Loss 8.132180\n",
      "Epoch : 754/1000, Train Loss : 4.637055, Valid Loss 8.159571\n",
      "Epoch : 755/1000, Train Loss : 4.601628, Valid Loss 8.440926\n",
      "Epoch : 756/1000, Train Loss : 5.058973, Valid Loss 8.411494\n",
      "Epoch : 757/1000, Train Loss : 4.553826, Valid Loss 9.445396\n",
      "Epoch : 758/1000, Train Loss : 4.024865, Valid Loss 8.548261\n",
      "Epoch : 759/1000, Train Loss : 4.432051, Valid Loss 8.412862\n",
      "Epoch : 760/1000, Train Loss : 4.828871, Valid Loss 8.978624\n",
      "Epoch : 761/1000, Train Loss : 5.285991, Valid Loss 10.332838\n",
      "Epoch : 762/1000, Train Loss : 5.659289, Valid Loss 8.542764\n",
      "Epoch : 763/1000, Train Loss : 4.126870, Valid Loss 10.992769\n",
      "Epoch : 764/1000, Train Loss : 5.114831, Valid Loss 9.214620\n",
      "Epoch : 765/1000, Train Loss : 4.376070, Valid Loss 8.063521\n",
      "Epoch : 766/1000, Train Loss : 4.213540, Valid Loss 9.228721\n",
      "Epoch : 767/1000, Train Loss : 4.765838, Valid Loss 9.029829\n",
      "Epoch : 768/1000, Train Loss : 4.709839, Valid Loss 11.513925\n",
      "Epoch : 769/1000, Train Loss : 4.708358, Valid Loss 8.153989\n",
      "Epoch : 770/1000, Train Loss : 4.106820, Valid Loss 8.219119\n",
      "Epoch : 771/1000, Train Loss : 3.904353, Valid Loss 8.577062\n",
      "Epoch : 772/1000, Train Loss : 4.252784, Valid Loss 7.770070\n",
      "Epoch : 773/1000, Train Loss : 3.819235, Valid Loss 7.923317\n",
      "Epoch : 774/1000, Train Loss : 4.063425, Valid Loss 7.656338\n",
      "Epoch : 775/1000, Train Loss : 4.292615, Valid Loss 8.067492\n",
      "Epoch : 776/1000, Train Loss : 3.883357, Valid Loss 9.451034\n",
      "Epoch : 777/1000, Train Loss : 4.791244, Valid Loss 7.821165\n",
      "Epoch : 778/1000, Train Loss : 3.911676, Valid Loss 7.584963\n",
      "Epoch : 779/1000, Train Loss : 3.887743, Valid Loss 8.627356\n",
      "Epoch : 780/1000, Train Loss : 4.237219, Valid Loss 8.016188\n",
      "Epoch : 781/1000, Train Loss : 4.220147, Valid Loss 7.792891\n",
      "Epoch : 782/1000, Train Loss : 3.899931, Valid Loss 7.881131\n",
      "Epoch : 783/1000, Train Loss : 5.024690, Valid Loss 8.714352\n",
      "Epoch : 784/1000, Train Loss : 4.196223, Valid Loss 8.229615\n",
      "Epoch : 785/1000, Train Loss : 3.949753, Valid Loss 7.941724\n",
      "Epoch : 786/1000, Train Loss : 4.020722, Valid Loss 8.744927\n",
      "Epoch : 787/1000, Train Loss : 3.915346, Valid Loss 8.740981\n",
      "Epoch : 788/1000, Train Loss : 4.272598, Valid Loss 9.097743\n",
      "Epoch : 789/1000, Train Loss : 4.105278, Valid Loss 8.101015\n",
      "Epoch : 790/1000, Train Loss : 4.671377, Valid Loss 7.631468\n",
      "Epoch : 791/1000, Train Loss : 3.996532, Valid Loss 7.739781\n",
      "Epoch : 792/1000, Train Loss : 3.860689, Valid Loss 8.884040\n",
      "Epoch : 793/1000, Train Loss : 4.067105, Valid Loss 8.868510\n",
      "Epoch : 794/1000, Train Loss : 3.837147, Valid Loss 7.838334\n",
      "Epoch : 795/1000, Train Loss : 4.002682, Valid Loss 7.776398\n",
      "Epoch : 796/1000, Train Loss : 3.999372, Valid Loss 8.676684\n",
      "Epoch : 797/1000, Train Loss : 3.890825, Valid Loss 8.266956\n",
      "Epoch : 798/1000, Train Loss : 4.092217, Valid Loss 8.344268\n",
      "Epoch : 799/1000, Train Loss : 3.842711, Valid Loss 8.045399\n",
      "Epoch : 800/1000, Train Loss : 3.950648, Valid Loss 8.760859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 801/1000, Train Loss : 3.973989, Valid Loss 8.197512\n",
      "Epoch : 802/1000, Train Loss : 3.582781, Valid Loss 7.610136\n",
      "Epoch : 803/1000, Train Loss : 3.871473, Valid Loss 7.828439\n",
      "Epoch : 804/1000, Train Loss : 4.752396, Valid Loss 7.932750\n",
      "Epoch : 805/1000, Train Loss : 4.297970, Valid Loss 8.514554\n",
      "Epoch : 806/1000, Train Loss : 4.354229, Valid Loss 8.325437\n",
      "Epoch : 807/1000, Train Loss : 4.601477, Valid Loss 7.759041\n",
      "Epoch : 808/1000, Train Loss : 3.737310, Valid Loss 7.652018\n",
      "Epoch : 809/1000, Train Loss : 3.485382, Valid Loss 8.126933\n",
      "Epoch : 810/1000, Train Loss : 3.882498, Valid Loss 10.041751\n",
      "Epoch : 811/1000, Train Loss : 4.369005, Valid Loss 8.454534\n",
      "Epoch : 812/1000, Train Loss : 3.880675, Valid Loss 8.404684\n",
      "Epoch : 813/1000, Train Loss : 3.930566, Valid Loss 8.199978\n",
      "Epoch : 814/1000, Train Loss : 3.679973, Valid Loss 7.619406\n",
      "Epoch : 815/1000, Train Loss : 3.953698, Valid Loss 8.844503\n",
      "Epoch : 816/1000, Train Loss : 4.349538, Valid Loss 11.050917\n",
      "Epoch : 817/1000, Train Loss : 4.662254, Valid Loss 7.592585\n",
      "Epoch : 818/1000, Train Loss : 3.828698, Valid Loss 7.653345\n",
      "Epoch : 819/1000, Train Loss : 3.988916, Valid Loss 7.901441\n",
      "Epoch : 820/1000, Train Loss : 3.922374, Valid Loss 7.720565\n",
      "Epoch : 821/1000, Train Loss : 3.791266, Valid Loss 7.623459\n",
      "Epoch : 822/1000, Train Loss : 3.948592, Valid Loss 7.786404\n",
      "Epoch : 823/1000, Train Loss : 4.027383, Valid Loss 7.651730\n",
      "Epoch : 824/1000, Train Loss : 3.763349, Valid Loss 7.575164\n",
      "Epoch : 825/1000, Train Loss : 3.986843, Valid Loss 7.906829\n",
      "Epoch : 826/1000, Train Loss : 4.089039, Valid Loss 7.896084\n",
      "Epoch : 827/1000, Train Loss : 3.905243, Valid Loss 7.708735\n",
      "Epoch : 828/1000, Train Loss : 3.586094, Valid Loss 7.712629\n",
      "Epoch : 829/1000, Train Loss : 3.542694, Valid Loss 8.236739\n",
      "Epoch : 830/1000, Train Loss : 3.480420, Valid Loss 7.411992\n",
      "Epoch : 831/1000, Train Loss : 4.609217, Valid Loss 8.248442\n",
      "Epoch : 832/1000, Train Loss : 4.828282, Valid Loss 7.714729\n",
      "Epoch : 833/1000, Train Loss : 4.013973, Valid Loss 7.784500\n",
      "Epoch : 834/1000, Train Loss : 4.071872, Valid Loss 7.631671\n",
      "Epoch : 835/1000, Train Loss : 3.952895, Valid Loss 7.830594\n",
      "Epoch : 836/1000, Train Loss : 3.601010, Valid Loss 7.836435\n",
      "Epoch : 837/1000, Train Loss : 3.371484, Valid Loss 7.786164\n",
      "Epoch : 838/1000, Train Loss : 3.733269, Valid Loss 7.861678\n",
      "Epoch : 839/1000, Train Loss : 3.654882, Valid Loss 8.006786\n",
      "Epoch : 840/1000, Train Loss : 3.616064, Valid Loss 7.705098\n",
      "Epoch : 841/1000, Train Loss : 4.150548, Valid Loss 8.877832\n",
      "Epoch : 842/1000, Train Loss : 4.011172, Valid Loss 8.296931\n",
      "Epoch : 843/1000, Train Loss : 3.528900, Valid Loss 7.584100\n",
      "Epoch : 844/1000, Train Loss : 3.453756, Valid Loss 7.425968\n",
      "Epoch : 845/1000, Train Loss : 3.406085, Valid Loss 7.497580\n",
      "Epoch : 846/1000, Train Loss : 3.539902, Valid Loss 7.826283\n",
      "Epoch : 847/1000, Train Loss : 4.668831, Valid Loss 11.215391\n",
      "Epoch : 848/1000, Train Loss : 4.660971, Valid Loss 7.601434\n",
      "Epoch : 849/1000, Train Loss : 3.588534, Valid Loss 7.491494\n",
      "Epoch : 850/1000, Train Loss : 3.705735, Valid Loss 7.801070\n",
      "Epoch : 851/1000, Train Loss : 3.442411, Valid Loss 8.600594\n",
      "Epoch : 852/1000, Train Loss : 4.026824, Valid Loss 7.873625\n",
      "Epoch : 853/1000, Train Loss : 3.664999, Valid Loss 7.797033\n",
      "Epoch : 854/1000, Train Loss : 3.368736, Valid Loss 7.787850\n",
      "Epoch : 855/1000, Train Loss : 3.445574, Valid Loss 8.542610\n",
      "Epoch : 856/1000, Train Loss : 3.733428, Valid Loss 7.986075\n",
      "Epoch : 857/1000, Train Loss : 3.704906, Valid Loss 7.517625\n",
      "Epoch : 858/1000, Train Loss : 4.217037, Valid Loss 8.356747\n",
      "Epoch : 859/1000, Train Loss : 3.924751, Valid Loss 7.644433\n",
      "Epoch : 860/1000, Train Loss : 3.373118, Valid Loss 7.796554\n",
      "Epoch : 861/1000, Train Loss : 3.585364, Valid Loss 7.954598\n",
      "Epoch : 862/1000, Train Loss : 3.724128, Valid Loss 7.849066\n",
      "Epoch : 863/1000, Train Loss : 3.809300, Valid Loss 7.388340\n",
      "Epoch : 864/1000, Train Loss : 3.610213, Valid Loss 8.463238\n",
      "Epoch : 865/1000, Train Loss : 3.439459, Valid Loss 7.986282\n",
      "Epoch : 866/1000, Train Loss : 3.383056, Valid Loss 7.334090\n",
      "Epoch : 867/1000, Train Loss : 3.232971, Valid Loss 8.080097\n",
      "Epoch : 868/1000, Train Loss : 3.400262, Valid Loss 8.526821\n",
      "Epoch : 869/1000, Train Loss : 3.869468, Valid Loss 7.484700\n",
      "Epoch : 870/1000, Train Loss : 3.415403, Valid Loss 7.377485\n",
      "Epoch : 871/1000, Train Loss : 3.429723, Valid Loss 8.126567\n",
      "Epoch : 872/1000, Train Loss : 3.774942, Valid Loss 7.796307\n",
      "Epoch : 873/1000, Train Loss : 3.443213, Valid Loss 7.564962\n",
      "Epoch : 874/1000, Train Loss : 3.644356, Valid Loss 8.683388\n",
      "Epoch : 875/1000, Train Loss : 3.519251, Valid Loss 8.671937\n",
      "Epoch : 876/1000, Train Loss : 3.410319, Valid Loss 7.496303\n",
      "Epoch : 877/1000, Train Loss : 3.259975, Valid Loss 7.426905\n",
      "Epoch : 878/1000, Train Loss : 3.423229, Valid Loss 7.579204\n",
      "Epoch : 879/1000, Train Loss : 3.504128, Valid Loss 7.370772\n",
      "Epoch : 880/1000, Train Loss : 3.612145, Valid Loss 9.246154\n",
      "Epoch : 881/1000, Train Loss : 4.094555, Valid Loss 8.210022\n",
      "Epoch : 882/1000, Train Loss : 3.702582, Valid Loss 7.738569\n",
      "Epoch : 883/1000, Train Loss : 3.468014, Valid Loss 7.599390\n",
      "Epoch : 884/1000, Train Loss : 3.351182, Valid Loss 7.300565\n",
      "Epoch : 885/1000, Train Loss : 3.483602, Valid Loss 7.616262\n",
      "Epoch : 886/1000, Train Loss : 3.675213, Valid Loss 10.983713\n",
      "Epoch : 887/1000, Train Loss : 4.223827, Valid Loss 7.872611\n",
      "Epoch : 888/1000, Train Loss : 3.434754, Valid Loss 8.627601\n",
      "Epoch : 889/1000, Train Loss : 4.525060, Valid Loss 8.437485\n",
      "Epoch : 890/1000, Train Loss : 4.228287, Valid Loss 8.690383\n",
      "Epoch : 891/1000, Train Loss : 3.784624, Valid Loss 7.528332\n",
      "Epoch : 892/1000, Train Loss : 3.321709, Valid Loss 7.758845\n",
      "Epoch : 893/1000, Train Loss : 3.298657, Valid Loss 7.425275\n",
      "Epoch : 894/1000, Train Loss : 3.128939, Valid Loss 7.490523\n",
      "Epoch : 895/1000, Train Loss : 3.213200, Valid Loss 7.108670\n",
      "Epoch : 896/1000, Train Loss : 3.247794, Valid Loss 7.856493\n",
      "Epoch : 897/1000, Train Loss : 3.234366, Valid Loss 7.549388\n",
      "Epoch : 898/1000, Train Loss : 3.270880, Valid Loss 7.331185\n",
      "Epoch : 899/1000, Train Loss : 3.252381, Valid Loss 7.675040\n",
      "Epoch : 900/1000, Train Loss : 4.413532, Valid Loss 8.369571\n",
      "Epoch : 901/1000, Train Loss : 4.021582, Valid Loss 7.708663\n",
      "Epoch : 902/1000, Train Loss : 3.236045, Valid Loss 7.574047\n",
      "Epoch : 903/1000, Train Loss : 3.348101, Valid Loss 7.665807\n",
      "Epoch : 904/1000, Train Loss : 3.313618, Valid Loss 8.051622\n",
      "Epoch : 905/1000, Train Loss : 3.792161, Valid Loss 7.417804\n",
      "Epoch : 906/1000, Train Loss : 4.500747, Valid Loss 7.403265\n",
      "Epoch : 907/1000, Train Loss : 3.498918, Valid Loss 7.246596\n",
      "Epoch : 908/1000, Train Loss : 3.225080, Valid Loss 7.203021\n",
      "Epoch : 909/1000, Train Loss : 3.226384, Valid Loss 9.306246\n",
      "Epoch : 910/1000, Train Loss : 3.685556, Valid Loss 7.571386\n",
      "Epoch : 911/1000, Train Loss : 3.244998, Valid Loss 7.301766\n",
      "Epoch : 912/1000, Train Loss : 3.124729, Valid Loss 7.949749\n",
      "Epoch : 913/1000, Train Loss : 3.080096, Valid Loss 8.000131\n",
      "Epoch : 914/1000, Train Loss : 3.066695, Valid Loss 7.794090\n",
      "Epoch : 915/1000, Train Loss : 3.243767, Valid Loss 7.375031\n",
      "Epoch : 916/1000, Train Loss : 3.534432, Valid Loss 8.119378\n",
      "Epoch : 917/1000, Train Loss : 3.361444, Valid Loss 7.641218\n",
      "Epoch : 918/1000, Train Loss : 3.309982, Valid Loss 9.273069\n",
      "Epoch : 919/1000, Train Loss : 4.123462, Valid Loss 7.818917\n",
      "Epoch : 920/1000, Train Loss : 3.942932, Valid Loss 7.644786\n",
      "Epoch : 921/1000, Train Loss : 3.289868, Valid Loss 8.728520\n",
      "Epoch : 922/1000, Train Loss : 3.797771, Valid Loss 8.749414\n",
      "Epoch : 923/1000, Train Loss : 3.439713, Valid Loss 7.369737\n",
      "Epoch : 924/1000, Train Loss : 3.478115, Valid Loss 8.047562\n",
      "Epoch : 925/1000, Train Loss : 3.375516, Valid Loss 8.916457\n",
      "Epoch : 926/1000, Train Loss : 4.184682, Valid Loss 8.877666\n",
      "Epoch : 927/1000, Train Loss : 4.371958, Valid Loss 7.937252\n",
      "Epoch : 928/1000, Train Loss : 3.197990, Valid Loss 7.584037\n",
      "Epoch : 929/1000, Train Loss : 3.211708, Valid Loss 10.289558\n",
      "Epoch : 930/1000, Train Loss : 3.893454, Valid Loss 7.199986\n",
      "Epoch : 931/1000, Train Loss : 3.196105, Valid Loss 7.482278\n",
      "Epoch : 932/1000, Train Loss : 2.969303, Valid Loss 7.539400\n",
      "Epoch : 933/1000, Train Loss : 4.139676, Valid Loss 7.720061\n",
      "Epoch : 934/1000, Train Loss : 3.370402, Valid Loss 8.349243\n",
      "Epoch : 935/1000, Train Loss : 3.240393, Valid Loss 7.495113\n",
      "Epoch : 936/1000, Train Loss : 3.309358, Valid Loss 7.954054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 937/1000, Train Loss : 2.934275, Valid Loss 7.051701\n",
      "Epoch : 938/1000, Train Loss : 3.003890, Valid Loss 7.562341\n",
      "Epoch : 939/1000, Train Loss : 3.297242, Valid Loss 7.363589\n",
      "Epoch : 940/1000, Train Loss : 3.097085, Valid Loss 7.292940\n",
      "Epoch : 941/1000, Train Loss : 3.011237, Valid Loss 7.556475\n",
      "Epoch : 942/1000, Train Loss : 2.950741, Valid Loss 7.680881\n",
      "Epoch : 943/1000, Train Loss : 3.345149, Valid Loss 7.649918\n",
      "Epoch : 944/1000, Train Loss : 3.020181, Valid Loss 7.617889\n",
      "Epoch : 945/1000, Train Loss : 3.001536, Valid Loss 7.345904\n",
      "Epoch : 946/1000, Train Loss : 2.997783, Valid Loss 7.056120\n",
      "Epoch : 947/1000, Train Loss : 2.972615, Valid Loss 7.246064\n",
      "Epoch : 948/1000, Train Loss : 2.818957, Valid Loss 7.482365\n",
      "Epoch : 949/1000, Train Loss : 2.999246, Valid Loss 7.297876\n",
      "Epoch : 950/1000, Train Loss : 2.932861, Valid Loss 7.638053\n",
      "Epoch : 951/1000, Train Loss : 3.005595, Valid Loss 7.516009\n",
      "Epoch : 952/1000, Train Loss : 2.983567, Valid Loss 7.789769\n",
      "Epoch : 953/1000, Train Loss : 3.120828, Valid Loss 7.522782\n",
      "Epoch : 954/1000, Train Loss : 3.590376, Valid Loss 7.502021\n",
      "Epoch : 955/1000, Train Loss : 2.918345, Valid Loss 7.666466\n",
      "Epoch : 956/1000, Train Loss : 3.584134, Valid Loss 7.908255\n",
      "Epoch : 957/1000, Train Loss : 3.226889, Valid Loss 7.437150\n",
      "Epoch : 958/1000, Train Loss : 3.204746, Valid Loss 8.097693\n",
      "Epoch : 959/1000, Train Loss : 3.656185, Valid Loss 8.988271\n",
      "Epoch : 960/1000, Train Loss : 3.595203, Valid Loss 8.420245\n",
      "Epoch : 961/1000, Train Loss : 3.443343, Valid Loss 8.326196\n",
      "Epoch : 962/1000, Train Loss : 2.822483, Valid Loss 7.385027\n",
      "Epoch : 963/1000, Train Loss : 3.263340, Valid Loss 8.144681\n",
      "Epoch : 964/1000, Train Loss : 3.228739, Valid Loss 7.957642\n",
      "Epoch : 965/1000, Train Loss : 3.216465, Valid Loss 7.809750\n",
      "Epoch : 966/1000, Train Loss : 3.111370, Valid Loss 7.955009\n",
      "Epoch : 967/1000, Train Loss : 3.102631, Valid Loss 7.040657\n",
      "Epoch : 968/1000, Train Loss : 3.745611, Valid Loss 7.963448\n",
      "Epoch : 969/1000, Train Loss : 3.353501, Valid Loss 7.107855\n",
      "Epoch : 970/1000, Train Loss : 3.012215, Valid Loss 7.844164\n",
      "Epoch : 971/1000, Train Loss : 3.190549, Valid Loss 7.319571\n",
      "Epoch : 972/1000, Train Loss : 2.816822, Valid Loss 7.151838\n",
      "Epoch : 973/1000, Train Loss : 2.860634, Valid Loss 7.270555\n",
      "Epoch : 974/1000, Train Loss : 4.274390, Valid Loss 9.734005\n",
      "Epoch : 975/1000, Train Loss : 3.834316, Valid Loss 7.412288\n",
      "Epoch : 976/1000, Train Loss : 2.989061, Valid Loss 7.198050\n",
      "Epoch : 977/1000, Train Loss : 2.849004, Valid Loss 6.938239\n",
      "Epoch : 978/1000, Train Loss : 2.840921, Valid Loss 7.872126\n",
      "Epoch : 979/1000, Train Loss : 3.373811, Valid Loss 7.434768\n",
      "Epoch : 980/1000, Train Loss : 2.910818, Valid Loss 7.515140\n",
      "Epoch : 981/1000, Train Loss : 3.499412, Valid Loss 8.826274\n",
      "Epoch : 982/1000, Train Loss : 3.739654, Valid Loss 7.282796\n",
      "Epoch : 983/1000, Train Loss : 3.663022, Valid Loss 7.245266\n",
      "Epoch : 984/1000, Train Loss : 2.908128, Valid Loss 7.397227\n",
      "Epoch : 985/1000, Train Loss : 2.833359, Valid Loss 7.045136\n",
      "Epoch : 986/1000, Train Loss : 2.737753, Valid Loss 7.033858\n",
      "Epoch : 987/1000, Train Loss : 3.103992, Valid Loss 7.550879\n",
      "Epoch : 988/1000, Train Loss : 2.745171, Valid Loss 7.213822\n",
      "Epoch : 989/1000, Train Loss : 2.807920, Valid Loss 7.547527\n",
      "Epoch : 990/1000, Train Loss : 2.632946, Valid Loss 7.324781\n",
      "Epoch : 991/1000, Train Loss : 3.123561, Valid Loss 7.112087\n",
      "Epoch : 992/1000, Train Loss : 2.988370, Valid Loss 7.376711\n",
      "Epoch : 993/1000, Train Loss : 2.797654, Valid Loss 7.132370\n",
      "Epoch : 994/1000, Train Loss : 2.635895, Valid Loss 7.187433\n",
      "Epoch : 995/1000, Train Loss : 2.840953, Valid Loss 7.373243\n",
      "Epoch : 996/1000, Train Loss : 2.689215, Valid Loss 7.255719\n",
      "Epoch : 997/1000, Train Loss : 3.190762, Valid Loss 7.230464\n",
      "Epoch : 998/1000, Train Loss : 3.031076, Valid Loss 7.388670\n",
      "Epoch : 999/1000, Train Loss : 2.885561, Valid Loss 7.514712\n",
      "Epoch : 1000/1000, Train Loss : 3.469408, Valid Loss 7.772133\n",
      "7.772132953008016\n"
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 100\n",
    "converge_cnt = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    val_epoch_loss.append(val_loss)\n",
    "    \n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 50:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "    \n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))\n",
    "print(min(val_epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01162acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ec1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a04e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b344b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce56a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81be170",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM_atn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc975cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model_atn = LSTM_atn(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_atn.parameters(), lr=learning_rate)\n",
    "n_epochs = 1000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9767a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/1000, Train Loss : 17660.966960, Valid Loss 17644.122721\n",
      "Epoch : 2/1000, Train Loss : 17266.199707, Valid Loss 16905.772461\n",
      "Epoch : 3/1000, Train Loss : 15816.544637, Valid Loss 14362.982259\n",
      "Epoch : 4/1000, Train Loss : 11815.631063, Valid Loss 8613.284180\n",
      "Epoch : 5/1000, Train Loss : 5051.536891, Valid Loss 1693.002950\n",
      "Epoch : 6/1000, Train Loss : 606.098094, Valid Loss 337.412420\n",
      "Epoch : 7/1000, Train Loss : 343.434059, Valid Loss 288.191589\n",
      "Epoch : 8/1000, Train Loss : 310.666709, Valid Loss 289.290138\n",
      "Epoch : 9/1000, Train Loss : 308.210809, Valid Loss 288.145910\n",
      "Epoch : 10/1000, Train Loss : 308.539772, Valid Loss 288.351435\n",
      "Epoch : 11/1000, Train Loss : 308.318309, Valid Loss 288.458148\n",
      "Epoch : 12/1000, Train Loss : 308.734711, Valid Loss 288.264303\n",
      "Epoch : 13/1000, Train Loss : 307.884874, Valid Loss 288.289459\n",
      "Epoch : 14/1000, Train Loss : 308.066785, Valid Loss 288.490784\n",
      "Epoch : 15/1000, Train Loss : 308.494948, Valid Loss 288.364845\n",
      "Epoch : 16/1000, Train Loss : 307.276866, Valid Loss 288.455429\n",
      "Epoch : 17/1000, Train Loss : 307.581749, Valid Loss 287.540532\n",
      "Epoch : 18/1000, Train Loss : 305.062653, Valid Loss 282.312332\n",
      "Epoch : 19/1000, Train Loss : 294.595431, Valid Loss 263.788811\n",
      "Epoch : 20/1000, Train Loss : 276.129316, Valid Loss 247.652837\n",
      "Epoch : 21/1000, Train Loss : 250.612825, Valid Loss 215.389008\n",
      "Epoch : 22/1000, Train Loss : 208.634253, Valid Loss 171.189634\n",
      "Epoch : 23/1000, Train Loss : 166.605473, Valid Loss 138.531590\n",
      "Epoch : 24/1000, Train Loss : 133.434826, Valid Loss 112.035820\n",
      "Epoch : 25/1000, Train Loss : 106.239213, Valid Loss 90.750468\n",
      "Epoch : 26/1000, Train Loss : 88.522891, Valid Loss 75.331450\n",
      "Epoch : 27/1000, Train Loss : 80.405424, Valid Loss 67.752833\n",
      "Epoch : 28/1000, Train Loss : 70.208543, Valid Loss 63.147416\n",
      "Epoch : 29/1000, Train Loss : 64.125014, Valid Loss 63.069360\n",
      "Epoch : 30/1000, Train Loss : 59.631169, Valid Loss 51.652545\n",
      "Epoch : 31/1000, Train Loss : 53.282129, Valid Loss 47.080912\n",
      "Epoch : 32/1000, Train Loss : 51.161039, Valid Loss 44.969226\n",
      "Epoch : 33/1000, Train Loss : 47.671316, Valid Loss 46.147624\n",
      "Epoch : 34/1000, Train Loss : 45.367707, Valid Loss 40.756236\n",
      "Epoch : 35/1000, Train Loss : 42.878054, Valid Loss 37.578783\n",
      "Epoch : 36/1000, Train Loss : 42.107470, Valid Loss 36.392535\n",
      "Epoch : 37/1000, Train Loss : 40.207688, Valid Loss 39.268981\n",
      "Epoch : 38/1000, Train Loss : 40.094989, Valid Loss 33.555031\n",
      "Epoch : 39/1000, Train Loss : 36.438908, Valid Loss 31.612412\n",
      "Epoch : 40/1000, Train Loss : 38.474971, Valid Loss 32.287492\n",
      "Epoch : 41/1000, Train Loss : 34.039616, Valid Loss 30.086678\n",
      "Epoch : 42/1000, Train Loss : 33.147199, Valid Loss 30.280798\n",
      "Epoch : 43/1000, Train Loss : 32.967798, Valid Loss 28.233656\n",
      "Epoch : 44/1000, Train Loss : 31.024153, Valid Loss 26.956882\n",
      "Epoch : 45/1000, Train Loss : 30.165055, Valid Loss 27.031520\n",
      "Epoch : 46/1000, Train Loss : 32.701219, Valid Loss 27.992500\n",
      "Epoch : 47/1000, Train Loss : 31.544888, Valid Loss 34.267164\n",
      "Epoch : 48/1000, Train Loss : 32.249182, Valid Loss 25.809827\n",
      "Epoch : 49/1000, Train Loss : 29.408710, Valid Loss 28.303036\n",
      "Epoch : 50/1000, Train Loss : 30.159018, Valid Loss 24.515136\n",
      "Epoch : 51/1000, Train Loss : 27.605820, Valid Loss 24.084107\n",
      "Epoch : 52/1000, Train Loss : 27.068004, Valid Loss 23.310031\n",
      "Epoch : 53/1000, Train Loss : 27.401395, Valid Loss 26.911913\n",
      "Epoch : 54/1000, Train Loss : 30.780178, Valid Loss 25.615941\n",
      "Epoch : 55/1000, Train Loss : 26.865104, Valid Loss 23.259811\n",
      "Epoch : 56/1000, Train Loss : 26.091641, Valid Loss 29.826603\n",
      "Epoch : 57/1000, Train Loss : 26.306807, Valid Loss 23.523971\n",
      "Epoch : 58/1000, Train Loss : 27.544502, Valid Loss 22.187564\n",
      "Epoch : 59/1000, Train Loss : 25.569086, Valid Loss 23.944137\n",
      "Epoch : 60/1000, Train Loss : 26.467290, Valid Loss 24.854124\n",
      "Epoch : 61/1000, Train Loss : 25.847044, Valid Loss 23.082709\n",
      "Epoch : 62/1000, Train Loss : 24.156490, Valid Loss 21.424997\n",
      "Epoch : 63/1000, Train Loss : 26.032650, Valid Loss 20.939948\n",
      "Epoch : 64/1000, Train Loss : 24.106908, Valid Loss 24.780574\n",
      "Epoch : 65/1000, Train Loss : 23.491865, Valid Loss 22.313033\n",
      "Epoch : 66/1000, Train Loss : 26.035904, Valid Loss 26.810025\n",
      "Epoch : 67/1000, Train Loss : 23.777395, Valid Loss 20.216941\n",
      "Epoch : 68/1000, Train Loss : 22.797594, Valid Loss 19.412180\n",
      "Epoch : 69/1000, Train Loss : 22.114328, Valid Loss 21.435959\n",
      "Epoch : 70/1000, Train Loss : 22.340708, Valid Loss 19.883396\n",
      "Epoch : 71/1000, Train Loss : 22.905411, Valid Loss 20.118319\n",
      "Epoch : 72/1000, Train Loss : 22.612214, Valid Loss 20.225528\n",
      "Epoch : 73/1000, Train Loss : 23.084712, Valid Loss 20.141558\n",
      "Epoch : 74/1000, Train Loss : 21.749922, Valid Loss 18.733934\n",
      "Epoch : 75/1000, Train Loss : 20.665588, Valid Loss 22.318134\n",
      "Epoch : 76/1000, Train Loss : 21.978842, Valid Loss 18.419274\n",
      "Epoch : 77/1000, Train Loss : 23.487723, Valid Loss 18.303857\n",
      "Epoch : 78/1000, Train Loss : 21.792817, Valid Loss 19.023798\n",
      "Epoch : 79/1000, Train Loss : 22.073934, Valid Loss 21.586941\n",
      "Epoch : 80/1000, Train Loss : 21.455602, Valid Loss 22.052141\n",
      "Epoch : 81/1000, Train Loss : 20.886452, Valid Loss 19.078574\n",
      "Epoch : 82/1000, Train Loss : 19.871015, Valid Loss 19.757605\n",
      "Epoch : 83/1000, Train Loss : 22.127669, Valid Loss 24.122152\n",
      "Epoch : 84/1000, Train Loss : 21.187658, Valid Loss 18.716122\n",
      "Epoch : 85/1000, Train Loss : 21.516910, Valid Loss 22.869972\n",
      "Epoch : 86/1000, Train Loss : 20.521562, Valid Loss 17.338244\n",
      "Epoch : 87/1000, Train Loss : 19.414445, Valid Loss 17.531416\n",
      "Epoch : 88/1000, Train Loss : 18.863286, Valid Loss 17.004426\n",
      "Epoch : 89/1000, Train Loss : 22.279938, Valid Loss 32.215694\n",
      "Epoch : 90/1000, Train Loss : 26.438037, Valid Loss 19.016204\n",
      "Epoch : 91/1000, Train Loss : 19.659953, Valid Loss 17.141754\n",
      "Epoch : 92/1000, Train Loss : 18.856760, Valid Loss 17.309835\n",
      "Epoch : 93/1000, Train Loss : 18.710794, Valid Loss 20.310187\n",
      "Epoch : 94/1000, Train Loss : 20.508865, Valid Loss 21.070159\n",
      "Epoch : 95/1000, Train Loss : 18.700946, Valid Loss 17.652880\n",
      "Epoch : 96/1000, Train Loss : 18.564503, Valid Loss 16.842806\n",
      "Epoch : 97/1000, Train Loss : 18.343600, Valid Loss 17.939730\n",
      "Epoch : 98/1000, Train Loss : 18.109088, Valid Loss 16.264043\n",
      "Epoch : 99/1000, Train Loss : 18.250344, Valid Loss 16.869585\n",
      "Epoch : 100/1000, Train Loss : 18.638274, Valid Loss 19.048289\n",
      "Epoch : 101/1000, Train Loss : 18.924298, Valid Loss 16.934911\n",
      "Epoch : 102/1000, Train Loss : 17.801063, Valid Loss 15.982772\n",
      "Epoch : 103/1000, Train Loss : 17.430718, Valid Loss 23.456811\n",
      "Epoch : 104/1000, Train Loss : 18.634173, Valid Loss 17.893508\n",
      "Epoch : 105/1000, Train Loss : 19.079212, Valid Loss 17.966364\n",
      "Epoch : 106/1000, Train Loss : 19.873860, Valid Loss 16.378821\n",
      "Epoch : 107/1000, Train Loss : 17.446636, Valid Loss 15.978452\n",
      "Epoch : 108/1000, Train Loss : 17.353829, Valid Loss 18.634969\n",
      "Epoch : 109/1000, Train Loss : 17.943659, Valid Loss 15.670993\n",
      "Epoch : 110/1000, Train Loss : 18.492566, Valid Loss 16.602417\n",
      "Epoch : 111/1000, Train Loss : 17.388039, Valid Loss 19.089646\n",
      "Epoch : 112/1000, Train Loss : 18.887139, Valid Loss 17.012014\n",
      "Epoch : 113/1000, Train Loss : 20.807955, Valid Loss 15.846782\n",
      "Epoch : 114/1000, Train Loss : 19.380501, Valid Loss 19.321583\n",
      "Epoch : 115/1000, Train Loss : 16.932781, Valid Loss 15.509339\n",
      "Epoch : 116/1000, Train Loss : 16.517083, Valid Loss 16.608727\n",
      "Epoch : 117/1000, Train Loss : 17.012590, Valid Loss 18.035260\n",
      "Epoch : 118/1000, Train Loss : 21.171400, Valid Loss 20.054763\n",
      "Epoch : 119/1000, Train Loss : 17.606040, Valid Loss 15.839662\n",
      "Epoch : 120/1000, Train Loss : 15.863773, Valid Loss 14.826318\n",
      "Epoch : 121/1000, Train Loss : 16.234962, Valid Loss 14.940590\n",
      "Epoch : 122/1000, Train Loss : 16.838169, Valid Loss 15.104710\n",
      "Epoch : 123/1000, Train Loss : 15.776051, Valid Loss 14.905208\n",
      "Epoch : 124/1000, Train Loss : 16.564751, Valid Loss 15.115355\n",
      "Epoch : 125/1000, Train Loss : 15.402274, Valid Loss 15.925574\n",
      "Epoch : 126/1000, Train Loss : 15.959948, Valid Loss 14.626154\n",
      "Epoch : 127/1000, Train Loss : 16.045472, Valid Loss 14.863644\n",
      "Epoch : 128/1000, Train Loss : 15.970366, Valid Loss 14.636612\n",
      "Epoch : 129/1000, Train Loss : 15.217608, Valid Loss 15.635103\n",
      "Epoch : 130/1000, Train Loss : 16.538877, Valid Loss 17.834514\n",
      "Epoch : 131/1000, Train Loss : 17.667328, Valid Loss 14.807589\n",
      "Epoch : 132/1000, Train Loss : 15.174792, Valid Loss 14.397424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 133/1000, Train Loss : 16.157513, Valid Loss 24.262449\n",
      "Epoch : 134/1000, Train Loss : 16.423825, Valid Loss 15.805951\n",
      "Epoch : 135/1000, Train Loss : 16.224046, Valid Loss 15.763993\n",
      "Epoch : 136/1000, Train Loss : 14.941618, Valid Loss 14.211970\n",
      "Epoch : 137/1000, Train Loss : 15.516154, Valid Loss 19.083103\n",
      "Epoch : 138/1000, Train Loss : 15.880020, Valid Loss 14.693691\n",
      "Epoch : 139/1000, Train Loss : 16.284811, Valid Loss 14.384340\n",
      "Epoch : 140/1000, Train Loss : 14.552223, Valid Loss 14.212931\n",
      "Epoch : 141/1000, Train Loss : 14.935271, Valid Loss 14.922416\n",
      "Epoch : 142/1000, Train Loss : 14.649582, Valid Loss 13.645583\n",
      "Epoch : 143/1000, Train Loss : 14.769726, Valid Loss 15.059640\n",
      "Epoch : 144/1000, Train Loss : 14.571427, Valid Loss 14.010738\n",
      "Epoch : 145/1000, Train Loss : 15.577603, Valid Loss 15.435946\n",
      "Epoch : 146/1000, Train Loss : 14.467370, Valid Loss 15.065392\n",
      "Epoch : 147/1000, Train Loss : 15.282425, Valid Loss 19.701740\n",
      "Epoch : 148/1000, Train Loss : 15.817983, Valid Loss 13.682556\n",
      "Epoch : 149/1000, Train Loss : 15.307589, Valid Loss 18.745412\n",
      "Epoch : 150/1000, Train Loss : 14.487084, Valid Loss 13.596344\n",
      "Epoch : 151/1000, Train Loss : 14.507317, Valid Loss 13.891367\n",
      "Epoch : 152/1000, Train Loss : 14.329272, Valid Loss 13.882348\n",
      "Epoch : 153/1000, Train Loss : 14.325962, Valid Loss 15.124974\n",
      "Epoch : 154/1000, Train Loss : 14.417241, Valid Loss 14.302336\n",
      "Epoch : 155/1000, Train Loss : 14.943909, Valid Loss 16.442097\n",
      "Epoch : 156/1000, Train Loss : 15.009836, Valid Loss 14.885045\n",
      "Epoch : 157/1000, Train Loss : 13.820473, Valid Loss 14.393237\n",
      "Epoch : 158/1000, Train Loss : 16.338517, Valid Loss 19.848942\n",
      "Epoch : 159/1000, Train Loss : 15.446178, Valid Loss 14.124063\n",
      "Epoch : 160/1000, Train Loss : 13.493436, Valid Loss 13.405566\n",
      "Epoch : 161/1000, Train Loss : 13.602386, Valid Loss 13.394782\n",
      "Epoch : 162/1000, Train Loss : 13.783480, Valid Loss 14.451426\n",
      "Epoch : 163/1000, Train Loss : 15.802941, Valid Loss 14.755303\n",
      "Epoch : 164/1000, Train Loss : 16.714952, Valid Loss 14.550064\n",
      "Epoch : 165/1000, Train Loss : 13.723028, Valid Loss 14.147596\n",
      "Epoch : 166/1000, Train Loss : 13.403003, Valid Loss 13.012711\n",
      "Epoch : 167/1000, Train Loss : 13.907128, Valid Loss 15.837908\n",
      "Epoch : 168/1000, Train Loss : 14.255979, Valid Loss 13.084847\n",
      "Epoch : 169/1000, Train Loss : 13.131063, Valid Loss 13.069802\n",
      "Epoch : 170/1000, Train Loss : 12.843133, Valid Loss 13.444436\n",
      "Epoch : 171/1000, Train Loss : 13.239491, Valid Loss 12.998636\n",
      "Epoch : 172/1000, Train Loss : 14.980405, Valid Loss 21.110566\n",
      "Epoch : 173/1000, Train Loss : 14.417919, Valid Loss 12.807257\n",
      "Epoch : 174/1000, Train Loss : 12.929390, Valid Loss 13.285801\n",
      "Epoch : 175/1000, Train Loss : 13.411803, Valid Loss 15.146040\n",
      "Epoch : 176/1000, Train Loss : 13.476614, Valid Loss 13.060288\n",
      "Epoch : 177/1000, Train Loss : 13.102399, Valid Loss 14.310097\n",
      "Epoch : 178/1000, Train Loss : 13.560720, Valid Loss 12.801634\n",
      "Epoch : 179/1000, Train Loss : 12.459840, Valid Loss 13.360026\n",
      "Epoch : 180/1000, Train Loss : 12.920230, Valid Loss 12.780008\n",
      "Epoch : 181/1000, Train Loss : 16.544884, Valid Loss 19.491048\n",
      "Epoch : 182/1000, Train Loss : 15.977023, Valid Loss 16.227382\n",
      "Epoch : 183/1000, Train Loss : 12.452722, Valid Loss 12.635728\n",
      "Epoch : 184/1000, Train Loss : 13.165100, Valid Loss 16.342251\n",
      "Epoch : 185/1000, Train Loss : 15.230301, Valid Loss 16.010130\n",
      "Epoch : 186/1000, Train Loss : 16.068341, Valid Loss 14.726946\n",
      "Epoch : 187/1000, Train Loss : 13.700985, Valid Loss 13.146899\n",
      "Epoch : 188/1000, Train Loss : 12.765306, Valid Loss 13.521167\n",
      "Epoch : 189/1000, Train Loss : 12.703680, Valid Loss 12.759219\n",
      "Epoch : 190/1000, Train Loss : 12.290424, Valid Loss 14.062226\n",
      "Epoch : 191/1000, Train Loss : 13.168808, Valid Loss 13.427694\n",
      "Epoch : 192/1000, Train Loss : 12.765581, Valid Loss 12.579108\n",
      "Epoch : 193/1000, Train Loss : 13.620455, Valid Loss 13.644618\n",
      "Epoch : 194/1000, Train Loss : 13.454502, Valid Loss 12.355228\n",
      "Epoch : 195/1000, Train Loss : 11.801907, Valid Loss 14.666592\n",
      "Epoch : 196/1000, Train Loss : 14.295027, Valid Loss 12.765277\n",
      "Epoch : 197/1000, Train Loss : 12.746606, Valid Loss 12.292609\n",
      "Epoch : 198/1000, Train Loss : 11.987876, Valid Loss 12.980274\n",
      "Epoch : 199/1000, Train Loss : 11.749753, Valid Loss 12.506059\n",
      "Epoch : 200/1000, Train Loss : 11.606802, Valid Loss 12.459709\n",
      "Epoch : 201/1000, Train Loss : 11.938144, Valid Loss 12.701282\n",
      "Epoch : 202/1000, Train Loss : 12.548662, Valid Loss 13.162812\n",
      "Epoch : 203/1000, Train Loss : 12.231122, Valid Loss 15.700507\n",
      "Epoch : 204/1000, Train Loss : 12.956213, Valid Loss 13.427283\n",
      "Epoch : 205/1000, Train Loss : 12.169479, Valid Loss 12.113846\n",
      "Epoch : 206/1000, Train Loss : 11.714975, Valid Loss 12.070125\n",
      "Epoch : 207/1000, Train Loss : 11.727328, Valid Loss 12.556601\n",
      "Epoch : 208/1000, Train Loss : 12.566108, Valid Loss 12.342269\n",
      "Epoch : 209/1000, Train Loss : 14.964420, Valid Loss 15.573581\n",
      "Epoch : 210/1000, Train Loss : 13.203908, Valid Loss 11.954114\n",
      "Epoch : 211/1000, Train Loss : 15.657463, Valid Loss 14.626127\n",
      "Epoch : 212/1000, Train Loss : 13.343194, Valid Loss 12.411887\n",
      "Epoch : 213/1000, Train Loss : 11.310407, Valid Loss 12.156674\n",
      "Epoch : 214/1000, Train Loss : 12.689103, Valid Loss 12.009380\n",
      "Epoch : 215/1000, Train Loss : 11.269828, Valid Loss 12.049176\n",
      "Epoch : 216/1000, Train Loss : 11.639865, Valid Loss 16.102564\n",
      "Epoch : 217/1000, Train Loss : 14.256759, Valid Loss 12.627955\n",
      "Epoch : 218/1000, Train Loss : 12.567034, Valid Loss 11.839997\n",
      "Epoch : 219/1000, Train Loss : 11.947404, Valid Loss 12.054194\n",
      "Epoch : 220/1000, Train Loss : 11.462432, Valid Loss 11.822810\n",
      "Epoch : 221/1000, Train Loss : 10.917009, Valid Loss 11.690682\n",
      "Epoch : 222/1000, Train Loss : 10.940023, Valid Loss 13.519251\n",
      "Epoch : 223/1000, Train Loss : 12.059251, Valid Loss 12.037955\n",
      "Epoch : 224/1000, Train Loss : 11.300591, Valid Loss 13.267711\n",
      "Epoch : 225/1000, Train Loss : 11.890622, Valid Loss 12.427943\n",
      "Epoch : 226/1000, Train Loss : 12.212112, Valid Loss 12.511784\n",
      "Epoch : 227/1000, Train Loss : 11.885912, Valid Loss 11.512786\n",
      "Epoch : 228/1000, Train Loss : 11.135732, Valid Loss 12.290658\n",
      "Epoch : 229/1000, Train Loss : 11.341492, Valid Loss 12.691529\n",
      "Epoch : 230/1000, Train Loss : 11.087452, Valid Loss 12.246293\n",
      "Epoch : 231/1000, Train Loss : 11.371678, Valid Loss 12.033384\n",
      "Epoch : 232/1000, Train Loss : 11.329090, Valid Loss 11.641555\n",
      "Epoch : 233/1000, Train Loss : 10.525035, Valid Loss 14.484324\n",
      "Epoch : 234/1000, Train Loss : 11.809977, Valid Loss 12.044774\n",
      "Epoch : 235/1000, Train Loss : 10.989739, Valid Loss 11.664828\n",
      "Epoch : 236/1000, Train Loss : 11.497676, Valid Loss 17.137513\n",
      "Epoch : 237/1000, Train Loss : 13.028641, Valid Loss 11.484382\n",
      "Epoch : 238/1000, Train Loss : 10.940497, Valid Loss 11.492544\n",
      "Epoch : 239/1000, Train Loss : 11.519177, Valid Loss 17.301134\n",
      "Epoch : 240/1000, Train Loss : 13.358005, Valid Loss 13.264606\n",
      "Epoch : 241/1000, Train Loss : 12.674742, Valid Loss 11.475803\n",
      "Epoch : 242/1000, Train Loss : 10.444780, Valid Loss 12.541196\n",
      "Epoch : 243/1000, Train Loss : 10.283337, Valid Loss 11.973406\n",
      "Epoch : 244/1000, Train Loss : 10.316077, Valid Loss 11.383186\n",
      "Epoch : 245/1000, Train Loss : 10.435235, Valid Loss 10.986611\n",
      "Epoch : 246/1000, Train Loss : 10.585980, Valid Loss 11.841751\n",
      "Epoch : 247/1000, Train Loss : 10.397705, Valid Loss 11.587576\n",
      "Epoch : 248/1000, Train Loss : 10.753947, Valid Loss 12.551381\n",
      "Epoch : 249/1000, Train Loss : 11.193882, Valid Loss 15.850851\n",
      "Epoch : 250/1000, Train Loss : 12.246603, Valid Loss 11.943421\n",
      "Epoch : 251/1000, Train Loss : 10.549831, Valid Loss 11.157967\n",
      "Epoch : 252/1000, Train Loss : 11.564123, Valid Loss 13.053070\n",
      "Epoch : 253/1000, Train Loss : 12.230862, Valid Loss 11.313699\n",
      "Epoch : 254/1000, Train Loss : 10.977295, Valid Loss 11.063636\n",
      "Epoch : 255/1000, Train Loss : 11.280385, Valid Loss 12.760369\n",
      "Epoch : 256/1000, Train Loss : 10.900743, Valid Loss 14.941520\n",
      "Epoch : 257/1000, Train Loss : 10.766819, Valid Loss 11.802550\n",
      "Epoch : 258/1000, Train Loss : 10.970606, Valid Loss 11.994705\n",
      "Epoch : 259/1000, Train Loss : 10.351533, Valid Loss 11.795631\n",
      "Epoch : 260/1000, Train Loss : 10.066861, Valid Loss 11.317755\n",
      "Epoch : 261/1000, Train Loss : 10.586161, Valid Loss 10.749620\n",
      "Epoch : 262/1000, Train Loss : 10.003889, Valid Loss 11.261645\n",
      "Epoch : 263/1000, Train Loss : 9.722690, Valid Loss 11.477369\n",
      "Epoch : 264/1000, Train Loss : 10.221115, Valid Loss 11.202044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 265/1000, Train Loss : 11.668904, Valid Loss 14.562171\n",
      "Epoch : 266/1000, Train Loss : 11.487328, Valid Loss 11.058732\n",
      "Epoch : 267/1000, Train Loss : 9.902067, Valid Loss 11.031490\n",
      "Epoch : 268/1000, Train Loss : 9.904469, Valid Loss 11.905921\n",
      "Epoch : 269/1000, Train Loss : 10.333526, Valid Loss 12.039819\n",
      "Epoch : 270/1000, Train Loss : 10.879412, Valid Loss 11.227145\n",
      "Epoch : 271/1000, Train Loss : 10.723051, Valid Loss 10.985297\n",
      "Epoch : 272/1000, Train Loss : 10.039472, Valid Loss 11.750648\n",
      "Epoch : 273/1000, Train Loss : 10.438324, Valid Loss 11.112342\n",
      "Epoch : 274/1000, Train Loss : 10.622888, Valid Loss 11.247583\n",
      "Epoch : 275/1000, Train Loss : 9.543184, Valid Loss 10.869694\n",
      "Epoch : 276/1000, Train Loss : 10.145164, Valid Loss 12.027805\n",
      "Epoch : 277/1000, Train Loss : 10.278276, Valid Loss 11.809650\n",
      "Epoch : 278/1000, Train Loss : 9.971650, Valid Loss 11.072384\n",
      "Epoch : 279/1000, Train Loss : 9.870681, Valid Loss 11.041224\n",
      "Epoch : 280/1000, Train Loss : 9.528111, Valid Loss 10.808817\n",
      "Epoch : 281/1000, Train Loss : 9.379328, Valid Loss 10.957421\n",
      "Epoch : 282/1000, Train Loss : 9.985357, Valid Loss 11.560571\n",
      "Epoch : 283/1000, Train Loss : 10.768260, Valid Loss 10.906378\n",
      "Epoch : 284/1000, Train Loss : 9.519685, Valid Loss 10.884156\n",
      "Epoch : 285/1000, Train Loss : 10.348468, Valid Loss 16.286503\n",
      "Epoch : 286/1000, Train Loss : 10.934913, Valid Loss 10.931056\n",
      "Epoch : 287/1000, Train Loss : 11.511283, Valid Loss 17.629039\n",
      "Epoch : 288/1000, Train Loss : 11.923437, Valid Loss 15.361407\n",
      "Epoch : 289/1000, Train Loss : 9.766746, Valid Loss 10.743335\n",
      "Epoch : 290/1000, Train Loss : 9.251076, Valid Loss 10.601240\n",
      "Epoch : 291/1000, Train Loss : 9.503241, Valid Loss 12.491820\n",
      "Epoch : 292/1000, Train Loss : 9.588271, Valid Loss 10.778618\n",
      "Epoch : 293/1000, Train Loss : 9.048051, Valid Loss 10.589610\n",
      "Epoch : 294/1000, Train Loss : 10.102456, Valid Loss 13.073956\n",
      "Epoch : 295/1000, Train Loss : 9.115994, Valid Loss 10.914267\n",
      "Epoch : 296/1000, Train Loss : 9.219051, Valid Loss 10.427313\n",
      "Epoch : 297/1000, Train Loss : 9.327767, Valid Loss 10.506998\n",
      "Epoch : 298/1000, Train Loss : 9.086682, Valid Loss 10.418147\n",
      "Epoch : 299/1000, Train Loss : 9.147954, Valid Loss 10.583367\n",
      "Epoch : 300/1000, Train Loss : 9.108469, Valid Loss 10.449064\n",
      "Epoch : 301/1000, Train Loss : 9.523386, Valid Loss 11.292716\n",
      "Epoch : 302/1000, Train Loss : 9.109357, Valid Loss 10.949743\n",
      "Epoch : 303/1000, Train Loss : 8.992713, Valid Loss 13.565994\n",
      "Epoch : 304/1000, Train Loss : 11.348963, Valid Loss 11.853972\n",
      "Epoch : 305/1000, Train Loss : 9.934393, Valid Loss 10.434110\n",
      "Epoch : 306/1000, Train Loss : 8.795250, Valid Loss 10.968764\n",
      "Epoch : 307/1000, Train Loss : 9.822886, Valid Loss 14.819485\n",
      "Epoch : 308/1000, Train Loss : 9.337259, Valid Loss 11.100125\n",
      "Epoch : 309/1000, Train Loss : 10.129903, Valid Loss 10.350062\n",
      "Epoch : 310/1000, Train Loss : 8.464552, Valid Loss 10.082218\n",
      "Epoch : 311/1000, Train Loss : 9.042477, Valid Loss 10.139190\n",
      "Epoch : 312/1000, Train Loss : 9.236382, Valid Loss 10.433516\n",
      "Epoch : 313/1000, Train Loss : 10.122295, Valid Loss 11.417905\n",
      "Epoch : 314/1000, Train Loss : 9.433883, Valid Loss 11.976901\n",
      "Epoch : 315/1000, Train Loss : 8.759066, Valid Loss 11.798022\n",
      "Epoch : 316/1000, Train Loss : 9.567720, Valid Loss 17.369803\n",
      "Epoch : 317/1000, Train Loss : 9.610566, Valid Loss 10.304076\n",
      "Epoch : 318/1000, Train Loss : 9.136434, Valid Loss 13.305962\n",
      "Epoch : 319/1000, Train Loss : 9.542523, Valid Loss 11.744319\n",
      "Epoch : 320/1000, Train Loss : 9.118755, Valid Loss 10.194282\n",
      "Epoch : 321/1000, Train Loss : 8.580492, Valid Loss 10.419170\n",
      "Epoch : 322/1000, Train Loss : 8.390780, Valid Loss 9.982461\n",
      "Epoch : 323/1000, Train Loss : 8.854360, Valid Loss 10.429335\n",
      "Epoch : 324/1000, Train Loss : 8.763032, Valid Loss 12.945939\n",
      "Epoch : 325/1000, Train Loss : 9.073770, Valid Loss 10.238770\n",
      "Epoch : 326/1000, Train Loss : 9.338477, Valid Loss 10.517130\n",
      "Epoch : 327/1000, Train Loss : 8.470378, Valid Loss 11.227051\n",
      "Epoch : 328/1000, Train Loss : 8.453714, Valid Loss 10.204414\n",
      "Epoch : 329/1000, Train Loss : 8.619747, Valid Loss 10.283478\n",
      "Epoch : 330/1000, Train Loss : 8.381447, Valid Loss 10.077978\n",
      "Epoch : 331/1000, Train Loss : 8.754939, Valid Loss 10.802009\n",
      "Epoch : 332/1000, Train Loss : 11.103534, Valid Loss 10.738968\n",
      "Epoch : 333/1000, Train Loss : 9.455824, Valid Loss 10.529771\n",
      "Epoch : 334/1000, Train Loss : 8.595309, Valid Loss 10.334772\n",
      "Epoch : 335/1000, Train Loss : 8.465040, Valid Loss 11.863469\n",
      "Epoch : 336/1000, Train Loss : 8.415555, Valid Loss 10.077004\n",
      "Epoch : 337/1000, Train Loss : 9.341330, Valid Loss 11.141253\n",
      "Epoch : 338/1000, Train Loss : 8.608104, Valid Loss 10.760721\n",
      "Epoch : 339/1000, Train Loss : 8.558428, Valid Loss 10.610046\n",
      "Epoch : 340/1000, Train Loss : 8.947336, Valid Loss 14.675127\n",
      "Epoch : 341/1000, Train Loss : 9.052068, Valid Loss 9.948070\n",
      "Epoch : 342/1000, Train Loss : 8.355577, Valid Loss 9.899138\n",
      "Epoch : 343/1000, Train Loss : 8.153976, Valid Loss 10.358379\n",
      "Epoch : 344/1000, Train Loss : 8.280366, Valid Loss 12.138405\n",
      "Epoch : 345/1000, Train Loss : 8.581047, Valid Loss 11.478678\n",
      "Epoch : 346/1000, Train Loss : 8.385224, Valid Loss 9.652852\n",
      "Epoch : 347/1000, Train Loss : 7.598323, Valid Loss 10.114627\n",
      "Epoch : 348/1000, Train Loss : 7.713903, Valid Loss 10.124375\n",
      "Epoch : 349/1000, Train Loss : 8.152262, Valid Loss 10.475136\n",
      "Epoch : 350/1000, Train Loss : 8.301933, Valid Loss 10.121587\n",
      "Epoch : 351/1000, Train Loss : 8.099037, Valid Loss 9.795570\n",
      "Epoch : 352/1000, Train Loss : 8.280053, Valid Loss 12.374136\n",
      "Epoch : 353/1000, Train Loss : 10.783429, Valid Loss 14.382806\n",
      "Epoch : 354/1000, Train Loss : 9.313341, Valid Loss 10.764820\n",
      "Epoch : 355/1000, Train Loss : 7.840668, Valid Loss 10.261443\n",
      "Epoch : 356/1000, Train Loss : 9.138473, Valid Loss 9.691613\n",
      "Epoch : 357/1000, Train Loss : 9.563200, Valid Loss 15.555407\n",
      "Epoch : 358/1000, Train Loss : 9.320946, Valid Loss 10.170348\n",
      "Epoch : 359/1000, Train Loss : 8.758899, Valid Loss 11.636976\n",
      "Epoch : 360/1000, Train Loss : 8.114598, Valid Loss 9.785937\n",
      "Epoch : 361/1000, Train Loss : 7.965036, Valid Loss 12.255921\n",
      "Epoch : 362/1000, Train Loss : 11.760716, Valid Loss 10.716991\n",
      "Epoch : 363/1000, Train Loss : 8.879995, Valid Loss 10.177111\n",
      "Epoch : 364/1000, Train Loss : 7.806341, Valid Loss 12.734744\n",
      "Epoch : 365/1000, Train Loss : 9.168762, Valid Loss 10.478862\n",
      "Epoch : 366/1000, Train Loss : 8.045963, Valid Loss 10.062686\n",
      "Epoch : 367/1000, Train Loss : 8.141965, Valid Loss 11.315174\n",
      "Epoch : 368/1000, Train Loss : 7.818092, Valid Loss 9.979705\n",
      "Epoch : 369/1000, Train Loss : 7.798266, Valid Loss 9.881723\n",
      "Epoch : 370/1000, Train Loss : 7.903818, Valid Loss 10.333369\n",
      "Epoch : 371/1000, Train Loss : 8.585960, Valid Loss 10.067211\n",
      "Epoch : 372/1000, Train Loss : 9.234752, Valid Loss 10.495265\n",
      "Epoch : 373/1000, Train Loss : 8.423065, Valid Loss 11.426484\n",
      "Epoch : 374/1000, Train Loss : 8.018880, Valid Loss 11.758383\n",
      "Epoch : 375/1000, Train Loss : 7.724771, Valid Loss 9.303727\n",
      "Epoch : 376/1000, Train Loss : 7.093600, Valid Loss 9.730374\n",
      "Epoch : 377/1000, Train Loss : 7.581016, Valid Loss 9.331065\n",
      "Epoch : 378/1000, Train Loss : 7.424710, Valid Loss 10.264901\n",
      "Epoch : 379/1000, Train Loss : 7.581132, Valid Loss 11.421152\n",
      "Epoch : 380/1000, Train Loss : 7.571926, Valid Loss 9.965718\n",
      "Epoch : 381/1000, Train Loss : 7.817941, Valid Loss 11.907806\n",
      "Epoch : 382/1000, Train Loss : 8.714352, Valid Loss 10.112926\n",
      "Epoch : 383/1000, Train Loss : 7.532960, Valid Loss 10.925461\n",
      "Epoch : 384/1000, Train Loss : 7.910887, Valid Loss 9.777026\n",
      "Epoch : 385/1000, Train Loss : 7.582622, Valid Loss 9.417508\n",
      "Epoch : 386/1000, Train Loss : 8.423370, Valid Loss 11.445614\n",
      "Epoch : 387/1000, Train Loss : 9.568002, Valid Loss 10.076898\n",
      "Epoch : 388/1000, Train Loss : 7.644488, Valid Loss 9.582156\n",
      "Epoch : 389/1000, Train Loss : 7.751378, Valid Loss 9.718606\n",
      "Epoch : 390/1000, Train Loss : 7.255768, Valid Loss 10.003377\n",
      "Epoch : 391/1000, Train Loss : 8.018733, Valid Loss 10.813748\n",
      "Epoch : 392/1000, Train Loss : 9.243719, Valid Loss 12.989810\n",
      "Epoch : 393/1000, Train Loss : 9.389191, Valid Loss 9.337183\n",
      "Epoch : 394/1000, Train Loss : 8.604380, Valid Loss 11.405207\n",
      "Epoch : 395/1000, Train Loss : 7.444435, Valid Loss 9.794687\n",
      "Epoch : 396/1000, Train Loss : 7.475700, Valid Loss 11.220355\n",
      "Epoch : 397/1000, Train Loss : 7.990769, Valid Loss 9.316098\n",
      "Epoch : 398/1000, Train Loss : 7.770608, Valid Loss 9.972819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 399/1000, Train Loss : 7.633287, Valid Loss 10.004035\n",
      "Epoch : 400/1000, Train Loss : 7.498902, Valid Loss 9.388809\n",
      "Epoch : 401/1000, Train Loss : 7.019390, Valid Loss 10.343939\n",
      "Epoch : 402/1000, Train Loss : 7.575585, Valid Loss 11.657354\n",
      "Epoch : 403/1000, Train Loss : 7.912183, Valid Loss 10.015063\n",
      "Epoch : 404/1000, Train Loss : 6.944044, Valid Loss 11.172003\n",
      "Epoch : 405/1000, Train Loss : 7.197232, Valid Loss 10.380101\n",
      "Epoch : 406/1000, Train Loss : 8.351777, Valid Loss 18.003573\n",
      "Epoch : 407/1000, Train Loss : 7.688339, Valid Loss 11.221991\n",
      "Epoch : 408/1000, Train Loss : 7.672661, Valid Loss 9.626038\n",
      "Epoch : 409/1000, Train Loss : 6.962174, Valid Loss 9.515000\n",
      "Epoch : 410/1000, Train Loss : 7.402101, Valid Loss 11.717601\n",
      "Epoch : 411/1000, Train Loss : 8.118859, Valid Loss 12.421859\n",
      "Epoch : 412/1000, Train Loss : 7.572845, Valid Loss 10.430881\n",
      "Epoch : 413/1000, Train Loss : 7.010349, Valid Loss 9.283987\n",
      "Epoch : 414/1000, Train Loss : 8.213525, Valid Loss 11.288054\n",
      "Epoch : 415/1000, Train Loss : 10.585471, Valid Loss 10.560574\n",
      "Epoch : 416/1000, Train Loss : 8.134821, Valid Loss 9.661975\n",
      "Epoch : 417/1000, Train Loss : 7.125771, Valid Loss 10.377791\n",
      "Epoch : 418/1000, Train Loss : 6.917777, Valid Loss 9.833968\n",
      "Epoch : 419/1000, Train Loss : 7.160731, Valid Loss 12.406009\n",
      "Epoch : 420/1000, Train Loss : 7.083269, Valid Loss 9.677927\n",
      "Epoch : 421/1000, Train Loss : 6.771344, Valid Loss 9.667298\n",
      "Epoch : 422/1000, Train Loss : 6.626136, Valid Loss 9.949707\n",
      "Epoch : 423/1000, Train Loss : 7.401501, Valid Loss 13.568503\n",
      "Epoch : 424/1000, Train Loss : 7.506965, Valid Loss 10.403605\n",
      "Epoch : 425/1000, Train Loss : 6.694618, Valid Loss 9.090945\n",
      "Epoch : 426/1000, Train Loss : 6.786581, Valid Loss 11.823499\n",
      "Epoch : 427/1000, Train Loss : 7.526542, Valid Loss 9.276454\n",
      "Epoch : 428/1000, Train Loss : 7.215275, Valid Loss 10.423230\n",
      "Epoch : 429/1000, Train Loss : 6.445727, Valid Loss 10.060109\n",
      "Epoch : 430/1000, Train Loss : 6.516277, Valid Loss 9.108051\n",
      "Epoch : 431/1000, Train Loss : 7.011540, Valid Loss 8.941971\n",
      "Epoch : 432/1000, Train Loss : 8.003093, Valid Loss 9.119279\n",
      "Epoch : 433/1000, Train Loss : 6.477004, Valid Loss 9.475267\n",
      "Epoch : 434/1000, Train Loss : 6.819993, Valid Loss 10.663459\n",
      "Epoch : 435/1000, Train Loss : 7.917387, Valid Loss 9.465468\n",
      "Epoch : 436/1000, Train Loss : 7.570012, Valid Loss 9.175985\n",
      "Epoch : 437/1000, Train Loss : 6.812037, Valid Loss 9.447488\n",
      "Epoch : 438/1000, Train Loss : 6.404466, Valid Loss 9.893956\n",
      "Epoch : 439/1000, Train Loss : 7.409392, Valid Loss 10.330697\n",
      "Epoch : 440/1000, Train Loss : 7.750997, Valid Loss 9.557012\n",
      "Epoch : 441/1000, Train Loss : 6.936730, Valid Loss 9.136137\n",
      "Epoch : 442/1000, Train Loss : 6.560139, Valid Loss 9.195250\n",
      "Epoch : 443/1000, Train Loss : 7.665168, Valid Loss 20.157558\n",
      "Epoch : 444/1000, Train Loss : 9.178216, Valid Loss 9.283155\n",
      "Epoch : 445/1000, Train Loss : 7.101515, Valid Loss 10.619586\n",
      "Epoch : 446/1000, Train Loss : 6.609718, Valid Loss 9.373897\n",
      "Epoch : 447/1000, Train Loss : 6.768414, Valid Loss 10.683178\n",
      "Epoch : 448/1000, Train Loss : 7.670104, Valid Loss 11.204036\n",
      "Epoch : 449/1000, Train Loss : 7.664987, Valid Loss 10.347957\n",
      "Epoch : 450/1000, Train Loss : 6.907017, Valid Loss 10.781842\n",
      "Epoch : 451/1000, Train Loss : 6.424026, Valid Loss 9.692447\n",
      "Epoch : 452/1000, Train Loss : 6.769781, Valid Loss 9.208380\n",
      "Epoch : 453/1000, Train Loss : 6.648952, Valid Loss 10.149764\n",
      "Epoch : 454/1000, Train Loss : 7.859940, Valid Loss 11.799633\n",
      "Epoch : 455/1000, Train Loss : 6.339351, Valid Loss 8.997471\n",
      "Epoch : 456/1000, Train Loss : 6.813358, Valid Loss 9.339592\n",
      "Epoch : 457/1000, Train Loss : 6.609903, Valid Loss 9.796899\n",
      "Epoch : 458/1000, Train Loss : 6.533872, Valid Loss 9.767528\n",
      "Epoch : 459/1000, Train Loss : 6.731982, Valid Loss 10.933979\n",
      "Epoch : 460/1000, Train Loss : 6.906849, Valid Loss 9.671483\n",
      "Epoch : 461/1000, Train Loss : 6.387172, Valid Loss 9.277328\n",
      "Epoch : 462/1000, Train Loss : 6.368664, Valid Loss 9.001450\n",
      "Epoch : 463/1000, Train Loss : 6.307591, Valid Loss 11.074197\n",
      "Epoch : 464/1000, Train Loss : 6.636672, Valid Loss 8.793794\n",
      "Epoch : 465/1000, Train Loss : 6.874601, Valid Loss 9.827919\n",
      "Epoch : 466/1000, Train Loss : 6.842998, Valid Loss 8.935508\n",
      "Epoch : 467/1000, Train Loss : 6.627281, Valid Loss 10.032124\n",
      "Epoch : 468/1000, Train Loss : 6.754528, Valid Loss 11.078968\n",
      "Epoch : 469/1000, Train Loss : 7.341077, Valid Loss 9.428275\n",
      "Epoch : 470/1000, Train Loss : 5.977929, Valid Loss 10.051702\n",
      "Epoch : 471/1000, Train Loss : 6.279347, Valid Loss 10.278987\n",
      "Epoch : 472/1000, Train Loss : 6.859643, Valid Loss 13.159573\n",
      "Epoch : 473/1000, Train Loss : 6.974402, Valid Loss 8.937129\n",
      "Epoch : 474/1000, Train Loss : 6.070155, Valid Loss 8.888760\n",
      "Epoch : 475/1000, Train Loss : 6.395076, Valid Loss 9.253870\n",
      "Epoch : 476/1000, Train Loss : 7.470287, Valid Loss 12.634650\n",
      "Epoch : 477/1000, Train Loss : 6.918054, Valid Loss 9.448730\n",
      "Epoch : 478/1000, Train Loss : 7.007920, Valid Loss 9.746303\n",
      "Epoch : 479/1000, Train Loss : 6.552475, Valid Loss 11.171709\n",
      "Epoch : 480/1000, Train Loss : 6.295977, Valid Loss 8.669700\n",
      "Epoch : 481/1000, Train Loss : 6.006331, Valid Loss 8.492501\n",
      "Epoch : 482/1000, Train Loss : 5.796413, Valid Loss 8.634482\n",
      "Epoch : 483/1000, Train Loss : 6.546780, Valid Loss 9.295046\n",
      "Epoch : 484/1000, Train Loss : 6.042681, Valid Loss 9.036616\n",
      "Epoch : 485/1000, Train Loss : 6.299386, Valid Loss 9.529283\n",
      "Epoch : 486/1000, Train Loss : 6.865952, Valid Loss 9.030902\n",
      "Epoch : 487/1000, Train Loss : 6.152980, Valid Loss 10.751105\n",
      "Epoch : 488/1000, Train Loss : 7.163581, Valid Loss 10.515573\n",
      "Epoch : 489/1000, Train Loss : 6.472964, Valid Loss 9.324089\n",
      "Epoch : 490/1000, Train Loss : 5.935274, Valid Loss 9.420336\n",
      "Epoch : 491/1000, Train Loss : 6.260638, Valid Loss 8.765097\n",
      "Epoch : 492/1000, Train Loss : 5.833010, Valid Loss 9.935045\n",
      "Epoch : 493/1000, Train Loss : 5.855318, Valid Loss 9.344328\n",
      "Epoch : 494/1000, Train Loss : 7.063331, Valid Loss 8.986941\n",
      "Epoch : 495/1000, Train Loss : 6.146211, Valid Loss 10.912041\n",
      "Epoch : 496/1000, Train Loss : 5.891018, Valid Loss 14.752738\n",
      "Epoch : 497/1000, Train Loss : 10.256095, Valid Loss 10.263658\n",
      "Epoch : 498/1000, Train Loss : 6.718626, Valid Loss 8.807929\n",
      "Epoch : 499/1000, Train Loss : 5.906863, Valid Loss 9.067662\n",
      "Epoch : 500/1000, Train Loss : 5.796717, Valid Loss 9.536301\n",
      "Epoch : 501/1000, Train Loss : 5.846648, Valid Loss 9.173333\n",
      "Epoch : 502/1000, Train Loss : 6.001991, Valid Loss 9.270177\n",
      "Epoch : 503/1000, Train Loss : 5.787137, Valid Loss 9.744780\n",
      "Epoch : 504/1000, Train Loss : 5.865827, Valid Loss 9.325710\n",
      "Epoch : 505/1000, Train Loss : 6.059143, Valid Loss 9.568678\n",
      "Epoch : 506/1000, Train Loss : 6.445994, Valid Loss 9.019291\n",
      "Epoch : 507/1000, Train Loss : 6.319465, Valid Loss 8.630637\n",
      "Epoch : 508/1000, Train Loss : 6.648177, Valid Loss 8.957588\n",
      "Epoch : 509/1000, Train Loss : 5.705170, Valid Loss 9.550722\n",
      "Epoch : 510/1000, Train Loss : 5.769396, Valid Loss 8.853580\n",
      "Epoch : 511/1000, Train Loss : 5.934740, Valid Loss 11.057789\n",
      "Epoch : 512/1000, Train Loss : 7.100286, Valid Loss 10.407527\n",
      "Epoch : 513/1000, Train Loss : 5.583306, Valid Loss 8.764070\n",
      "Epoch : 514/1000, Train Loss : 5.573447, Valid Loss 8.661083\n",
      "Epoch : 515/1000, Train Loss : 6.267330, Valid Loss 8.790776\n",
      "Epoch : 516/1000, Train Loss : 5.498002, Valid Loss 12.458287\n",
      "Epoch : 517/1000, Train Loss : 6.947803, Valid Loss 10.865606\n",
      "Epoch : 518/1000, Train Loss : 6.503549, Valid Loss 12.023552\n",
      "Epoch : 519/1000, Train Loss : 6.185457, Valid Loss 9.124066\n",
      "Epoch : 520/1000, Train Loss : 6.422029, Valid Loss 10.609625\n",
      "Epoch : 521/1000, Train Loss : 6.026470, Valid Loss 10.556871\n",
      "Epoch : 522/1000, Train Loss : 6.112703, Valid Loss 8.856512\n",
      "Epoch : 523/1000, Train Loss : 5.420222, Valid Loss 8.625082\n",
      "Epoch : 524/1000, Train Loss : 5.707674, Valid Loss 8.803650\n",
      "Epoch : 525/1000, Train Loss : 6.403142, Valid Loss 12.438935\n",
      "Epoch : 526/1000, Train Loss : 6.227640, Valid Loss 11.172376\n",
      "Epoch : 527/1000, Train Loss : 7.415081, Valid Loss 13.003687\n",
      "Epoch : 528/1000, Train Loss : 7.077250, Valid Loss 8.648185\n",
      "Epoch : 529/1000, Train Loss : 5.819283, Valid Loss 12.698461\n",
      "Epoch : 530/1000, Train Loss : 8.139208, Valid Loss 11.775326\n",
      "Epoch : 531/1000, Train Loss : 5.778664, Valid Loss 11.421553\n",
      "Epoch : 532/1000, Train Loss : 6.293212, Valid Loss 9.768460\n",
      "Early stopping\n",
      "9.768460273742676\n"
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 100\n",
    "converge_cnt = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model_atn.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model_atn.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    val_epoch_loss.append(val_loss)\n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 50:\n",
    "        print('Early stopping')\n",
    "        break\n",
    "\n",
    "print(min(val_epoch_loss))\n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7621716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85211f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86606fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cnn_acc = nn.Conv2d(in_channels=3, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait",
   "language": "python",
   "name": "gait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
