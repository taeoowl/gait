{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dc5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error # mse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from dataloader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5a4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        _, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        _, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "    \n",
    "        inputs_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        out_lstm = self.reg_module1(inputs_concat)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83facf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "dataset = Gait_Dataset_Salted(file_path)\n",
    "val_percent = 0.2\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7788b4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3784"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e009bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8209fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bbbae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 17768.288005, Valid Loss 17595.775065, Error 0.994135\n",
      "Epoch : 2/2000, Train Loss : 17101.405640, Valid Loss 16343.888835, Error 0.957427\n",
      "Epoch : 3/2000, Train Loss : 14800.816366, Valid Loss 12534.089355, Error 0.835870\n",
      "Epoch : 4/2000, Train Loss : 9222.625509, Valid Loss 5254.845703, Error 0.531379\n",
      "Epoch : 5/2000, Train Loss : 2262.653364, Valid Loss 363.996516, Error 0.106073\n",
      "Epoch : 6/2000, Train Loss : 357.530891, Valid Loss 334.210592, Error 0.108742\n",
      "Epoch : 7/2000, Train Loss : 310.104084, Valid Loss 305.854177, Error 0.100184\n",
      "Epoch : 8/2000, Train Loss : 305.627284, Valid Loss 302.595200, Error 0.101288\n",
      "Epoch : 9/2000, Train Loss : 305.127252, Valid Loss 302.612490, Error 0.101361\n",
      "Epoch : 10/2000, Train Loss : 304.747272, Valid Loss 302.581896, Error 0.101153\n",
      "Epoch : 11/2000, Train Loss : 304.812725, Valid Loss 302.577843, Error 0.101127\n",
      "Epoch : 12/2000, Train Loss : 304.442446, Valid Loss 302.546735, Error 0.101261\n",
      "Epoch : 13/2000, Train Loss : 305.715525, Valid Loss 302.559285, Error 0.101075\n",
      "Epoch : 14/2000, Train Loss : 305.717740, Valid Loss 302.497879, Error 0.101113\n",
      "Epoch : 15/2000, Train Loss : 304.883742, Valid Loss 302.432945, Error 0.101288\n",
      "Epoch : 16/2000, Train Loss : 304.797331, Valid Loss 301.357908, Error 0.100373\n",
      "Epoch : 17/2000, Train Loss : 298.968833, Valid Loss 288.281425, Error 0.096889\n",
      "Epoch : 18/2000, Train Loss : 283.648319, Valid Loss 268.752162, Error 0.092848\n",
      "Epoch : 19/2000, Train Loss : 262.554530, Valid Loss 256.784101, Error 0.090350\n",
      "Epoch : 20/2000, Train Loss : 240.561253, Valid Loss 225.289546, Error 0.086159\n",
      "Epoch : 21/2000, Train Loss : 214.568774, Valid Loss 200.700706, Error 0.084313\n",
      "Epoch : 22/2000, Train Loss : 193.420753, Valid Loss 177.697522, Error 0.080360\n",
      "Epoch : 23/2000, Train Loss : 171.587044, Valid Loss 157.288971, Error 0.076686\n",
      "Epoch : 24/2000, Train Loss : 153.868644, Valid Loss 150.266762, Error 0.073490\n",
      "Epoch : 25/2000, Train Loss : 138.842893, Valid Loss 131.106927, Error 0.071960\n",
      "Epoch : 26/2000, Train Loss : 126.496107, Valid Loss 123.793888, Error 0.071864\n",
      "Epoch : 27/2000, Train Loss : 117.189006, Valid Loss 107.555106, Error 0.064185\n",
      "Epoch : 28/2000, Train Loss : 106.949350, Valid Loss 98.584830, Error 0.063377\n",
      "Epoch : 29/2000, Train Loss : 97.619938, Valid Loss 92.224248, Error 0.062253\n",
      "Epoch : 30/2000, Train Loss : 89.689809, Valid Loss 84.728275, Error 0.060092\n",
      "Epoch : 31/2000, Train Loss : 83.720456, Valid Loss 78.002285, Error 0.057268\n",
      "Epoch : 32/2000, Train Loss : 76.758777, Valid Loss 71.627237, Error 0.055761\n",
      "Epoch : 33/2000, Train Loss : 71.178735, Valid Loss 69.125563, Error 0.057545\n",
      "Epoch : 34/2000, Train Loss : 67.543111, Valid Loss 65.118612, Error 0.050614\n",
      "Epoch : 35/2000, Train Loss : 61.856756, Valid Loss 58.705318, Error 0.049957\n",
      "Epoch : 36/2000, Train Loss : 60.836266, Valid Loss 62.084371, Error 0.056212\n",
      "Epoch : 37/2000, Train Loss : 60.848607, Valid Loss 62.928097, Error 0.048035\n",
      "Epoch : 38/2000, Train Loss : 53.825503, Valid Loss 51.082898, Error 0.047658\n",
      "Epoch : 39/2000, Train Loss : 50.974301, Valid Loss 54.917065, Error 0.053493\n",
      "Epoch : 40/2000, Train Loss : 61.124133, Valid Loss 63.743643, Error 0.058198\n",
      "Epoch : 41/2000, Train Loss : 50.665752, Valid Loss 49.757039, Error 0.050564\n",
      "Epoch : 42/2000, Train Loss : 47.064903, Valid Loss 45.557349, Error 0.044734\n",
      "Epoch : 43/2000, Train Loss : 44.817164, Valid Loss 44.117253, Error 0.045481\n",
      "Epoch : 44/2000, Train Loss : 43.490270, Valid Loss 46.169844, Error 0.048499\n",
      "Epoch : 45/2000, Train Loss : 41.943718, Valid Loss 47.685626, Error 0.050107\n",
      "Epoch : 46/2000, Train Loss : 42.113078, Valid Loss 40.671219, Error 0.042708\n",
      "Epoch : 47/2000, Train Loss : 39.687986, Valid Loss 39.676642, Error 0.041479\n",
      "Epoch : 48/2000, Train Loss : 40.253247, Valid Loss 39.032950, Error 0.040857\n",
      "Epoch : 49/2000, Train Loss : 38.436926, Valid Loss 38.374215, Error 0.041007\n",
      "Epoch : 50/2000, Train Loss : 37.326848, Valid Loss 37.805686, Error 0.039812\n",
      "Epoch : 51/2000, Train Loss : 37.493088, Valid Loss 36.881783, Error 0.041768\n",
      "Epoch : 52/2000, Train Loss : 37.740182, Valid Loss 39.755893, Error 0.038235\n",
      "Epoch : 53/2000, Train Loss : 39.064508, Valid Loss 37.063523, Error 0.039071\n",
      "Epoch : 54/2000, Train Loss : 35.811209, Valid Loss 34.581121, Error 0.039057\n",
      "Epoch : 55/2000, Train Loss : 36.192513, Valid Loss 43.440467, Error 0.049064\n",
      "Epoch : 56/2000, Train Loss : 33.864992, Valid Loss 35.718921, Error 0.041995\n",
      "Epoch : 57/2000, Train Loss : 33.090929, Valid Loss 35.075644, Error 0.041954\n",
      "Epoch : 58/2000, Train Loss : 32.604783, Valid Loss 32.934705, Error 0.037169\n",
      "Epoch : 59/2000, Train Loss : 32.761225, Valid Loss 33.027634, Error 0.037313\n",
      "Epoch : 60/2000, Train Loss : 31.188078, Valid Loss 31.867512, Error 0.038711\n",
      "Epoch : 61/2000, Train Loss : 31.011009, Valid Loss 34.315036, Error 0.042595\n",
      "Epoch : 62/2000, Train Loss : 32.119937, Valid Loss 37.697112, Error 0.044356\n",
      "Epoch : 63/2000, Train Loss : 32.236976, Valid Loss 30.599801, Error 0.038893\n",
      "Epoch : 64/2000, Train Loss : 29.440182, Valid Loss 30.371337, Error 0.038932\n",
      "Epoch : 65/2000, Train Loss : 29.870966, Valid Loss 29.956691, Error 0.035457\n",
      "Epoch : 66/2000, Train Loss : 31.111059, Valid Loss 30.803370, Error 0.034774\n",
      "Epoch : 67/2000, Train Loss : 29.400661, Valid Loss 28.515431, Error 0.035476\n",
      "Epoch : 68/2000, Train Loss : 27.524402, Valid Loss 27.831790, Error 0.035401\n",
      "Epoch : 69/2000, Train Loss : 28.311570, Valid Loss 29.651816, Error 0.039388\n",
      "Epoch : 70/2000, Train Loss : 30.167473, Valid Loss 38.427125, Error 0.045903\n",
      "Epoch : 71/2000, Train Loss : 30.768195, Valid Loss 33.909991, Error 0.042784\n",
      "Epoch : 72/2000, Train Loss : 27.630781, Valid Loss 26.793563, Error 0.035211\n",
      "Epoch : 73/2000, Train Loss : 29.045854, Valid Loss 26.324342, Error 0.035679\n",
      "Epoch : 74/2000, Train Loss : 26.630642, Valid Loss 26.493678, Error 0.036361\n",
      "Epoch : 75/2000, Train Loss : 26.232485, Valid Loss 25.489061, Error 0.034400\n",
      "Epoch : 76/2000, Train Loss : 24.959632, Valid Loss 26.096615, Error 0.033271\n",
      "Epoch : 77/2000, Train Loss : 25.477496, Valid Loss 27.640579, Error 0.037788\n",
      "Epoch : 78/2000, Train Loss : 25.495059, Valid Loss 26.193112, Error 0.032948\n",
      "Epoch : 79/2000, Train Loss : 26.242336, Valid Loss 24.828218, Error 0.033780\n",
      "Epoch : 80/2000, Train Loss : 24.689591, Valid Loss 23.862880, Error 0.033242\n",
      "Epoch : 81/2000, Train Loss : 26.497878, Valid Loss 27.626240, Error 0.032238\n",
      "Epoch : 82/2000, Train Loss : 26.529523, Valid Loss 23.858005, Error 0.032774\n",
      "Epoch : 83/2000, Train Loss : 24.317103, Valid Loss 26.381437, Error 0.031424\n",
      "Epoch : 84/2000, Train Loss : 23.277083, Valid Loss 23.011441, Error 0.032433\n",
      "Epoch : 85/2000, Train Loss : 23.455884, Valid Loss 24.403969, Error 0.031284\n",
      "Epoch : 86/2000, Train Loss : 23.154624, Valid Loss 22.731674, Error 0.031716\n",
      "Epoch : 87/2000, Train Loss : 22.584753, Valid Loss 22.133145, Error 0.031843\n",
      "Epoch : 88/2000, Train Loss : 22.322678, Valid Loss 23.005524, Error 0.031210\n",
      "Epoch : 89/2000, Train Loss : 22.300918, Valid Loss 21.828004, Error 0.031274\n",
      "Epoch : 90/2000, Train Loss : 21.715305, Valid Loss 21.871694, Error 0.032263\n",
      "Epoch : 91/2000, Train Loss : 23.533799, Valid Loss 23.416317, Error 0.034137\n",
      "Epoch : 92/2000, Train Loss : 21.866646, Valid Loss 21.708528, Error 0.032269\n",
      "Epoch : 93/2000, Train Loss : 22.015295, Valid Loss 21.441196, Error 0.030135\n",
      "Epoch : 94/2000, Train Loss : 21.201246, Valid Loss 28.755999, Error 0.031344\n",
      "Epoch : 95/2000, Train Loss : 25.842093, Valid Loss 21.153330, Error 0.031961\n",
      "Epoch : 96/2000, Train Loss : 21.024215, Valid Loss 20.488263, Error 0.030514\n",
      "Epoch : 97/2000, Train Loss : 21.587884, Valid Loss 20.475358, Error 0.030906\n",
      "Epoch : 98/2000, Train Loss : 22.019731, Valid Loss 21.865492, Error 0.029907\n",
      "Epoch : 99/2000, Train Loss : 21.973832, Valid Loss 20.192699, Error 0.029633\n",
      "Epoch : 100/2000, Train Loss : 20.567514, Valid Loss 20.701228, Error 0.031939\n",
      "Epoch : 101/2000, Train Loss : 21.867945, Valid Loss 20.850754, Error 0.028886\n",
      "Epoch : 102/2000, Train Loss : 22.027006, Valid Loss 20.478703, Error 0.031538\n",
      "Epoch : 103/2000, Train Loss : 22.527855, Valid Loss 20.853234, Error 0.032175\n",
      "Epoch : 104/2000, Train Loss : 20.535182, Valid Loss 20.013156, Error 0.031214\n",
      "Epoch : 105/2000, Train Loss : 19.598163, Valid Loss 19.688694, Error 0.028787\n",
      "Epoch : 106/2000, Train Loss : 20.340571, Valid Loss 19.210179, Error 0.029255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 107/2000, Train Loss : 19.640806, Valid Loss 21.855531, Error 0.028171\n",
      "Epoch : 108/2000, Train Loss : 21.511225, Valid Loss 19.086078, Error 0.029381\n",
      "Epoch : 109/2000, Train Loss : 19.779672, Valid Loss 18.810523, Error 0.028968\n",
      "Epoch : 110/2000, Train Loss : 19.361368, Valid Loss 19.522345, Error 0.030284\n",
      "Epoch : 111/2000, Train Loss : 19.383135, Valid Loss 18.546446, Error 0.029162\n",
      "Epoch : 112/2000, Train Loss : 18.548320, Valid Loss 18.333964, Error 0.029285\n",
      "Epoch : 113/2000, Train Loss : 19.037642, Valid Loss 22.365559, Error 0.028181\n",
      "Epoch : 114/2000, Train Loss : 18.774802, Valid Loss 18.650407, Error 0.029984\n",
      "Epoch : 115/2000, Train Loss : 19.571761, Valid Loss 18.365340, Error 0.029950\n",
      "Epoch : 116/2000, Train Loss : 21.897657, Valid Loss 22.832508, Error 0.028899\n",
      "Epoch : 117/2000, Train Loss : 18.600072, Valid Loss 17.938812, Error 0.028836\n",
      "Epoch : 118/2000, Train Loss : 18.244386, Valid Loss 19.524285, Error 0.031672\n",
      "Epoch : 119/2000, Train Loss : 18.262336, Valid Loss 18.234278, Error 0.029844\n",
      "Epoch : 120/2000, Train Loss : 20.976490, Valid Loss 22.220700, Error 0.033965\n",
      "Epoch : 121/2000, Train Loss : 19.652034, Valid Loss 17.384069, Error 0.028219\n",
      "Epoch : 122/2000, Train Loss : 17.324531, Valid Loss 17.737223, Error 0.028138\n",
      "Epoch : 123/2000, Train Loss : 17.688423, Valid Loss 17.018331, Error 0.027934\n",
      "Epoch : 124/2000, Train Loss : 17.783067, Valid Loss 17.228247, Error 0.027861\n",
      "Epoch : 125/2000, Train Loss : 16.868359, Valid Loss 17.458367, Error 0.028576\n",
      "Epoch : 126/2000, Train Loss : 18.035785, Valid Loss 16.973973, Error 0.027935\n",
      "Epoch : 127/2000, Train Loss : 16.850707, Valid Loss 17.821970, Error 0.029246\n",
      "Epoch : 128/2000, Train Loss : 17.238761, Valid Loss 17.037769, Error 0.027956\n",
      "Epoch : 129/2000, Train Loss : 16.801021, Valid Loss 16.676320, Error 0.027203\n",
      "Epoch : 130/2000, Train Loss : 16.818019, Valid Loss 16.507140, Error 0.026394\n",
      "Epoch : 131/2000, Train Loss : 16.918159, Valid Loss 17.189340, Error 0.025779\n",
      "Epoch : 132/2000, Train Loss : 18.025338, Valid Loss 18.506063, Error 0.029934\n",
      "Epoch : 133/2000, Train Loss : 16.926556, Valid Loss 16.248530, Error 0.026231\n",
      "Epoch : 134/2000, Train Loss : 16.066628, Valid Loss 16.413631, Error 0.027185\n",
      "Epoch : 135/2000, Train Loss : 17.212693, Valid Loss 17.693917, Error 0.025354\n",
      "Epoch : 136/2000, Train Loss : 16.652262, Valid Loss 18.845966, Error 0.025813\n",
      "Epoch : 137/2000, Train Loss : 16.887178, Valid Loss 16.274907, Error 0.027311\n",
      "Epoch : 138/2000, Train Loss : 16.408465, Valid Loss 25.424179, Error 0.035760\n",
      "Epoch : 139/2000, Train Loss : 19.621701, Valid Loss 18.999542, Error 0.025780\n",
      "Epoch : 140/2000, Train Loss : 17.117243, Valid Loss 18.347694, Error 0.030181\n",
      "Epoch : 141/2000, Train Loss : 17.043256, Valid Loss 15.689716, Error 0.025669\n",
      "Epoch : 142/2000, Train Loss : 16.010101, Valid Loss 15.857767, Error 0.027036\n",
      "Epoch : 143/2000, Train Loss : 15.619330, Valid Loss 16.835419, Error 0.024626\n",
      "Epoch : 144/2000, Train Loss : 16.069012, Valid Loss 18.276191, Error 0.025438\n",
      "Epoch : 145/2000, Train Loss : 17.123498, Valid Loss 16.171954, Error 0.025331\n",
      "Epoch : 146/2000, Train Loss : 15.993815, Valid Loss 15.188733, Error 0.026088\n",
      "Epoch : 147/2000, Train Loss : 17.096656, Valid Loss 20.936693, Error 0.027239\n",
      "Epoch : 148/2000, Train Loss : 16.388511, Valid Loss 15.366419, Error 0.025067\n",
      "Epoch : 149/2000, Train Loss : 15.591748, Valid Loss 17.534805, Error 0.029071\n",
      "Epoch : 150/2000, Train Loss : 16.369127, Valid Loss 20.126567, Error 0.026232\n",
      "Epoch : 151/2000, Train Loss : 17.287183, Valid Loss 16.694402, Error 0.025221\n",
      "Epoch : 152/2000, Train Loss : 16.509871, Valid Loss 15.294948, Error 0.024814\n",
      "Epoch : 153/2000, Train Loss : 15.346626, Valid Loss 15.854783, Error 0.024554\n",
      "Epoch : 154/2000, Train Loss : 15.557670, Valid Loss 15.033218, Error 0.024676\n",
      "Epoch : 155/2000, Train Loss : 15.367408, Valid Loss 14.834468, Error 0.024666\n",
      "Epoch : 156/2000, Train Loss : 14.598143, Valid Loss 16.110210, Error 0.024759\n",
      "Epoch : 157/2000, Train Loss : 15.335757, Valid Loss 15.353791, Error 0.024346\n",
      "Epoch : 158/2000, Train Loss : 14.884451, Valid Loss 15.046240, Error 0.024606\n",
      "Epoch : 159/2000, Train Loss : 14.729697, Valid Loss 14.734030, Error 0.024792\n",
      "Epoch : 160/2000, Train Loss : 14.692171, Valid Loss 15.551228, Error 0.025541\n",
      "Epoch : 161/2000, Train Loss : 14.788483, Valid Loss 14.746032, Error 0.024592\n",
      "Epoch : 162/2000, Train Loss : 14.324258, Valid Loss 14.589114, Error 0.024418\n",
      "Epoch : 163/2000, Train Loss : 15.200007, Valid Loss 14.481681, Error 0.024154\n",
      "Epoch : 164/2000, Train Loss : 14.575863, Valid Loss 14.782314, Error 0.023484\n",
      "Epoch : 165/2000, Train Loss : 14.748592, Valid Loss 18.827575, Error 0.025440\n",
      "Epoch : 166/2000, Train Loss : 15.201908, Valid Loss 15.274879, Error 0.025768\n",
      "Epoch : 167/2000, Train Loss : 14.353339, Valid Loss 16.397772, Error 0.027557\n",
      "Epoch : 168/2000, Train Loss : 14.457896, Valid Loss 14.249102, Error 0.024518\n",
      "Epoch : 169/2000, Train Loss : 13.893943, Valid Loss 15.499985, Error 0.025872\n",
      "Epoch : 170/2000, Train Loss : 14.109904, Valid Loss 14.087600, Error 0.023582\n",
      "Epoch : 171/2000, Train Loss : 13.881375, Valid Loss 15.980521, Error 0.027826\n",
      "Epoch : 172/2000, Train Loss : 13.839527, Valid Loss 14.162707, Error 0.023003\n",
      "Epoch : 173/2000, Train Loss : 13.801350, Valid Loss 14.828366, Error 0.025491\n",
      "Epoch : 174/2000, Train Loss : 13.682925, Valid Loss 14.355502, Error 0.024488\n",
      "Epoch : 175/2000, Train Loss : 14.197145, Valid Loss 14.220734, Error 0.023088\n",
      "Epoch : 176/2000, Train Loss : 13.743382, Valid Loss 14.069078, Error 0.023667\n",
      "Epoch : 177/2000, Train Loss : 14.047364, Valid Loss 14.826597, Error 0.025563\n",
      "Epoch : 178/2000, Train Loss : 14.119317, Valid Loss 15.025708, Error 0.025374\n",
      "Epoch : 179/2000, Train Loss : 14.146888, Valid Loss 13.638925, Error 0.023161\n",
      "Epoch : 180/2000, Train Loss : 15.139990, Valid Loss 14.101613, Error 0.023482\n",
      "Epoch : 181/2000, Train Loss : 16.578397, Valid Loss 14.460604, Error 0.024895\n",
      "Epoch : 182/2000, Train Loss : 13.598587, Valid Loss 15.932383, Error 0.023630\n",
      "Epoch : 183/2000, Train Loss : 13.988410, Valid Loss 13.524776, Error 0.023117\n",
      "Epoch : 184/2000, Train Loss : 14.794058, Valid Loss 13.952504, Error 0.022577\n",
      "Epoch : 185/2000, Train Loss : 13.107410, Valid Loss 13.403616, Error 0.023285\n",
      "Epoch : 186/2000, Train Loss : 14.435861, Valid Loss 14.594432, Error 0.023256\n",
      "Epoch : 187/2000, Train Loss : 13.912263, Valid Loss 13.655382, Error 0.022875\n",
      "Epoch : 188/2000, Train Loss : 15.008728, Valid Loss 16.865978, Error 0.028592\n",
      "Epoch : 189/2000, Train Loss : 14.719679, Valid Loss 13.830580, Error 0.024422\n",
      "Epoch : 190/2000, Train Loss : 13.235534, Valid Loss 13.398879, Error 0.022517\n",
      "Epoch : 191/2000, Train Loss : 13.391616, Valid Loss 13.968964, Error 0.024415\n",
      "Epoch : 192/2000, Train Loss : 13.268620, Valid Loss 13.299055, Error 0.023252\n",
      "Epoch : 193/2000, Train Loss : 12.884905, Valid Loss 13.252437, Error 0.021917\n",
      "Epoch : 194/2000, Train Loss : 13.862680, Valid Loss 14.329763, Error 0.025084\n",
      "Epoch : 195/2000, Train Loss : 13.369687, Valid Loss 14.534612, Error 0.022778\n",
      "Epoch : 196/2000, Train Loss : 13.195134, Valid Loss 13.016345, Error 0.022222\n",
      "Epoch : 197/2000, Train Loss : 12.561807, Valid Loss 13.662742, Error 0.022197\n",
      "Epoch : 198/2000, Train Loss : 12.898653, Valid Loss 14.180325, Error 0.022413\n",
      "Epoch : 199/2000, Train Loss : 12.364405, Valid Loss 13.049625, Error 0.021768\n",
      "Epoch : 200/2000, Train Loss : 12.188535, Valid Loss 13.150705, Error 0.022999\n",
      "Epoch : 201/2000, Train Loss : 12.426496, Valid Loss 15.101095, Error 0.022842\n",
      "Epoch : 202/2000, Train Loss : 13.146304, Valid Loss 15.141359, Error 0.023021\n",
      "Epoch : 203/2000, Train Loss : 12.658094, Valid Loss 14.231832, Error 0.022415\n",
      "Epoch : 204/2000, Train Loss : 13.530862, Valid Loss 13.793508, Error 0.022159\n",
      "Epoch : 205/2000, Train Loss : 12.347346, Valid Loss 14.848714, Error 0.024989\n",
      "Epoch : 206/2000, Train Loss : 12.711809, Valid Loss 16.469024, Error 0.024218\n",
      "Epoch : 207/2000, Train Loss : 12.455026, Valid Loss 12.627762, Error 0.021853\n",
      "Epoch : 208/2000, Train Loss : 12.030855, Valid Loss 12.813326, Error 0.021507\n",
      "Epoch : 209/2000, Train Loss : 13.350646, Valid Loss 13.222923, Error 0.021730\n",
      "Epoch : 210/2000, Train Loss : 13.758423, Valid Loss 14.616208, Error 0.022642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 211/2000, Train Loss : 12.248595, Valid Loss 13.407844, Error 0.023133\n",
      "Epoch : 212/2000, Train Loss : 12.742539, Valid Loss 18.398629, Error 0.029754\n",
      "Epoch : 213/2000, Train Loss : 12.793797, Valid Loss 12.873816, Error 0.021903\n",
      "Epoch : 214/2000, Train Loss : 12.013634, Valid Loss 12.377978, Error 0.021395\n",
      "Epoch : 215/2000, Train Loss : 11.800115, Valid Loss 13.185993, Error 0.021369\n",
      "Epoch : 216/2000, Train Loss : 13.778708, Valid Loss 12.663954, Error 0.022157\n",
      "Epoch : 217/2000, Train Loss : 12.870879, Valid Loss 12.744956, Error 0.022045\n",
      "Epoch : 218/2000, Train Loss : 12.944067, Valid Loss 13.868086, Error 0.022506\n",
      "Epoch : 219/2000, Train Loss : 12.279795, Valid Loss 12.948880, Error 0.021309\n",
      "Epoch : 220/2000, Train Loss : 11.972809, Valid Loss 12.445718, Error 0.021691\n",
      "Epoch : 221/2000, Train Loss : 11.867019, Valid Loss 12.357506, Error 0.021693\n",
      "Epoch : 222/2000, Train Loss : 11.694688, Valid Loss 12.403354, Error 0.021293\n",
      "Epoch : 223/2000, Train Loss : 11.580642, Valid Loss 13.621369, Error 0.023738\n",
      "Epoch : 224/2000, Train Loss : 11.762139, Valid Loss 12.639100, Error 0.021115\n",
      "Epoch : 225/2000, Train Loss : 12.253088, Valid Loss 12.360345, Error 0.021139\n",
      "Epoch : 226/2000, Train Loss : 12.707020, Valid Loss 15.190492, Error 0.023002\n",
      "Epoch : 227/2000, Train Loss : 14.391577, Valid Loss 15.124884, Error 0.023562\n",
      "Epoch : 228/2000, Train Loss : 11.976041, Valid Loss 12.235561, Error 0.020931\n",
      "Epoch : 229/2000, Train Loss : 12.772702, Valid Loss 13.781344, Error 0.024476\n",
      "Epoch : 230/2000, Train Loss : 12.236418, Valid Loss 13.408385, Error 0.022825\n",
      "Epoch : 231/2000, Train Loss : 11.660624, Valid Loss 12.102365, Error 0.020914\n",
      "Epoch : 232/2000, Train Loss : 11.669002, Valid Loss 11.987120, Error 0.020986\n",
      "Epoch : 233/2000, Train Loss : 11.703797, Valid Loss 12.833909, Error 0.020994\n",
      "Epoch : 234/2000, Train Loss : 12.168109, Valid Loss 11.905347, Error 0.020362\n",
      "Epoch : 235/2000, Train Loss : 12.954243, Valid Loss 12.718307, Error 0.021220\n",
      "Epoch : 236/2000, Train Loss : 12.274246, Valid Loss 16.550908, Error 0.024450\n",
      "Epoch : 237/2000, Train Loss : 12.741700, Valid Loss 12.067228, Error 0.021046\n",
      "Epoch : 238/2000, Train Loss : 10.800049, Valid Loss 11.778050, Error 0.020208\n",
      "Epoch : 239/2000, Train Loss : 11.365474, Valid Loss 13.278694, Error 0.021307\n",
      "Epoch : 240/2000, Train Loss : 12.100017, Valid Loss 11.883309, Error 0.021209\n",
      "Epoch : 241/2000, Train Loss : 10.903158, Valid Loss 11.987019, Error 0.021261\n",
      "Epoch : 242/2000, Train Loss : 11.084517, Valid Loss 12.973719, Error 0.020957\n",
      "Epoch : 243/2000, Train Loss : 11.310829, Valid Loss 12.428993, Error 0.020910\n",
      "Epoch : 244/2000, Train Loss : 10.852968, Valid Loss 12.097892, Error 0.021016\n",
      "Epoch : 245/2000, Train Loss : 10.961814, Valid Loss 11.598359, Error 0.019907\n",
      "Epoch : 246/2000, Train Loss : 11.766756, Valid Loss 12.976342, Error 0.022658\n",
      "Epoch : 247/2000, Train Loss : 11.072692, Valid Loss 11.388487, Error 0.020052\n",
      "Epoch : 248/2000, Train Loss : 10.773920, Valid Loss 13.130543, Error 0.023327\n",
      "Epoch : 249/2000, Train Loss : 12.019678, Valid Loss 13.044424, Error 0.023080\n",
      "Epoch : 250/2000, Train Loss : 11.384554, Valid Loss 11.637011, Error 0.020375\n",
      "Epoch : 251/2000, Train Loss : 11.247500, Valid Loss 11.622746, Error 0.020133\n",
      "Epoch : 252/2000, Train Loss : 11.007942, Valid Loss 12.841138, Error 0.022480\n",
      "Epoch : 253/2000, Train Loss : 10.458678, Valid Loss 11.373836, Error 0.019764\n",
      "Epoch : 254/2000, Train Loss : 11.002060, Valid Loss 12.227644, Error 0.021001\n",
      "Epoch : 255/2000, Train Loss : 10.803947, Valid Loss 14.214631, Error 0.024151\n",
      "Epoch : 256/2000, Train Loss : 10.846179, Valid Loss 11.333277, Error 0.019373\n",
      "Epoch : 257/2000, Train Loss : 10.227101, Valid Loss 14.642736, Error 0.024605\n",
      "Epoch : 258/2000, Train Loss : 12.331659, Valid Loss 15.785777, Error 0.023673\n",
      "Epoch : 259/2000, Train Loss : 11.701659, Valid Loss 11.994251, Error 0.020841\n",
      "Epoch : 260/2000, Train Loss : 11.413263, Valid Loss 11.758943, Error 0.020012\n",
      "Epoch : 261/2000, Train Loss : 10.815076, Valid Loss 11.418449, Error 0.020492\n",
      "Epoch : 262/2000, Train Loss : 10.399166, Valid Loss 11.763140, Error 0.020292\n",
      "Epoch : 263/2000, Train Loss : 10.121401, Valid Loss 11.549277, Error 0.020134\n",
      "Epoch : 264/2000, Train Loss : 11.018462, Valid Loss 12.139089, Error 0.020894\n",
      "Epoch : 265/2000, Train Loss : 10.109433, Valid Loss 11.340496, Error 0.019766\n",
      "Epoch : 266/2000, Train Loss : 9.880497, Valid Loss 13.001040, Error 0.022920\n",
      "Epoch : 267/2000, Train Loss : 10.239725, Valid Loss 12.715033, Error 0.021558\n",
      "Epoch : 268/2000, Train Loss : 10.364144, Valid Loss 12.666753, Error 0.020763\n",
      "Epoch : 269/2000, Train Loss : 10.800999, Valid Loss 11.117961, Error 0.019953\n",
      "Epoch : 270/2000, Train Loss : 10.962074, Valid Loss 11.295869, Error 0.019987\n",
      "Epoch : 271/2000, Train Loss : 10.231505, Valid Loss 12.446639, Error 0.022218\n",
      "Epoch : 272/2000, Train Loss : 10.588598, Valid Loss 11.324788, Error 0.020279\n",
      "Epoch : 273/2000, Train Loss : 10.864017, Valid Loss 12.396382, Error 0.021892\n",
      "Epoch : 274/2000, Train Loss : 10.313357, Valid Loss 11.313763, Error 0.019247\n",
      "Epoch : 275/2000, Train Loss : 9.758544, Valid Loss 11.515811, Error 0.020879\n",
      "Epoch : 276/2000, Train Loss : 10.224067, Valid Loss 11.390864, Error 0.019779\n",
      "Epoch : 277/2000, Train Loss : 9.703609, Valid Loss 11.514048, Error 0.019777\n",
      "Epoch : 278/2000, Train Loss : 10.744529, Valid Loss 11.710079, Error 0.020231\n",
      "Epoch : 279/2000, Train Loss : 9.970634, Valid Loss 11.824394, Error 0.020616\n",
      "Epoch : 280/2000, Train Loss : 9.881947, Valid Loss 11.697543, Error 0.020202\n",
      "Epoch : 281/2000, Train Loss : 11.906076, Valid Loss 15.607445, Error 0.025192\n",
      "Epoch : 282/2000, Train Loss : 10.259238, Valid Loss 11.435671, Error 0.020338\n",
      "Epoch : 283/2000, Train Loss : 10.242651, Valid Loss 12.032141, Error 0.019909\n",
      "Epoch : 284/2000, Train Loss : 9.876048, Valid Loss 11.519206, Error 0.019612\n",
      "Epoch : 285/2000, Train Loss : 10.557283, Valid Loss 12.768555, Error 0.022436\n",
      "Epoch : 286/2000, Train Loss : 9.727678, Valid Loss 12.097293, Error 0.021277\n",
      "Epoch : 287/2000, Train Loss : 9.756061, Valid Loss 11.266136, Error 0.019494\n",
      "Epoch : 288/2000, Train Loss : 9.878676, Valid Loss 11.245414, Error 0.018541\n",
      "Epoch : 289/2000, Train Loss : 9.579757, Valid Loss 12.097200, Error 0.020158\n",
      "Epoch : 290/2000, Train Loss : 11.332141, Valid Loss 11.704501, Error 0.019751\n",
      "Early stopping\n",
      "Total Error Mean 0.044655\n"
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "total_error = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    error = torch.sum(torch.abs(outputs - targets) / targets) / len(targets)\n",
    "    total_error += error\n",
    "    \n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, Error {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      error))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Total Error Mean {:4f}'.format(total_error/(epoch+1)))\n",
    "        break\n",
    "    \n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01162acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ec1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a04e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b344b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce56a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a81be170",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM_atn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc975cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model_atn = LSTM_atn(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_atn.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9767a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 17809.330811, Valid Loss 17810.959961, Error 1.000306\n",
      "Epoch : 2/2000, Train Loss : 17705.975586, Valid Loss 17595.224284, Error 0.994116\n",
      "Epoch : 3/2000, Train Loss : 17284.637370, Valid Loss 16812.329427, Error 0.971321\n",
      "Epoch : 4/2000, Train Loss : 15836.249349, Valid Loss 14409.176270, Error 0.897749\n",
      "Epoch : 5/2000, Train Loss : 12091.572144, Valid Loss 9031.609212, Error 0.705918\n",
      "Epoch : 6/2000, Train Loss : 5661.638814, Valid Loss 2289.317668, Error 0.336069\n",
      "Epoch : 7/2000, Train Loss : 864.994368, Valid Loss 309.332011, Error 0.103828\n",
      "Epoch : 8/2000, Train Loss : 333.044937, Valid Loss 307.961863, Error 0.103487\n",
      "Epoch : 9/2000, Train Loss : 307.390025, Valid Loss 303.463531, Error 0.100660\n",
      "Epoch : 10/2000, Train Loss : 304.888391, Valid Loss 302.669324, Error 0.101335\n",
      "Epoch : 11/2000, Train Loss : 304.865779, Valid Loss 302.659442, Error 0.101172\n",
      "Epoch : 12/2000, Train Loss : 304.510859, Valid Loss 302.650981, Error 0.101199\n",
      "Epoch : 13/2000, Train Loss : 305.871648, Valid Loss 302.647939, Error 0.101197\n",
      "Epoch : 14/2000, Train Loss : 305.770130, Valid Loss 302.704974, Error 0.101066\n",
      "Epoch : 15/2000, Train Loss : 304.996590, Valid Loss 302.673843, Error 0.101383\n",
      "Epoch : 16/2000, Train Loss : 305.246563, Valid Loss 302.668749, Error 0.101090\n",
      "Epoch : 17/2000, Train Loss : 303.839524, Valid Loss 302.618457, Error 0.101293\n",
      "Epoch : 18/2000, Train Loss : 305.012335, Valid Loss 301.744751, Error 0.100623\n",
      "Epoch : 19/2000, Train Loss : 301.930566, Valid Loss 297.461741, Error 0.099133\n",
      "Epoch : 20/2000, Train Loss : 294.982937, Valid Loss 284.843602, Error 0.095776\n",
      "Epoch : 21/2000, Train Loss : 279.058219, Valid Loss 268.349785, Error 0.092487\n",
      "Epoch : 22/2000, Train Loss : 260.118399, Valid Loss 241.610135, Error 0.087866\n",
      "Epoch : 23/2000, Train Loss : 231.104549, Valid Loss 212.160027, Error 0.081286\n",
      "Epoch : 24/2000, Train Loss : 202.217246, Valid Loss 182.613594, Error 0.076762\n",
      "Epoch : 25/2000, Train Loss : 181.018814, Valid Loss 171.430280, Error 0.078248\n",
      "Epoch : 26/2000, Train Loss : 162.269876, Valid Loss 150.450205, Error 0.073670\n",
      "Epoch : 27/2000, Train Loss : 150.793246, Valid Loss 135.614549, Error 0.070646\n",
      "Epoch : 28/2000, Train Loss : 134.826805, Valid Loss 131.571435, Error 0.072550\n",
      "Epoch : 29/2000, Train Loss : 123.137685, Valid Loss 109.139913, Error 0.063588\n",
      "Epoch : 30/2000, Train Loss : 106.187047, Valid Loss 103.234091, Error 0.066715\n",
      "Epoch : 31/2000, Train Loss : 93.391049, Valid Loss 85.026263, Error 0.060690\n",
      "Epoch : 32/2000, Train Loss : 79.871421, Valid Loss 73.919950, Error 0.056769\n",
      "Epoch : 33/2000, Train Loss : 70.527724, Valid Loss 66.415614, Error 0.051595\n",
      "Epoch : 34/2000, Train Loss : 66.209872, Valid Loss 72.416106, Error 0.060099\n",
      "Epoch : 35/2000, Train Loss : 61.436560, Valid Loss 67.237984, Error 0.047420\n",
      "Epoch : 36/2000, Train Loss : 57.532169, Valid Loss 54.422691, Error 0.049424\n",
      "Epoch : 37/2000, Train Loss : 59.714779, Valid Loss 53.404994, Error 0.048753\n",
      "Epoch : 38/2000, Train Loss : 49.984588, Valid Loss 47.615477, Error 0.044414\n",
      "Epoch : 39/2000, Train Loss : 46.039162, Valid Loss 51.960218, Error 0.050374\n",
      "Epoch : 40/2000, Train Loss : 46.017713, Valid Loss 57.027971, Error 0.054243\n",
      "Epoch : 41/2000, Train Loss : 42.726984, Valid Loss 40.665337, Error 0.041712\n",
      "Epoch : 42/2000, Train Loss : 42.565732, Valid Loss 40.593714, Error 0.038742\n",
      "Epoch : 43/2000, Train Loss : 41.677398, Valid Loss 40.436266, Error 0.044585\n",
      "Epoch : 44/2000, Train Loss : 41.180150, Valid Loss 41.412118, Error 0.045286\n",
      "Epoch : 45/2000, Train Loss : 36.492802, Valid Loss 44.797197, Error 0.048392\n",
      "Epoch : 46/2000, Train Loss : 37.806285, Valid Loss 35.988276, Error 0.035861\n",
      "Epoch : 47/2000, Train Loss : 34.347050, Valid Loss 33.442544, Error 0.037014\n",
      "Epoch : 48/2000, Train Loss : 34.453866, Valid Loss 34.587525, Error 0.040946\n",
      "Epoch : 49/2000, Train Loss : 32.860347, Valid Loss 32.419350, Error 0.036737\n",
      "Epoch : 50/2000, Train Loss : 31.566351, Valid Loss 30.785404, Error 0.035703\n",
      "Epoch : 51/2000, Train Loss : 31.781192, Valid Loss 33.239361, Error 0.040315\n",
      "Epoch : 52/2000, Train Loss : 31.713889, Valid Loss 32.095038, Error 0.039812\n",
      "Epoch : 53/2000, Train Loss : 29.738190, Valid Loss 29.115042, Error 0.035569\n",
      "Epoch : 54/2000, Train Loss : 29.264426, Valid Loss 35.340926, Error 0.042580\n",
      "Epoch : 55/2000, Train Loss : 28.709788, Valid Loss 27.837178, Error 0.035092\n",
      "Epoch : 56/2000, Train Loss : 28.717980, Valid Loss 39.303321, Error 0.045135\n",
      "Epoch : 57/2000, Train Loss : 33.182406, Valid Loss 30.148183, Error 0.038801\n",
      "Epoch : 58/2000, Train Loss : 27.756456, Valid Loss 26.514508, Error 0.034089\n",
      "Epoch : 59/2000, Train Loss : 27.555586, Valid Loss 27.132650, Error 0.030812\n",
      "Epoch : 60/2000, Train Loss : 26.154772, Valid Loss 25.008397, Error 0.032326\n",
      "Epoch : 61/2000, Train Loss : 25.657877, Valid Loss 24.500397, Error 0.032606\n",
      "Epoch : 62/2000, Train Loss : 25.084048, Valid Loss 25.357634, Error 0.034498\n",
      "Epoch : 63/2000, Train Loss : 25.638261, Valid Loss 24.148072, Error 0.031199\n",
      "Epoch : 64/2000, Train Loss : 24.737846, Valid Loss 31.264858, Error 0.040834\n",
      "Epoch : 65/2000, Train Loss : 25.324965, Valid Loss 29.161287, Error 0.030903\n",
      "Epoch : 66/2000, Train Loss : 24.986799, Valid Loss 22.825878, Error 0.029811\n",
      "Epoch : 67/2000, Train Loss : 26.895409, Valid Loss 22.741001, Error 0.029530\n",
      "Epoch : 68/2000, Train Loss : 25.754413, Valid Loss 22.124317, Error 0.029959\n",
      "Epoch : 69/2000, Train Loss : 23.518772, Valid Loss 22.063396, Error 0.030747\n",
      "Epoch : 70/2000, Train Loss : 24.132168, Valid Loss 21.584266, Error 0.030558\n",
      "Epoch : 71/2000, Train Loss : 23.769559, Valid Loss 21.376193, Error 0.028907\n",
      "Epoch : 72/2000, Train Loss : 22.980917, Valid Loss 23.545513, Error 0.028624\n",
      "Epoch : 73/2000, Train Loss : 22.249318, Valid Loss 20.538466, Error 0.029081\n",
      "Epoch : 74/2000, Train Loss : 22.848860, Valid Loss 23.259967, Error 0.028358\n",
      "Epoch : 75/2000, Train Loss : 22.726340, Valid Loss 29.218923, Error 0.038484\n",
      "Epoch : 76/2000, Train Loss : 22.519780, Valid Loss 22.976978, Error 0.027591\n",
      "Epoch : 77/2000, Train Loss : 22.224640, Valid Loss 20.671535, Error 0.030192\n",
      "Epoch : 78/2000, Train Loss : 22.019984, Valid Loss 19.979815, Error 0.027847\n",
      "Epoch : 79/2000, Train Loss : 21.251437, Valid Loss 19.909730, Error 0.029181\n",
      "Epoch : 80/2000, Train Loss : 23.871647, Valid Loss 19.685404, Error 0.028469\n",
      "Epoch : 81/2000, Train Loss : 22.462409, Valid Loss 23.595123, Error 0.033792\n",
      "Epoch : 82/2000, Train Loss : 22.756031, Valid Loss 21.348804, Error 0.031814\n",
      "Epoch : 83/2000, Train Loss : 20.565651, Valid Loss 18.932759, Error 0.028232\n",
      "Epoch : 84/2000, Train Loss : 20.320841, Valid Loss 18.805175, Error 0.028904\n",
      "Epoch : 85/2000, Train Loss : 20.487232, Valid Loss 20.836976, Error 0.026292\n",
      "Epoch : 86/2000, Train Loss : 22.903283, Valid Loss 26.264222, Error 0.028518\n",
      "Epoch : 87/2000, Train Loss : 22.767014, Valid Loss 22.139053, Error 0.032966\n",
      "Epoch : 88/2000, Train Loss : 19.908487, Valid Loss 19.947145, Error 0.026200\n",
      "Epoch : 89/2000, Train Loss : 20.224420, Valid Loss 19.183280, Error 0.026036\n",
      "Epoch : 90/2000, Train Loss : 19.076594, Valid Loss 21.019344, Error 0.032015\n",
      "Epoch : 91/2000, Train Loss : 21.350584, Valid Loss 19.894051, Error 0.030676\n",
      "Epoch : 92/2000, Train Loss : 19.178751, Valid Loss 17.947539, Error 0.027810\n",
      "Epoch : 93/2000, Train Loss : 19.827517, Valid Loss 17.811915, Error 0.025798\n",
      "Epoch : 94/2000, Train Loss : 18.882040, Valid Loss 19.729854, Error 0.025729\n",
      "Epoch : 95/2000, Train Loss : 20.419017, Valid Loss 18.268052, Error 0.028552\n",
      "Epoch : 96/2000, Train Loss : 19.723601, Valid Loss 18.260470, Error 0.028284\n",
      "Epoch : 97/2000, Train Loss : 18.680379, Valid Loss 17.549673, Error 0.027500\n",
      "Epoch : 98/2000, Train Loss : 20.202284, Valid Loss 21.224941, Error 0.032947\n",
      "Epoch : 99/2000, Train Loss : 18.513400, Valid Loss 17.159269, Error 0.026223\n",
      "Epoch : 100/2000, Train Loss : 20.578729, Valid Loss 20.393960, Error 0.031477\n",
      "Epoch : 101/2000, Train Loss : 20.411020, Valid Loss 17.695586, Error 0.025436\n",
      "Epoch : 102/2000, Train Loss : 19.684789, Valid Loss 20.647983, Error 0.031246\n",
      "Epoch : 103/2000, Train Loss : 18.784107, Valid Loss 17.557364, Error 0.028217\n",
      "Epoch : 104/2000, Train Loss : 18.795231, Valid Loss 16.795346, Error 0.025428\n",
      "Epoch : 105/2000, Train Loss : 17.823166, Valid Loss 17.067647, Error 0.024550\n",
      "Epoch : 106/2000, Train Loss : 18.284444, Valid Loss 17.949341, Error 0.024490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 107/2000, Train Loss : 18.015500, Valid Loss 16.701385, Error 0.024514\n",
      "Epoch : 108/2000, Train Loss : 20.050982, Valid Loss 23.469064, Error 0.026794\n",
      "Epoch : 109/2000, Train Loss : 20.760057, Valid Loss 17.867274, Error 0.028161\n",
      "Epoch : 110/2000, Train Loss : 18.469908, Valid Loss 17.019600, Error 0.026741\n",
      "Epoch : 111/2000, Train Loss : 17.567713, Valid Loss 16.617675, Error 0.026969\n",
      "Epoch : 112/2000, Train Loss : 16.924603, Valid Loss 15.930993, Error 0.025613\n",
      "Epoch : 113/2000, Train Loss : 16.960772, Valid Loss 16.211357, Error 0.024662\n",
      "Epoch : 114/2000, Train Loss : 17.197542, Valid Loss 17.076216, Error 0.028177\n",
      "Epoch : 115/2000, Train Loss : 17.906984, Valid Loss 16.759534, Error 0.024631\n",
      "Epoch : 116/2000, Train Loss : 18.386769, Valid Loss 18.142805, Error 0.028709\n",
      "Epoch : 117/2000, Train Loss : 20.635526, Valid Loss 17.396743, Error 0.024467\n",
      "Epoch : 118/2000, Train Loss : 18.664769, Valid Loss 19.164827, Error 0.030087\n",
      "Epoch : 119/2000, Train Loss : 16.943431, Valid Loss 15.704384, Error 0.024807\n",
      "Epoch : 120/2000, Train Loss : 19.352914, Valid Loss 18.961077, Error 0.030075\n",
      "Epoch : 121/2000, Train Loss : 17.878053, Valid Loss 15.548379, Error 0.024656\n",
      "Epoch : 122/2000, Train Loss : 16.658066, Valid Loss 15.552042, Error 0.025169\n",
      "Epoch : 123/2000, Train Loss : 16.744237, Valid Loss 16.199723, Error 0.027160\n",
      "Epoch : 124/2000, Train Loss : 16.466288, Valid Loss 15.444415, Error 0.024333\n",
      "Epoch : 125/2000, Train Loss : 15.852169, Valid Loss 15.778388, Error 0.026223\n",
      "Epoch : 126/2000, Train Loss : 17.543202, Valid Loss 18.691376, Error 0.028902\n",
      "Epoch : 127/2000, Train Loss : 16.228023, Valid Loss 15.859134, Error 0.026592\n",
      "Epoch : 128/2000, Train Loss : 16.349444, Valid Loss 16.672270, Error 0.027144\n",
      "Epoch : 129/2000, Train Loss : 15.775228, Valid Loss 14.911986, Error 0.024583\n",
      "Epoch : 130/2000, Train Loss : 16.463908, Valid Loss 14.981381, Error 0.024485\n",
      "Epoch : 131/2000, Train Loss : 16.842938, Valid Loss 17.040600, Error 0.023517\n",
      "Epoch : 132/2000, Train Loss : 17.153636, Valid Loss 16.468745, Error 0.027347\n",
      "Epoch : 133/2000, Train Loss : 15.836366, Valid Loss 15.324285, Error 0.025783\n",
      "Epoch : 134/2000, Train Loss : 15.939993, Valid Loss 14.848260, Error 0.024006\n",
      "Epoch : 135/2000, Train Loss : 16.466553, Valid Loss 16.218381, Error 0.022862\n",
      "Epoch : 136/2000, Train Loss : 15.792518, Valid Loss 16.447478, Error 0.023190\n",
      "Epoch : 137/2000, Train Loss : 15.973652, Valid Loss 14.581871, Error 0.023757\n",
      "Epoch : 138/2000, Train Loss : 16.028328, Valid Loss 23.171802, Error 0.032863\n",
      "Epoch : 139/2000, Train Loss : 17.759403, Valid Loss 14.809508, Error 0.023943\n",
      "Epoch : 140/2000, Train Loss : 16.064561, Valid Loss 14.915464, Error 0.025403\n",
      "Epoch : 141/2000, Train Loss : 15.676681, Valid Loss 16.816469, Error 0.027561\n",
      "Epoch : 142/2000, Train Loss : 16.577583, Valid Loss 14.948979, Error 0.025602\n",
      "Epoch : 143/2000, Train Loss : 14.956469, Valid Loss 15.964564, Error 0.023012\n",
      "Epoch : 144/2000, Train Loss : 15.429332, Valid Loss 16.576987, Error 0.022701\n",
      "Epoch : 145/2000, Train Loss : 16.803146, Valid Loss 14.962275, Error 0.022730\n",
      "Epoch : 146/2000, Train Loss : 14.874056, Valid Loss 14.744539, Error 0.023983\n",
      "Epoch : 147/2000, Train Loss : 16.538991, Valid Loss 17.871481, Error 0.024203\n",
      "Epoch : 148/2000, Train Loss : 16.345688, Valid Loss 14.841689, Error 0.022752\n",
      "Epoch : 149/2000, Train Loss : 15.202723, Valid Loss 16.705504, Error 0.028020\n",
      "Epoch : 150/2000, Train Loss : 15.342067, Valid Loss 17.247952, Error 0.023013\n",
      "Epoch : 151/2000, Train Loss : 16.474558, Valid Loss 15.802786, Error 0.022737\n",
      "Epoch : 152/2000, Train Loss : 15.313528, Valid Loss 15.107588, Error 0.022407\n",
      "Epoch : 153/2000, Train Loss : 16.641251, Valid Loss 16.206017, Error 0.023532\n",
      "Epoch : 154/2000, Train Loss : 15.954061, Valid Loss 14.071251, Error 0.023221\n",
      "Epoch : 155/2000, Train Loss : 15.509570, Valid Loss 14.812662, Error 0.022611\n",
      "Epoch : 156/2000, Train Loss : 14.278702, Valid Loss 14.744576, Error 0.021845\n",
      "Epoch : 157/2000, Train Loss : 15.284162, Valid Loss 14.468972, Error 0.021890\n",
      "Epoch : 158/2000, Train Loss : 14.725482, Valid Loss 14.401839, Error 0.021900\n",
      "Epoch : 159/2000, Train Loss : 14.330108, Valid Loss 13.873995, Error 0.023330\n",
      "Epoch : 160/2000, Train Loss : 14.583927, Valid Loss 15.929894, Error 0.026483\n",
      "Epoch : 161/2000, Train Loss : 14.445459, Valid Loss 13.985912, Error 0.022417\n",
      "Epoch : 162/2000, Train Loss : 13.899270, Valid Loss 14.403812, Error 0.022723\n",
      "Epoch : 163/2000, Train Loss : 14.732819, Valid Loss 13.856736, Error 0.022515\n",
      "Epoch : 164/2000, Train Loss : 14.404167, Valid Loss 14.015583, Error 0.022414\n",
      "Epoch : 165/2000, Train Loss : 14.551471, Valid Loss 17.167179, Error 0.023415\n",
      "Epoch : 166/2000, Train Loss : 14.388883, Valid Loss 14.471624, Error 0.024933\n",
      "Epoch : 167/2000, Train Loss : 14.203066, Valid Loss 16.311621, Error 0.027444\n",
      "Epoch : 168/2000, Train Loss : 14.089339, Valid Loss 13.847697, Error 0.023684\n",
      "Epoch : 169/2000, Train Loss : 13.657814, Valid Loss 15.851888, Error 0.026391\n",
      "Epoch : 170/2000, Train Loss : 13.875185, Valid Loss 13.596336, Error 0.023032\n",
      "Epoch : 171/2000, Train Loss : 14.001449, Valid Loss 13.850691, Error 0.023662\n",
      "Epoch : 172/2000, Train Loss : 13.418210, Valid Loss 13.992544, Error 0.020917\n",
      "Epoch : 173/2000, Train Loss : 13.942282, Valid Loss 15.009664, Error 0.025409\n",
      "Epoch : 174/2000, Train Loss : 13.626347, Valid Loss 13.439611, Error 0.022344\n",
      "Epoch : 175/2000, Train Loss : 14.407864, Valid Loss 13.867472, Error 0.021558\n",
      "Epoch : 176/2000, Train Loss : 13.545484, Valid Loss 13.249731, Error 0.022489\n",
      "Epoch : 177/2000, Train Loss : 14.718050, Valid Loss 15.845875, Error 0.026448\n",
      "Epoch : 178/2000, Train Loss : 14.088609, Valid Loss 14.403192, Error 0.024121\n",
      "Epoch : 179/2000, Train Loss : 13.981293, Valid Loss 13.035307, Error 0.021524\n",
      "Epoch : 180/2000, Train Loss : 14.231771, Valid Loss 14.012843, Error 0.020765\n",
      "Epoch : 181/2000, Train Loss : 16.341836, Valid Loss 13.426087, Error 0.021458\n",
      "Epoch : 182/2000, Train Loss : 13.598323, Valid Loss 14.734820, Error 0.021503\n",
      "Epoch : 183/2000, Train Loss : 13.704114, Valid Loss 13.006564, Error 0.021180\n",
      "Epoch : 184/2000, Train Loss : 14.846539, Valid Loss 13.413905, Error 0.021165\n",
      "Epoch : 185/2000, Train Loss : 12.916494, Valid Loss 12.868377, Error 0.022019\n",
      "Epoch : 186/2000, Train Loss : 13.613392, Valid Loss 17.415493, Error 0.023030\n",
      "Epoch : 187/2000, Train Loss : 14.416911, Valid Loss 13.078447, Error 0.021475\n",
      "Epoch : 188/2000, Train Loss : 16.150133, Valid Loss 20.485415, Error 0.031802\n",
      "Epoch : 189/2000, Train Loss : 15.074148, Valid Loss 13.346110, Error 0.022501\n",
      "Epoch : 190/2000, Train Loss : 13.186835, Valid Loss 13.584016, Error 0.020724\n",
      "Epoch : 191/2000, Train Loss : 12.898331, Valid Loss 13.456177, Error 0.023243\n",
      "Epoch : 192/2000, Train Loss : 13.720936, Valid Loss 12.932963, Error 0.021494\n",
      "Epoch : 193/2000, Train Loss : 12.818570, Valid Loss 13.637659, Error 0.021133\n",
      "Epoch : 194/2000, Train Loss : 12.966597, Valid Loss 12.789062, Error 0.022517\n",
      "Epoch : 195/2000, Train Loss : 12.804606, Valid Loss 12.881197, Error 0.020275\n",
      "Epoch : 196/2000, Train Loss : 13.506928, Valid Loss 13.298200, Error 0.021031\n",
      "Epoch : 197/2000, Train Loss : 12.540315, Valid Loss 12.804909, Error 0.020540\n",
      "Epoch : 198/2000, Train Loss : 12.273641, Valid Loss 12.583218, Error 0.020708\n",
      "Epoch : 199/2000, Train Loss : 12.678342, Valid Loss 12.832048, Error 0.021515\n",
      "Epoch : 200/2000, Train Loss : 12.285958, Valid Loss 12.286644, Error 0.020579\n",
      "Epoch : 201/2000, Train Loss : 12.841413, Valid Loss 19.192565, Error 0.024765\n",
      "Epoch : 202/2000, Train Loss : 14.082746, Valid Loss 13.105556, Error 0.020877\n",
      "Epoch : 203/2000, Train Loss : 12.176194, Valid Loss 13.213330, Error 0.020582\n",
      "Epoch : 204/2000, Train Loss : 13.660206, Valid Loss 14.137820, Error 0.020875\n",
      "Epoch : 205/2000, Train Loss : 12.662642, Valid Loss 14.359584, Error 0.023574\n",
      "Epoch : 206/2000, Train Loss : 13.152159, Valid Loss 17.161983, Error 0.023647\n",
      "Epoch : 207/2000, Train Loss : 12.500590, Valid Loss 12.387052, Error 0.020854\n",
      "Epoch : 208/2000, Train Loss : 12.244468, Valid Loss 12.759916, Error 0.022133\n",
      "Epoch : 209/2000, Train Loss : 14.918685, Valid Loss 13.515621, Error 0.023564\n",
      "Epoch : 210/2000, Train Loss : 11.860101, Valid Loss 12.760691, Error 0.020329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 211/2000, Train Loss : 11.852481, Valid Loss 12.276677, Error 0.020806\n",
      "Epoch : 212/2000, Train Loss : 12.579038, Valid Loss 19.752625, Error 0.030108\n",
      "Epoch : 213/2000, Train Loss : 12.566195, Valid Loss 12.422962, Error 0.020672\n",
      "Epoch : 214/2000, Train Loss : 11.922456, Valid Loss 12.249483, Error 0.020549\n",
      "Epoch : 215/2000, Train Loss : 11.675397, Valid Loss 12.251623, Error 0.019911\n",
      "Epoch : 216/2000, Train Loss : 14.083663, Valid Loss 12.488376, Error 0.020904\n",
      "Epoch : 217/2000, Train Loss : 12.578484, Valid Loss 12.106949, Error 0.020747\n",
      "Epoch : 218/2000, Train Loss : 12.644394, Valid Loss 12.672681, Error 0.020528\n",
      "Epoch : 219/2000, Train Loss : 11.650748, Valid Loss 12.717748, Error 0.020180\n",
      "Epoch : 220/2000, Train Loss : 11.901282, Valid Loss 12.885255, Error 0.022878\n",
      "Epoch : 221/2000, Train Loss : 11.553152, Valid Loss 13.185049, Error 0.021992\n",
      "Epoch : 222/2000, Train Loss : 12.768729, Valid Loss 12.113019, Error 0.020840\n",
      "Epoch : 223/2000, Train Loss : 11.026358, Valid Loss 11.979122, Error 0.021260\n",
      "Epoch : 224/2000, Train Loss : 11.144904, Valid Loss 11.780111, Error 0.019921\n",
      "Epoch : 225/2000, Train Loss : 12.796715, Valid Loss 12.754715, Error 0.021809\n",
      "Epoch : 226/2000, Train Loss : 12.715975, Valid Loss 13.974765, Error 0.021190\n",
      "Epoch : 227/2000, Train Loss : 13.570178, Valid Loss 13.896061, Error 0.021139\n",
      "Epoch : 228/2000, Train Loss : 11.918691, Valid Loss 12.243158, Error 0.020188\n",
      "Epoch : 229/2000, Train Loss : 12.543974, Valid Loss 14.562496, Error 0.025226\n",
      "Epoch : 230/2000, Train Loss : 13.134750, Valid Loss 13.117348, Error 0.023007\n",
      "Epoch : 231/2000, Train Loss : 12.060306, Valid Loss 13.048125, Error 0.020281\n",
      "Epoch : 232/2000, Train Loss : 11.079217, Valid Loss 11.940030, Error 0.019710\n",
      "Epoch : 233/2000, Train Loss : 11.830700, Valid Loss 12.076298, Error 0.020094\n",
      "Epoch : 234/2000, Train Loss : 11.253968, Valid Loss 11.807929, Error 0.020115\n",
      "Epoch : 235/2000, Train Loss : 12.647452, Valid Loss 11.842780, Error 0.020211\n",
      "Epoch : 236/2000, Train Loss : 12.206315, Valid Loss 15.667603, Error 0.023235\n",
      "Epoch : 237/2000, Train Loss : 12.247444, Valid Loss 11.542007, Error 0.019855\n",
      "Epoch : 238/2000, Train Loss : 10.440155, Valid Loss 11.526975, Error 0.019494\n",
      "Epoch : 239/2000, Train Loss : 11.208304, Valid Loss 11.606062, Error 0.019755\n",
      "Epoch : 240/2000, Train Loss : 11.252214, Valid Loss 12.396505, Error 0.021030\n",
      "Epoch : 241/2000, Train Loss : 11.124144, Valid Loss 11.386263, Error 0.019243\n",
      "Epoch : 242/2000, Train Loss : 10.670738, Valid Loss 11.656441, Error 0.019145\n",
      "Epoch : 243/2000, Train Loss : 10.653261, Valid Loss 11.672277, Error 0.019355\n",
      "Epoch : 244/2000, Train Loss : 10.864561, Valid Loss 12.314850, Error 0.019524\n",
      "Epoch : 245/2000, Train Loss : 11.149799, Valid Loss 11.405461, Error 0.019146\n",
      "Epoch : 246/2000, Train Loss : 11.667649, Valid Loss 13.167194, Error 0.022469\n",
      "Epoch : 247/2000, Train Loss : 10.902649, Valid Loss 11.666993, Error 0.019502\n",
      "Epoch : 248/2000, Train Loss : 10.259301, Valid Loss 13.642792, Error 0.023713\n",
      "Epoch : 249/2000, Train Loss : 12.535784, Valid Loss 12.620863, Error 0.022331\n",
      "Epoch : 250/2000, Train Loss : 11.465052, Valid Loss 11.493313, Error 0.019107\n",
      "Epoch : 251/2000, Train Loss : 12.449833, Valid Loss 11.493059, Error 0.019319\n",
      "Epoch : 252/2000, Train Loss : 10.652822, Valid Loss 13.547905, Error 0.023077\n",
      "Epoch : 253/2000, Train Loss : 10.514227, Valid Loss 11.393319, Error 0.019069\n",
      "Epoch : 254/2000, Train Loss : 10.388070, Valid Loss 11.135660, Error 0.019633\n",
      "Epoch : 255/2000, Train Loss : 11.269833, Valid Loss 18.349403, Error 0.027995\n",
      "Epoch : 256/2000, Train Loss : 10.876356, Valid Loss 11.631164, Error 0.020708\n",
      "Epoch : 257/2000, Train Loss : 10.479989, Valid Loss 14.641957, Error 0.025098\n",
      "Epoch : 258/2000, Train Loss : 11.620840, Valid Loss 13.567712, Error 0.020887\n",
      "Epoch : 259/2000, Train Loss : 10.620241, Valid Loss 11.258483, Error 0.019124\n",
      "Epoch : 260/2000, Train Loss : 10.376185, Valid Loss 11.315916, Error 0.019525\n",
      "Epoch : 261/2000, Train Loss : 10.444768, Valid Loss 11.320101, Error 0.019579\n",
      "Epoch : 262/2000, Train Loss : 10.673791, Valid Loss 12.692192, Error 0.021222\n",
      "Epoch : 263/2000, Train Loss : 10.441598, Valid Loss 11.527309, Error 0.019355\n",
      "Epoch : 264/2000, Train Loss : 11.448070, Valid Loss 13.941318, Error 0.023624\n",
      "Epoch : 265/2000, Train Loss : 10.314495, Valid Loss 11.265445, Error 0.018535\n",
      "Epoch : 266/2000, Train Loss : 10.073419, Valid Loss 13.586852, Error 0.023110\n",
      "Epoch : 267/2000, Train Loss : 10.145832, Valid Loss 11.890798, Error 0.019606\n",
      "Epoch : 268/2000, Train Loss : 9.604888, Valid Loss 10.798922, Error 0.018716\n",
      "Epoch : 269/2000, Train Loss : 9.921645, Valid Loss 10.994359, Error 0.019323\n",
      "Epoch : 270/2000, Train Loss : 10.598705, Valid Loss 11.085733, Error 0.018511\n",
      "Epoch : 271/2000, Train Loss : 9.931567, Valid Loss 11.550486, Error 0.020465\n",
      "Epoch : 272/2000, Train Loss : 10.151783, Valid Loss 11.035641, Error 0.019691\n",
      "Epoch : 273/2000, Train Loss : 9.928843, Valid Loss 13.469813, Error 0.023264\n",
      "Epoch : 274/2000, Train Loss : 10.593770, Valid Loss 11.336657, Error 0.018667\n",
      "Epoch : 275/2000, Train Loss : 9.874631, Valid Loss 11.062471, Error 0.020033\n",
      "Epoch : 276/2000, Train Loss : 10.531587, Valid Loss 11.113463, Error 0.019224\n",
      "Epoch : 277/2000, Train Loss : 9.832683, Valid Loss 12.261718, Error 0.021220\n",
      "Epoch : 278/2000, Train Loss : 12.069513, Valid Loss 11.183053, Error 0.020053\n",
      "Epoch : 279/2000, Train Loss : 10.098564, Valid Loss 12.096723, Error 0.021289\n",
      "Epoch : 280/2000, Train Loss : 9.383040, Valid Loss 11.022123, Error 0.018995\n",
      "Epoch : 281/2000, Train Loss : 11.751854, Valid Loss 12.439953, Error 0.021453\n",
      "Epoch : 282/2000, Train Loss : 10.274265, Valid Loss 11.389889, Error 0.021262\n",
      "Epoch : 283/2000, Train Loss : 10.368954, Valid Loss 12.207156, Error 0.020203\n",
      "Epoch : 284/2000, Train Loss : 9.768151, Valid Loss 13.976851, Error 0.020849\n",
      "Epoch : 285/2000, Train Loss : 11.779182, Valid Loss 11.461199, Error 0.020784\n",
      "Epoch : 286/2000, Train Loss : 9.431820, Valid Loss 13.015824, Error 0.022492\n",
      "Epoch : 287/2000, Train Loss : 10.477477, Valid Loss 10.800931, Error 0.020050\n",
      "Epoch : 288/2000, Train Loss : 9.784089, Valid Loss 10.831870, Error 0.018563\n",
      "Epoch : 289/2000, Train Loss : 9.897320, Valid Loss 10.757367, Error 0.018762\n",
      "Epoch : 290/2000, Train Loss : 10.497798, Valid Loss 13.199376, Error 0.020836\n",
      "Epoch : 291/2000, Train Loss : 10.615517, Valid Loss 12.900701, Error 0.022380\n",
      "Epoch : 292/2000, Train Loss : 9.696960, Valid Loss 11.871085, Error 0.019774\n",
      "Epoch : 293/2000, Train Loss : 9.638764, Valid Loss 11.182494, Error 0.018646\n",
      "Epoch : 294/2000, Train Loss : 9.286795, Valid Loss 10.512645, Error 0.018389\n",
      "Epoch : 295/2000, Train Loss : 8.734056, Valid Loss 11.045088, Error 0.018568\n",
      "Epoch : 296/2000, Train Loss : 8.863724, Valid Loss 11.435521, Error 0.020374\n",
      "Epoch : 297/2000, Train Loss : 10.203357, Valid Loss 12.553260, Error 0.020411\n",
      "Epoch : 298/2000, Train Loss : 9.907893, Valid Loss 10.364773, Error 0.018583\n",
      "Epoch : 299/2000, Train Loss : 9.388363, Valid Loss 10.498129, Error 0.018469\n",
      "Epoch : 300/2000, Train Loss : 8.888468, Valid Loss 10.184691, Error 0.017843\n",
      "Epoch : 301/2000, Train Loss : 8.903105, Valid Loss 10.865662, Error 0.020012\n",
      "Epoch : 302/2000, Train Loss : 10.293250, Valid Loss 11.776153, Error 0.020352\n",
      "Epoch : 303/2000, Train Loss : 9.276755, Valid Loss 11.349420, Error 0.020832\n",
      "Epoch : 304/2000, Train Loss : 9.269295, Valid Loss 10.541227, Error 0.018827\n",
      "Epoch : 305/2000, Train Loss : 9.765786, Valid Loss 10.754545, Error 0.020213\n",
      "Epoch : 306/2000, Train Loss : 10.208145, Valid Loss 15.170559, Error 0.024617\n",
      "Epoch : 307/2000, Train Loss : 9.764721, Valid Loss 10.644771, Error 0.019274\n",
      "Epoch : 308/2000, Train Loss : 8.542722, Valid Loss 10.731750, Error 0.019223\n",
      "Epoch : 309/2000, Train Loss : 8.627500, Valid Loss 10.407101, Error 0.018797\n",
      "Epoch : 310/2000, Train Loss : 9.009902, Valid Loss 9.945672, Error 0.018811\n",
      "Epoch : 311/2000, Train Loss : 8.574484, Valid Loss 11.507407, Error 0.019285\n",
      "Epoch : 312/2000, Train Loss : 9.114737, Valid Loss 12.223499, Error 0.021622\n",
      "Epoch : 313/2000, Train Loss : 8.934163, Valid Loss 9.968641, Error 0.018064\n",
      "Epoch : 314/2000, Train Loss : 8.660873, Valid Loss 10.341933, Error 0.018631\n",
      "Epoch : 315/2000, Train Loss : 8.565670, Valid Loss 10.451807, Error 0.018347\n",
      "Epoch : 316/2000, Train Loss : 8.269013, Valid Loss 9.997775, Error 0.018630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 317/2000, Train Loss : 8.782989, Valid Loss 10.907836, Error 0.020059\n",
      "Epoch : 318/2000, Train Loss : 9.548005, Valid Loss 10.259814, Error 0.018625\n",
      "Epoch : 319/2000, Train Loss : 8.427741, Valid Loss 10.380807, Error 0.018099\n",
      "Epoch : 320/2000, Train Loss : 8.838083, Valid Loss 9.779932, Error 0.017815\n",
      "Epoch : 321/2000, Train Loss : 8.317040, Valid Loss 13.055228, Error 0.022473\n",
      "Epoch : 322/2000, Train Loss : 12.143257, Valid Loss 10.954665, Error 0.018826\n",
      "Epoch : 323/2000, Train Loss : 8.527623, Valid Loss 9.830847, Error 0.018560\n",
      "Epoch : 324/2000, Train Loss : 8.526702, Valid Loss 10.382600, Error 0.018506\n",
      "Epoch : 325/2000, Train Loss : 8.274683, Valid Loss 9.910796, Error 0.017793\n",
      "Epoch : 326/2000, Train Loss : 8.582165, Valid Loss 11.470673, Error 0.019214\n",
      "Epoch : 327/2000, Train Loss : 8.396877, Valid Loss 9.831412, Error 0.017873\n",
      "Epoch : 328/2000, Train Loss : 9.075743, Valid Loss 10.956607, Error 0.019063\n",
      "Epoch : 329/2000, Train Loss : 8.819498, Valid Loss 10.886836, Error 0.020378\n",
      "Epoch : 330/2000, Train Loss : 8.199719, Valid Loss 10.812659, Error 0.019040\n",
      "Epoch : 331/2000, Train Loss : 8.742637, Valid Loss 10.612158, Error 0.018805\n",
      "Epoch : 332/2000, Train Loss : 8.160714, Valid Loss 14.002347, Error 0.023688\n",
      "Epoch : 333/2000, Train Loss : 8.688175, Valid Loss 10.370040, Error 0.018595\n",
      "Epoch : 334/2000, Train Loss : 9.114733, Valid Loss 10.255024, Error 0.019176\n",
      "Epoch : 335/2000, Train Loss : 7.852367, Valid Loss 10.022210, Error 0.017586\n",
      "Epoch : 336/2000, Train Loss : 8.118396, Valid Loss 11.007679, Error 0.019349\n",
      "Epoch : 337/2000, Train Loss : 8.031029, Valid Loss 9.988432, Error 0.018062\n",
      "Epoch : 338/2000, Train Loss : 9.761067, Valid Loss 10.439968, Error 0.018514\n",
      "Epoch : 339/2000, Train Loss : 8.502545, Valid Loss 9.534617, Error 0.017881\n",
      "Epoch : 340/2000, Train Loss : 8.036450, Valid Loss 12.808873, Error 0.022151\n",
      "Epoch : 341/2000, Train Loss : 8.605638, Valid Loss 10.099820, Error 0.018466\n",
      "Epoch : 342/2000, Train Loss : 10.033829, Valid Loss 16.592263, Error 0.026626\n",
      "Epoch : 343/2000, Train Loss : 13.678123, Valid Loss 10.950975, Error 0.019252\n",
      "Epoch : 344/2000, Train Loss : 8.689092, Valid Loss 10.149241, Error 0.018483\n",
      "Epoch : 345/2000, Train Loss : 10.644974, Valid Loss 9.951250, Error 0.018854\n",
      "Epoch : 346/2000, Train Loss : 10.076294, Valid Loss 11.442968, Error 0.021363\n",
      "Epoch : 347/2000, Train Loss : 8.534083, Valid Loss 10.546799, Error 0.018638\n",
      "Epoch : 348/2000, Train Loss : 8.040218, Valid Loss 10.212211, Error 0.018161\n",
      "Epoch : 349/2000, Train Loss : 8.626780, Valid Loss 11.808467, Error 0.020140\n",
      "Epoch : 350/2000, Train Loss : 8.000742, Valid Loss 10.002958, Error 0.019320\n",
      "Epoch : 351/2000, Train Loss : 7.813727, Valid Loss 9.718549, Error 0.017581\n",
      "Epoch : 352/2000, Train Loss : 7.911530, Valid Loss 10.537775, Error 0.019279\n",
      "Epoch : 353/2000, Train Loss : 9.088061, Valid Loss 12.385806, Error 0.019746\n",
      "Epoch : 354/2000, Train Loss : 8.742644, Valid Loss 9.745191, Error 0.017967\n",
      "Epoch : 355/2000, Train Loss : 8.820180, Valid Loss 10.492137, Error 0.018071\n",
      "Epoch : 356/2000, Train Loss : 8.304753, Valid Loss 10.014377, Error 0.017992\n",
      "Epoch : 357/2000, Train Loss : 8.368415, Valid Loss 10.028170, Error 0.018657\n",
      "Epoch : 358/2000, Train Loss : 9.266221, Valid Loss 9.757888, Error 0.018014\n",
      "Epoch : 359/2000, Train Loss : 8.076160, Valid Loss 10.122061, Error 0.017801\n",
      "Epoch : 360/2000, Train Loss : 7.899087, Valid Loss 11.058901, Error 0.020447\n",
      "Early stopping\n",
      "Total Error Mean 0.042363\n"
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "total_error = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model_atn.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model_atn.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    error = torch.sum(torch.abs(outputs - targets) / targets) / len(targets)\n",
    "    total_error += error\n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, Error {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      error))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Total Error Mean {:4f}'.format(total_error/(epoch+1)))\n",
    "        break\n",
    "\n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7621716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85211f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86606fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cnn_acc = nn.Conv2d(in_channels=3, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait",
   "language": "python",
   "name": "gait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
