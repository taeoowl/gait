{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dc5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error # mse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from dataloader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f5a4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        _, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        _, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "    \n",
    "        inputs_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        out_lstm = self.reg_module1(inputs_concat)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83facf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "dataset = Gait_Dataset_Salted(file_path)\n",
    "val_percent = 0.2\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e009bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8209fc42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17588\\3086851112.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LSTM' is not defined"
     ]
    }
   ],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bbbae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 17870.113444, Valid Loss 17772.395182, Error 1.001722\n",
      "Epoch : 2/2000, Train Loss : 17801.854655, Valid Loss 17598.065104, Error 0.996687\n",
      "Epoch : 3/2000, Train Loss : 17427.435140, Valid Loss 16929.443848, Error 0.977135\n",
      "Epoch : 4/2000, Train Loss : 16222.534668, Valid Loss 14921.828613, Error 0.915941\n",
      "Epoch : 5/2000, Train Loss : 13039.101603, Valid Loss 10320.661133, Error 0.757391\n",
      "Epoch : 6/2000, Train Loss : 7228.896627, Valid Loss 3718.507487, Error 0.440337\n",
      "Epoch : 7/2000, Train Loss : 1632.990761, Valid Loss 336.568532, Error 0.113193\n",
      "Epoch : 8/2000, Train Loss : 322.377593, Valid Loss 317.694407, Error 0.115807\n",
      "Epoch : 9/2000, Train Loss : 310.540378, Valid Loss 297.115957, Error 0.111464\n",
      "Epoch : 10/2000, Train Loss : 308.271188, Valid Loss 296.308207, Error 0.112335\n",
      "Epoch : 11/2000, Train Loss : 306.046806, Valid Loss 296.102046, Error 0.112159\n",
      "Epoch : 12/2000, Train Loss : 307.019051, Valid Loss 296.019750, Error 0.111901\n",
      "Epoch : 13/2000, Train Loss : 307.211285, Valid Loss 296.182627, Error 0.112246\n",
      "Epoch : 14/2000, Train Loss : 307.051214, Valid Loss 296.032445, Error 0.112066\n",
      "Epoch : 15/2000, Train Loss : 306.373758, Valid Loss 296.123734, Error 0.112198\n",
      "Epoch : 16/2000, Train Loss : 307.367266, Valid Loss 296.391368, Error 0.112397\n",
      "Epoch : 17/2000, Train Loss : 306.514753, Valid Loss 296.060109, Error 0.112144\n",
      "Epoch : 18/2000, Train Loss : 305.105707, Valid Loss 296.074432, Error 0.112179\n",
      "Epoch : 19/2000, Train Loss : 306.616820, Valid Loss 295.613764, Error 0.112094\n",
      "Epoch : 20/2000, Train Loss : 304.223783, Valid Loss 288.294500, Error 0.110833\n",
      "Epoch : 21/2000, Train Loss : 292.977036, Valid Loss 276.550451, Error 0.109460\n",
      "Epoch : 22/2000, Train Loss : 279.885853, Valid Loss 267.036652, Error 0.107596\n",
      "Epoch : 23/2000, Train Loss : 266.855641, Valid Loss 246.708580, Error 0.104299\n",
      "Epoch : 24/2000, Train Loss : 244.734183, Valid Loss 222.161580, Error 0.098734\n",
      "Epoch : 25/2000, Train Loss : 220.637618, Valid Loss 204.150747, Error 0.093638\n",
      "Epoch : 26/2000, Train Loss : 199.226404, Valid Loss 182.519986, Error 0.087087\n",
      "Epoch : 27/2000, Train Loss : 181.787449, Valid Loss 165.146210, Error 0.082108\n",
      "Epoch : 28/2000, Train Loss : 159.491703, Valid Loss 147.221659, Error 0.076404\n",
      "Epoch : 29/2000, Train Loss : 139.433276, Valid Loss 123.459948, Error 0.068262\n",
      "Epoch : 30/2000, Train Loss : 119.228305, Valid Loss 106.217635, Error 0.061434\n",
      "Epoch : 31/2000, Train Loss : 101.413337, Valid Loss 91.455363, Error 0.056352\n",
      "Epoch : 32/2000, Train Loss : 91.791391, Valid Loss 82.615421, Error 0.053758\n",
      "Epoch : 33/2000, Train Loss : 82.693172, Valid Loss 82.519131, Error 0.054514\n",
      "Epoch : 34/2000, Train Loss : 76.581317, Valid Loss 69.843495, Error 0.050489\n",
      "Epoch : 35/2000, Train Loss : 70.174619, Valid Loss 64.659991, Error 0.049046\n",
      "Epoch : 36/2000, Train Loss : 64.553358, Valid Loss 59.783069, Error 0.047903\n",
      "Epoch : 37/2000, Train Loss : 59.841999, Valid Loss 61.540233, Error 0.047990\n",
      "Epoch : 38/2000, Train Loss : 55.888369, Valid Loss 54.064701, Error 0.044795\n",
      "Epoch : 39/2000, Train Loss : 54.218304, Valid Loss 55.708757, Error 0.045600\n",
      "Epoch : 40/2000, Train Loss : 55.782700, Valid Loss 49.813336, Error 0.044698\n",
      "Epoch : 41/2000, Train Loss : 50.832179, Valid Loss 46.327291, Error 0.042163\n",
      "Epoch : 42/2000, Train Loss : 46.518066, Valid Loss 44.789768, Error 0.041752\n",
      "Epoch : 43/2000, Train Loss : 44.995367, Valid Loss 42.351905, Error 0.039764\n",
      "Epoch : 44/2000, Train Loss : 42.208953, Valid Loss 40.619640, Error 0.039481\n",
      "Epoch : 45/2000, Train Loss : 41.356568, Valid Loss 40.198836, Error 0.039721\n",
      "Epoch : 46/2000, Train Loss : 40.574493, Valid Loss 39.139463, Error 0.038518\n",
      "Epoch : 47/2000, Train Loss : 41.918916, Valid Loss 37.710550, Error 0.037518\n",
      "Epoch : 48/2000, Train Loss : 39.462344, Valid Loss 37.228296, Error 0.038146\n",
      "Epoch : 49/2000, Train Loss : 36.962077, Valid Loss 37.262345, Error 0.038365\n",
      "Epoch : 50/2000, Train Loss : 36.842131, Valid Loss 36.443393, Error 0.037265\n",
      "Epoch : 51/2000, Train Loss : 35.046398, Valid Loss 34.197221, Error 0.035806\n",
      "Epoch : 52/2000, Train Loss : 34.540301, Valid Loss 36.775284, Error 0.038562\n",
      "Epoch : 53/2000, Train Loss : 35.014130, Valid Loss 33.790317, Error 0.036128\n",
      "Epoch : 54/2000, Train Loss : 32.891026, Valid Loss 33.318908, Error 0.034304\n",
      "Epoch : 55/2000, Train Loss : 32.548941, Valid Loss 32.305901, Error 0.034519\n",
      "Epoch : 56/2000, Train Loss : 31.933401, Valid Loss 31.080032, Error 0.033112\n",
      "Epoch : 57/2000, Train Loss : 31.374649, Valid Loss 32.166110, Error 0.034594\n",
      "Epoch : 58/2000, Train Loss : 31.035259, Valid Loss 30.232376, Error 0.032514\n",
      "Epoch : 59/2000, Train Loss : 30.354251, Valid Loss 30.889941, Error 0.034877\n",
      "Epoch : 60/2000, Train Loss : 30.618823, Valid Loss 28.894769, Error 0.032279\n",
      "Epoch : 61/2000, Train Loss : 31.821558, Valid Loss 37.986711, Error 0.036897\n",
      "Epoch : 62/2000, Train Loss : 30.777616, Valid Loss 28.512086, Error 0.032522\n",
      "Epoch : 63/2000, Train Loss : 29.046611, Valid Loss 28.788035, Error 0.032371\n",
      "Epoch : 64/2000, Train Loss : 27.551043, Valid Loss 27.484636, Error 0.031252\n",
      "Epoch : 65/2000, Train Loss : 27.022683, Valid Loss 27.057357, Error 0.030953\n",
      "Epoch : 66/2000, Train Loss : 28.477454, Valid Loss 27.965092, Error 0.031967\n",
      "Epoch : 67/2000, Train Loss : 28.023622, Valid Loss 28.148175, Error 0.031681\n",
      "Epoch : 68/2000, Train Loss : 28.243241, Valid Loss 26.668800, Error 0.030876\n",
      "Epoch : 69/2000, Train Loss : 26.792584, Valid Loss 26.346757, Error 0.030033\n",
      "Epoch : 70/2000, Train Loss : 25.625847, Valid Loss 25.149398, Error 0.030131\n",
      "Epoch : 71/2000, Train Loss : 25.467700, Valid Loss 25.688148, Error 0.030436\n",
      "Epoch : 72/2000, Train Loss : 25.370731, Valid Loss 27.267218, Error 0.030564\n",
      "Epoch : 73/2000, Train Loss : 24.824255, Valid Loss 24.431897, Error 0.029062\n",
      "Epoch : 74/2000, Train Loss : 25.569608, Valid Loss 24.099472, Error 0.029206\n",
      "Epoch : 75/2000, Train Loss : 24.228161, Valid Loss 24.319636, Error 0.029829\n",
      "Epoch : 76/2000, Train Loss : 24.365193, Valid Loss 23.147468, Error 0.028674\n",
      "Epoch : 77/2000, Train Loss : 23.745494, Valid Loss 25.152914, Error 0.029685\n",
      "Epoch : 78/2000, Train Loss : 23.890444, Valid Loss 26.184047, Error 0.030464\n",
      "Epoch : 79/2000, Train Loss : 26.716640, Valid Loss 23.209085, Error 0.028442\n",
      "Epoch : 80/2000, Train Loss : 23.893987, Valid Loss 23.141389, Error 0.028147\n",
      "Epoch : 81/2000, Train Loss : 23.528974, Valid Loss 22.308314, Error 0.028371\n",
      "Epoch : 82/2000, Train Loss : 23.315425, Valid Loss 23.597093, Error 0.028242\n",
      "Epoch : 83/2000, Train Loss : 22.808178, Valid Loss 24.758707, Error 0.029274\n",
      "Epoch : 84/2000, Train Loss : 22.555619, Valid Loss 24.361837, Error 0.030397\n",
      "Epoch : 85/2000, Train Loss : 21.848606, Valid Loss 22.283141, Error 0.027903\n",
      "Epoch : 86/2000, Train Loss : 22.905386, Valid Loss 21.139114, Error 0.026937\n",
      "Epoch : 87/2000, Train Loss : 21.614277, Valid Loss 23.486314, Error 0.027952\n",
      "Epoch : 88/2000, Train Loss : 24.010807, Valid Loss 26.654972, Error 0.029837\n",
      "Epoch : 89/2000, Train Loss : 21.033460, Valid Loss 21.223691, Error 0.026997\n",
      "Epoch : 90/2000, Train Loss : 22.396445, Valid Loss 21.303139, Error 0.028055\n",
      "Epoch : 91/2000, Train Loss : 20.648347, Valid Loss 20.084190, Error 0.026237\n",
      "Epoch : 92/2000, Train Loss : 20.824407, Valid Loss 19.876004, Error 0.026138\n",
      "Epoch : 93/2000, Train Loss : 21.706274, Valid Loss 20.309065, Error 0.026016\n",
      "Epoch : 94/2000, Train Loss : 19.786420, Valid Loss 19.865726, Error 0.026088\n",
      "Epoch : 95/2000, Train Loss : 20.091626, Valid Loss 21.137472, Error 0.027878\n",
      "Epoch : 96/2000, Train Loss : 21.259157, Valid Loss 19.880250, Error 0.026798\n",
      "Epoch : 97/2000, Train Loss : 19.897125, Valid Loss 19.350260, Error 0.026266\n",
      "Epoch : 98/2000, Train Loss : 19.425289, Valid Loss 20.292323, Error 0.025993\n",
      "Epoch : 99/2000, Train Loss : 19.382183, Valid Loss 20.814395, Error 0.027543\n",
      "Epoch : 100/2000, Train Loss : 19.397079, Valid Loss 19.271458, Error 0.025449\n",
      "Epoch : 101/2000, Train Loss : 19.725873, Valid Loss 18.601020, Error 0.024917\n",
      "Epoch : 102/2000, Train Loss : 18.708749, Valid Loss 20.194560, Error 0.027169\n",
      "Epoch : 103/2000, Train Loss : 19.413873, Valid Loss 22.046868, Error 0.027626\n",
      "Epoch : 104/2000, Train Loss : 19.434858, Valid Loss 18.376674, Error 0.025066\n",
      "Epoch : 105/2000, Train Loss : 19.049062, Valid Loss 18.099699, Error 0.024652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 106/2000, Train Loss : 18.549611, Valid Loss 18.009409, Error 0.024962\n",
      "Epoch : 107/2000, Train Loss : 19.989504, Valid Loss 18.274359, Error 0.025668\n",
      "Epoch : 108/2000, Train Loss : 18.380557, Valid Loss 18.348245, Error 0.025792\n",
      "Epoch : 109/2000, Train Loss : 18.119791, Valid Loss 17.825162, Error 0.024638\n",
      "Epoch : 110/2000, Train Loss : 18.384524, Valid Loss 18.475912, Error 0.024201\n",
      "Epoch : 111/2000, Train Loss : 18.588512, Valid Loss 19.550170, Error 0.026404\n",
      "Epoch : 112/2000, Train Loss : 17.599518, Valid Loss 17.358676, Error 0.024347\n",
      "Epoch : 113/2000, Train Loss : 17.287250, Valid Loss 17.135252, Error 0.023959\n",
      "Epoch : 114/2000, Train Loss : 17.792346, Valid Loss 18.559091, Error 0.024726\n",
      "Epoch : 115/2000, Train Loss : 19.340130, Valid Loss 17.930935, Error 0.024958\n",
      "Epoch : 116/2000, Train Loss : 20.691455, Valid Loss 17.482421, Error 0.023789\n",
      "Epoch : 117/2000, Train Loss : 17.295385, Valid Loss 16.944252, Error 0.023801\n",
      "Epoch : 118/2000, Train Loss : 17.175164, Valid Loss 17.609751, Error 0.023514\n",
      "Epoch : 119/2000, Train Loss : 17.527461, Valid Loss 16.777736, Error 0.024124\n",
      "Epoch : 120/2000, Train Loss : 17.094572, Valid Loss 17.592388, Error 0.024714\n",
      "Epoch : 121/2000, Train Loss : 17.491397, Valid Loss 19.368417, Error 0.024763\n",
      "Epoch : 122/2000, Train Loss : 17.194750, Valid Loss 16.583482, Error 0.024007\n",
      "Epoch : 123/2000, Train Loss : 18.626215, Valid Loss 20.175424, Error 0.027611\n",
      "Epoch : 124/2000, Train Loss : 16.620053, Valid Loss 18.253553, Error 0.025151\n",
      "Epoch : 125/2000, Train Loss : 18.595212, Valid Loss 16.701970, Error 0.023753\n",
      "Epoch : 126/2000, Train Loss : 18.233684, Valid Loss 16.166521, Error 0.023608\n",
      "Epoch : 127/2000, Train Loss : 17.501792, Valid Loss 18.569456, Error 0.026369\n",
      "Epoch : 128/2000, Train Loss : 16.301795, Valid Loss 16.013261, Error 0.023292\n",
      "Epoch : 129/2000, Train Loss : 16.203360, Valid Loss 16.155504, Error 0.023591\n",
      "Epoch : 130/2000, Train Loss : 16.850857, Valid Loss 18.415552, Error 0.024656\n",
      "Epoch : 131/2000, Train Loss : 17.799864, Valid Loss 17.880190, Error 0.025006\n",
      "Epoch : 132/2000, Train Loss : 16.258465, Valid Loss 16.894428, Error 0.024835\n",
      "Epoch : 133/2000, Train Loss : 16.191840, Valid Loss 15.918014, Error 0.023798\n",
      "Epoch : 134/2000, Train Loss : 15.763004, Valid Loss 19.381450, Error 0.026238\n",
      "Epoch : 135/2000, Train Loss : 15.924247, Valid Loss 15.187871, Error 0.022490\n",
      "Epoch : 136/2000, Train Loss : 16.742187, Valid Loss 15.325948, Error 0.022619\n",
      "Epoch : 137/2000, Train Loss : 15.450073, Valid Loss 15.668260, Error 0.022567\n",
      "Epoch : 138/2000, Train Loss : 15.745723, Valid Loss 17.487969, Error 0.024406\n",
      "Epoch : 139/2000, Train Loss : 16.212450, Valid Loss 19.290007, Error 0.026044\n",
      "Epoch : 140/2000, Train Loss : 16.899662, Valid Loss 16.853732, Error 0.023456\n",
      "Epoch : 141/2000, Train Loss : 16.393916, Valid Loss 18.005944, Error 0.025396\n",
      "Epoch : 142/2000, Train Loss : 17.063994, Valid Loss 16.213211, Error 0.023879\n",
      "Epoch : 143/2000, Train Loss : 15.419674, Valid Loss 14.800864, Error 0.022609\n",
      "Epoch : 144/2000, Train Loss : 15.660705, Valid Loss 16.687468, Error 0.023924\n",
      "Epoch : 145/2000, Train Loss : 14.965190, Valid Loss 14.889546, Error 0.022499\n",
      "Epoch : 146/2000, Train Loss : 15.511976, Valid Loss 14.959035, Error 0.022739\n",
      "Epoch : 147/2000, Train Loss : 14.990044, Valid Loss 15.718786, Error 0.023233\n",
      "Epoch : 148/2000, Train Loss : 15.048915, Valid Loss 15.817243, Error 0.023362\n",
      "Epoch : 149/2000, Train Loss : 16.355770, Valid Loss 14.886301, Error 0.021995\n",
      "Epoch : 150/2000, Train Loss : 15.059162, Valid Loss 14.437384, Error 0.021859\n",
      "Epoch : 151/2000, Train Loss : 14.772558, Valid Loss 14.521892, Error 0.021818\n",
      "Epoch : 152/2000, Train Loss : 14.318888, Valid Loss 14.639611, Error 0.022085\n",
      "Epoch : 153/2000, Train Loss : 14.838552, Valid Loss 14.587701, Error 0.021560\n",
      "Epoch : 154/2000, Train Loss : 15.131505, Valid Loss 14.417521, Error 0.022238\n",
      "Epoch : 155/2000, Train Loss : 14.620370, Valid Loss 17.694634, Error 0.025760\n",
      "Epoch : 156/2000, Train Loss : 15.026762, Valid Loss 14.354327, Error 0.022494\n",
      "Epoch : 157/2000, Train Loss : 14.605969, Valid Loss 14.402359, Error 0.022125\n",
      "Epoch : 158/2000, Train Loss : 14.035533, Valid Loss 14.225338, Error 0.022571\n",
      "Epoch : 159/2000, Train Loss : 13.732298, Valid Loss 14.586939, Error 0.022774\n",
      "Epoch : 160/2000, Train Loss : 14.619085, Valid Loss 14.049008, Error 0.021425\n",
      "Epoch : 161/2000, Train Loss : 14.755141, Valid Loss 14.191595, Error 0.022326\n",
      "Epoch : 162/2000, Train Loss : 14.251301, Valid Loss 16.243943, Error 0.023805\n",
      "Epoch : 163/2000, Train Loss : 13.746455, Valid Loss 13.684872, Error 0.021116\n",
      "Epoch : 164/2000, Train Loss : 14.691122, Valid Loss 15.627453, Error 0.022608\n",
      "Epoch : 165/2000, Train Loss : 16.773684, Valid Loss 14.128149, Error 0.022104\n",
      "Epoch : 166/2000, Train Loss : 16.160528, Valid Loss 19.419927, Error 0.026302\n",
      "Epoch : 167/2000, Train Loss : 15.342772, Valid Loss 14.295779, Error 0.022118\n",
      "Epoch : 168/2000, Train Loss : 14.489014, Valid Loss 14.029668, Error 0.022434\n",
      "Epoch : 169/2000, Train Loss : 15.202599, Valid Loss 17.023758, Error 0.024851\n",
      "Epoch : 170/2000, Train Loss : 14.189427, Valid Loss 13.527417, Error 0.021100\n",
      "Epoch : 171/2000, Train Loss : 13.749506, Valid Loss 13.399983, Error 0.021334\n",
      "Epoch : 172/2000, Train Loss : 13.491434, Valid Loss 14.072507, Error 0.022682\n",
      "Epoch : 173/2000, Train Loss : 13.617196, Valid Loss 13.611625, Error 0.020758\n",
      "Epoch : 174/2000, Train Loss : 13.454612, Valid Loss 13.635657, Error 0.021165\n",
      "Epoch : 175/2000, Train Loss : 13.140174, Valid Loss 13.731126, Error 0.021382\n",
      "Epoch : 176/2000, Train Loss : 13.918585, Valid Loss 16.057231, Error 0.023709\n",
      "Epoch : 177/2000, Train Loss : 14.753595, Valid Loss 13.892731, Error 0.022153\n",
      "Epoch : 178/2000, Train Loss : 13.231132, Valid Loss 14.349329, Error 0.021556\n",
      "Epoch : 179/2000, Train Loss : 14.969357, Valid Loss 18.681156, Error 0.026091\n",
      "Epoch : 180/2000, Train Loss : 14.125279, Valid Loss 13.389634, Error 0.021609\n",
      "Epoch : 181/2000, Train Loss : 13.784166, Valid Loss 13.316201, Error 0.021323\n",
      "Epoch : 182/2000, Train Loss : 13.838502, Valid Loss 12.961735, Error 0.020923\n",
      "Epoch : 183/2000, Train Loss : 13.657416, Valid Loss 13.438088, Error 0.021129\n",
      "Epoch : 184/2000, Train Loss : 13.145249, Valid Loss 15.416568, Error 0.023360\n",
      "Epoch : 185/2000, Train Loss : 14.752987, Valid Loss 13.230135, Error 0.020840\n",
      "Epoch : 186/2000, Train Loss : 14.454666, Valid Loss 14.393069, Error 0.022153\n",
      "Epoch : 187/2000, Train Loss : 12.861203, Valid Loss 13.206121, Error 0.021306\n",
      "Epoch : 188/2000, Train Loss : 12.589635, Valid Loss 13.642279, Error 0.021524\n",
      "Epoch : 189/2000, Train Loss : 13.008769, Valid Loss 12.827392, Error 0.020961\n",
      "Epoch : 190/2000, Train Loss : 13.789496, Valid Loss 13.384228, Error 0.021287\n",
      "Epoch : 191/2000, Train Loss : 12.620945, Valid Loss 13.305088, Error 0.020943\n",
      "Epoch : 192/2000, Train Loss : 12.390730, Valid Loss 12.823906, Error 0.020775\n",
      "Epoch : 193/2000, Train Loss : 12.359708, Valid Loss 13.442373, Error 0.021189\n",
      "Epoch : 194/2000, Train Loss : 12.352440, Valid Loss 12.618485, Error 0.020726\n",
      "Epoch : 195/2000, Train Loss : 12.188111, Valid Loss 12.709615, Error 0.020921\n",
      "Epoch : 196/2000, Train Loss : 12.235792, Valid Loss 15.642017, Error 0.023145\n",
      "Epoch : 197/2000, Train Loss : 13.431865, Valid Loss 14.803077, Error 0.022637\n",
      "Epoch : 198/2000, Train Loss : 13.322354, Valid Loss 14.186012, Error 0.022235\n",
      "Epoch : 199/2000, Train Loss : 12.512089, Valid Loss 14.008687, Error 0.022013\n",
      "Epoch : 200/2000, Train Loss : 11.957248, Valid Loss 13.046241, Error 0.021203\n",
      "Epoch : 201/2000, Train Loss : 12.058246, Valid Loss 13.254208, Error 0.021064\n",
      "Epoch : 202/2000, Train Loss : 14.165441, Valid Loss 15.620042, Error 0.022784\n",
      "Epoch : 203/2000, Train Loss : 15.682101, Valid Loss 14.218471, Error 0.022523\n",
      "Epoch : 204/2000, Train Loss : 14.791041, Valid Loss 24.394943, Error 0.030493\n",
      "Epoch : 205/2000, Train Loss : 14.772471, Valid Loss 12.743303, Error 0.021591\n",
      "Epoch : 206/2000, Train Loss : 12.307296, Valid Loss 12.906991, Error 0.021067\n",
      "Epoch : 207/2000, Train Loss : 12.137103, Valid Loss 13.670879, Error 0.022104\n",
      "Epoch : 208/2000, Train Loss : 12.041692, Valid Loss 12.860842, Error 0.021388\n",
      "Epoch : 209/2000, Train Loss : 12.519867, Valid Loss 13.337946, Error 0.021019\n",
      "Epoch : 210/2000, Train Loss : 12.068390, Valid Loss 12.264473, Error 0.019892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 211/2000, Train Loss : 11.937827, Valid Loss 12.047107, Error 0.019278\n",
      "Epoch : 212/2000, Train Loss : 11.794584, Valid Loss 12.795289, Error 0.020453\n",
      "Epoch : 213/2000, Train Loss : 12.200970, Valid Loss 13.878427, Error 0.021646\n",
      "Epoch : 214/2000, Train Loss : 11.516532, Valid Loss 11.854633, Error 0.020106\n",
      "Epoch : 215/2000, Train Loss : 12.023094, Valid Loss 12.622129, Error 0.021103\n",
      "Epoch : 216/2000, Train Loss : 12.156019, Valid Loss 12.346034, Error 0.019806\n",
      "Epoch : 217/2000, Train Loss : 11.715419, Valid Loss 12.000402, Error 0.019957\n",
      "Epoch : 218/2000, Train Loss : 11.208218, Valid Loss 13.216921, Error 0.021351\n",
      "Epoch : 219/2000, Train Loss : 12.285202, Valid Loss 18.388893, Error 0.025400\n",
      "Epoch : 220/2000, Train Loss : 12.146947, Valid Loss 12.380015, Error 0.020348\n",
      "Epoch : 221/2000, Train Loss : 11.231564, Valid Loss 12.949394, Error 0.021203\n",
      "Epoch : 222/2000, Train Loss : 12.098408, Valid Loss 14.154082, Error 0.022949\n",
      "Epoch : 223/2000, Train Loss : 12.741477, Valid Loss 13.253424, Error 0.021421\n",
      "Epoch : 224/2000, Train Loss : 11.383644, Valid Loss 12.052325, Error 0.020668\n",
      "Epoch : 225/2000, Train Loss : 11.369540, Valid Loss 12.225510, Error 0.020469\n",
      "Epoch : 226/2000, Train Loss : 11.308193, Valid Loss 12.306078, Error 0.020862\n",
      "Epoch : 227/2000, Train Loss : 11.483568, Valid Loss 11.650169, Error 0.019936\n",
      "Epoch : 228/2000, Train Loss : 11.122568, Valid Loss 11.771729, Error 0.020295\n",
      "Epoch : 229/2000, Train Loss : 13.357107, Valid Loss 14.985849, Error 0.023193\n",
      "Epoch : 230/2000, Train Loss : 13.809222, Valid Loss 12.166556, Error 0.020960\n",
      "Epoch : 231/2000, Train Loss : 12.731815, Valid Loss 14.029953, Error 0.022280\n",
      "Epoch : 232/2000, Train Loss : 12.291966, Valid Loss 11.885165, Error 0.020135\n",
      "Epoch : 233/2000, Train Loss : 10.775060, Valid Loss 11.802667, Error 0.020570\n",
      "Epoch : 234/2000, Train Loss : 11.309888, Valid Loss 12.128842, Error 0.020408\n",
      "Epoch : 235/2000, Train Loss : 10.578127, Valid Loss 11.800238, Error 0.019962\n",
      "Epoch : 236/2000, Train Loss : 11.201755, Valid Loss 12.328673, Error 0.019515\n",
      "Epoch : 237/2000, Train Loss : 12.202708, Valid Loss 12.243273, Error 0.020925\n",
      "Epoch : 238/2000, Train Loss : 11.274074, Valid Loss 13.644966, Error 0.022013\n",
      "Epoch : 239/2000, Train Loss : 10.847765, Valid Loss 11.794902, Error 0.019740\n",
      "Epoch : 240/2000, Train Loss : 10.927975, Valid Loss 11.743571, Error 0.020411\n",
      "Epoch : 241/2000, Train Loss : 11.116271, Valid Loss 14.032716, Error 0.022224\n",
      "Epoch : 242/2000, Train Loss : 13.481003, Valid Loss 11.761282, Error 0.020731\n",
      "Epoch : 243/2000, Train Loss : 11.352870, Valid Loss 12.309767, Error 0.020095\n",
      "Epoch : 244/2000, Train Loss : 11.306459, Valid Loss 12.589097, Error 0.020992\n",
      "Epoch : 245/2000, Train Loss : 11.925106, Valid Loss 12.347591, Error 0.021577\n",
      "Epoch : 246/2000, Train Loss : 11.842817, Valid Loss 13.048385, Error 0.021183\n",
      "Epoch : 247/2000, Train Loss : 10.997174, Valid Loss 14.772344, Error 0.022828\n",
      "Epoch : 248/2000, Train Loss : 11.177506, Valid Loss 17.261803, Error 0.025671\n",
      "Early stopping\n",
      "Total Error Mean 0.053626\n"
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "total_error = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    error = torch.sum(torch.abs(outputs - targets) / targets) / len(targets)\n",
    "    total_error += error\n",
    "    \n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, Error {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      error))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Total Error Mean {:4f}'.format(total_error/(epoch+1)))\n",
    "        break\n",
    "    \n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01162acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ec1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a04e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b344b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce56a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81be170",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM_atn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc975cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model_atn = LSTM_atn(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_atn.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9767a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 17746.155599, Valid Loss 17663.092773, Error 0.996257\n",
      "Epoch : 2/2000, Train Loss : 17380.334147, Valid Loss 16920.583984, Error 0.974798\n",
      "Epoch : 3/2000, Train Loss : 15908.585612, Valid Loss 14371.164388, Error 0.897196\n",
      "Epoch : 4/2000, Train Loss : 11838.384481, Valid Loss 8483.621582, Error 0.684904\n",
      "Epoch : 5/2000, Train Loss : 4909.334173, Valid Loss 1584.802490, Error 0.271992\n",
      "Epoch : 6/2000, Train Loss : 565.266740, Valid Loss 394.996328, Error 0.128506\n",
      "Epoch : 7/2000, Train Loss : 329.174536, Valid Loss 340.072881, Error 0.123538\n",
      "Epoch : 8/2000, Train Loss : 296.477468, Valid Loss 340.002772, Error 0.123510\n",
      "Epoch : 9/2000, Train Loss : 295.836207, Valid Loss 339.882629, Error 0.123446\n",
      "Epoch : 10/2000, Train Loss : 295.486719, Valid Loss 339.912120, Error 0.123472\n",
      "Epoch : 11/2000, Train Loss : 295.184500, Valid Loss 339.863561, Error 0.123433\n",
      "Epoch : 12/2000, Train Loss : 296.545444, Valid Loss 340.076136, Error 0.123540\n",
      "Epoch : 13/2000, Train Loss : 294.946229, Valid Loss 339.854431, Error 0.123438\n",
      "Epoch : 14/2000, Train Loss : 294.862552, Valid Loss 340.019643, Error 0.123523\n",
      "Epoch : 15/2000, Train Loss : 294.877044, Valid Loss 339.817434, Error 0.123418\n",
      "Epoch : 16/2000, Train Loss : 294.848628, Valid Loss 339.783452, Error 0.123420\n",
      "Epoch : 17/2000, Train Loss : 295.017262, Valid Loss 339.360936, Error 0.123323\n",
      "Epoch : 18/2000, Train Loss : 292.719739, Valid Loss 333.350235, Error 0.122192\n",
      "Epoch : 19/2000, Train Loss : 284.976927, Valid Loss 319.274663, Error 0.120121\n",
      "Epoch : 20/2000, Train Loss : 269.810402, Valid Loss 311.082342, Error 0.116594\n",
      "Epoch : 21/2000, Train Loss : 253.366926, Valid Loss 274.417819, Error 0.109991\n",
      "Epoch : 22/2000, Train Loss : 226.034573, Valid Loss 253.345248, Error 0.108301\n",
      "Epoch : 23/2000, Train Loss : 205.381980, Valid Loss 209.580866, Error 0.096801\n",
      "Epoch : 24/2000, Train Loss : 178.651913, Valid Loss 181.918142, Error 0.089978\n",
      "Epoch : 25/2000, Train Loss : 160.750230, Valid Loss 161.475731, Error 0.083765\n",
      "Epoch : 26/2000, Train Loss : 140.399963, Valid Loss 135.445318, Error 0.077339\n",
      "Epoch : 27/2000, Train Loss : 120.676627, Valid Loss 114.604837, Error 0.071518\n",
      "Epoch : 28/2000, Train Loss : 119.167442, Valid Loss 105.989690, Error 0.066888\n",
      "Epoch : 29/2000, Train Loss : 105.127706, Valid Loss 95.019975, Error 0.062763\n",
      "Epoch : 30/2000, Train Loss : 89.500161, Valid Loss 86.874620, Error 0.060904\n",
      "Epoch : 31/2000, Train Loss : 82.375968, Valid Loss 79.259506, Error 0.053925\n",
      "Epoch : 32/2000, Train Loss : 73.736136, Valid Loss 63.797724, Error 0.048389\n",
      "Epoch : 33/2000, Train Loss : 65.693051, Valid Loss 58.884479, Error 0.044746\n",
      "Epoch : 34/2000, Train Loss : 60.965403, Valid Loss 51.474033, Error 0.042420\n",
      "Epoch : 35/2000, Train Loss : 59.008686, Valid Loss 53.671929, Error 0.041714\n",
      "Epoch : 36/2000, Train Loss : 51.879035, Valid Loss 45.732187, Error 0.040263\n",
      "Epoch : 37/2000, Train Loss : 49.906691, Valid Loss 55.248451, Error 0.046250\n",
      "Epoch : 38/2000, Train Loss : 48.228613, Valid Loss 39.790150, Error 0.036455\n",
      "Epoch : 39/2000, Train Loss : 45.640045, Valid Loss 39.472632, Error 0.034993\n",
      "Epoch : 40/2000, Train Loss : 42.263751, Valid Loss 36.572996, Error 0.035190\n",
      "Epoch : 41/2000, Train Loss : 40.745937, Valid Loss 34.994902, Error 0.034676\n",
      "Epoch : 42/2000, Train Loss : 39.428334, Valid Loss 37.691392, Error 0.038224\n",
      "Epoch : 43/2000, Train Loss : 37.774772, Valid Loss 42.641071, Error 0.036270\n",
      "Epoch : 44/2000, Train Loss : 36.862978, Valid Loss 31.629064, Error 0.032656\n",
      "Epoch : 45/2000, Train Loss : 34.214095, Valid Loss 31.460793, Error 0.032372\n",
      "Epoch : 46/2000, Train Loss : 33.584019, Valid Loss 31.170683, Error 0.033978\n",
      "Epoch : 47/2000, Train Loss : 33.105454, Valid Loss 31.436364, Error 0.032098\n",
      "Epoch : 48/2000, Train Loss : 30.862766, Valid Loss 29.917188, Error 0.033005\n",
      "Epoch : 49/2000, Train Loss : 32.046125, Valid Loss 28.229021, Error 0.032779\n",
      "Epoch : 50/2000, Train Loss : 32.380711, Valid Loss 28.622046, Error 0.031022\n",
      "Epoch : 51/2000, Train Loss : 29.036500, Valid Loss 28.944723, Error 0.030971\n",
      "Epoch : 52/2000, Train Loss : 28.529365, Valid Loss 27.612034, Error 0.030611\n",
      "Epoch : 53/2000, Train Loss : 28.666894, Valid Loss 28.620946, Error 0.030449\n",
      "Epoch : 54/2000, Train Loss : 32.510382, Valid Loss 28.506569, Error 0.032454\n",
      "Epoch : 55/2000, Train Loss : 27.756232, Valid Loss 26.653507, Error 0.031307\n",
      "Epoch : 56/2000, Train Loss : 27.737658, Valid Loss 25.484940, Error 0.031198\n",
      "Epoch : 57/2000, Train Loss : 27.286329, Valid Loss 24.305438, Error 0.029072\n",
      "Epoch : 58/2000, Train Loss : 26.070024, Valid Loss 28.215765, Error 0.030063\n",
      "Epoch : 59/2000, Train Loss : 25.892390, Valid Loss 31.437661, Error 0.034912\n",
      "Epoch : 60/2000, Train Loss : 27.172318, Valid Loss 23.698163, Error 0.028461\n",
      "Epoch : 61/2000, Train Loss : 26.226291, Valid Loss 24.768416, Error 0.029213\n",
      "Epoch : 62/2000, Train Loss : 27.148716, Valid Loss 23.361794, Error 0.028963\n",
      "Epoch : 63/2000, Train Loss : 23.484789, Valid Loss 25.665076, Error 0.028903\n",
      "Epoch : 64/2000, Train Loss : 23.392354, Valid Loss 25.235213, Error 0.028635\n",
      "Epoch : 65/2000, Train Loss : 23.829504, Valid Loss 22.651375, Error 0.027501\n",
      "Epoch : 66/2000, Train Loss : 23.208577, Valid Loss 24.322181, Error 0.028362\n",
      "Epoch : 67/2000, Train Loss : 22.806966, Valid Loss 22.480411, Error 0.027995\n",
      "Epoch : 68/2000, Train Loss : 23.713401, Valid Loss 23.209682, Error 0.027365\n",
      "Epoch : 69/2000, Train Loss : 22.668682, Valid Loss 22.988604, Error 0.028683\n",
      "Epoch : 70/2000, Train Loss : 22.305296, Valid Loss 21.357707, Error 0.027051\n",
      "Epoch : 71/2000, Train Loss : 21.897731, Valid Loss 22.396448, Error 0.028017\n",
      "Epoch : 72/2000, Train Loss : 22.281193, Valid Loss 22.136411, Error 0.027142\n",
      "Epoch : 73/2000, Train Loss : 22.507941, Valid Loss 20.887829, Error 0.026670\n",
      "Epoch : 74/2000, Train Loss : 21.432510, Valid Loss 22.759131, Error 0.027335\n",
      "Epoch : 75/2000, Train Loss : 21.533336, Valid Loss 20.304359, Error 0.025855\n",
      "Epoch : 76/2000, Train Loss : 20.357301, Valid Loss 21.342022, Error 0.026787\n",
      "Epoch : 77/2000, Train Loss : 19.961457, Valid Loss 21.228907, Error 0.026372\n",
      "Epoch : 78/2000, Train Loss : 21.283344, Valid Loss 23.162016, Error 0.027679\n",
      "Epoch : 79/2000, Train Loss : 20.539526, Valid Loss 27.181420, Error 0.029945\n",
      "Epoch : 80/2000, Train Loss : 20.463899, Valid Loss 22.218242, Error 0.026194\n",
      "Epoch : 81/2000, Train Loss : 19.964327, Valid Loss 19.642453, Error 0.026516\n",
      "Epoch : 82/2000, Train Loss : 19.101043, Valid Loss 20.954659, Error 0.025623\n",
      "Epoch : 83/2000, Train Loss : 18.918130, Valid Loss 20.222287, Error 0.025625\n",
      "Epoch : 84/2000, Train Loss : 21.299348, Valid Loss 19.516720, Error 0.025228\n",
      "Epoch : 85/2000, Train Loss : 18.834789, Valid Loss 21.568552, Error 0.026563\n",
      "Epoch : 86/2000, Train Loss : 19.838210, Valid Loss 18.958966, Error 0.025005\n",
      "Epoch : 87/2000, Train Loss : 18.742500, Valid Loss 19.063059, Error 0.025103\n",
      "Epoch : 88/2000, Train Loss : 18.977407, Valid Loss 19.354151, Error 0.025388\n",
      "Epoch : 89/2000, Train Loss : 18.999027, Valid Loss 21.581952, Error 0.026551\n",
      "Epoch : 90/2000, Train Loss : 20.260914, Valid Loss 19.604070, Error 0.025300\n",
      "Epoch : 91/2000, Train Loss : 18.133022, Valid Loss 21.156068, Error 0.026201\n",
      "Epoch : 92/2000, Train Loss : 18.410487, Valid Loss 19.750679, Error 0.025167\n",
      "Epoch : 93/2000, Train Loss : 18.545669, Valid Loss 20.263473, Error 0.026370\n",
      "Epoch : 94/2000, Train Loss : 18.859083, Valid Loss 19.521643, Error 0.026508\n",
      "Epoch : 95/2000, Train Loss : 17.738393, Valid Loss 18.728705, Error 0.024656\n",
      "Epoch : 96/2000, Train Loss : 17.532592, Valid Loss 17.471943, Error 0.024050\n",
      "Epoch : 97/2000, Train Loss : 17.971662, Valid Loss 17.911412, Error 0.023967\n",
      "Epoch : 98/2000, Train Loss : 16.833942, Valid Loss 18.258144, Error 0.024346\n",
      "Epoch : 99/2000, Train Loss : 16.856601, Valid Loss 18.100742, Error 0.023566\n",
      "Epoch : 100/2000, Train Loss : 16.854650, Valid Loss 22.984829, Error 0.028729\n",
      "Epoch : 101/2000, Train Loss : 18.258764, Valid Loss 17.689452, Error 0.023562\n",
      "Epoch : 102/2000, Train Loss : 16.599231, Valid Loss 23.436054, Error 0.028163\n",
      "Epoch : 103/2000, Train Loss : 16.860558, Valid Loss 18.469821, Error 0.023948\n",
      "Epoch : 104/2000, Train Loss : 16.306420, Valid Loss 21.383687, Error 0.026154\n",
      "Epoch : 105/2000, Train Loss : 18.215283, Valid Loss 17.920603, Error 0.023236\n",
      "Epoch : 106/2000, Train Loss : 17.268888, Valid Loss 21.424931, Error 0.026351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 107/2000, Train Loss : 16.216515, Valid Loss 23.194283, Error 0.027629\n",
      "Epoch : 108/2000, Train Loss : 17.692392, Valid Loss 17.909806, Error 0.024385\n",
      "Epoch : 109/2000, Train Loss : 16.120957, Valid Loss 16.687341, Error 0.022427\n",
      "Epoch : 110/2000, Train Loss : 15.845936, Valid Loss 17.254032, Error 0.023990\n",
      "Epoch : 111/2000, Train Loss : 15.293227, Valid Loss 16.690448, Error 0.023370\n",
      "Epoch : 112/2000, Train Loss : 15.404807, Valid Loss 17.810793, Error 0.024242\n",
      "Epoch : 113/2000, Train Loss : 16.718792, Valid Loss 16.615735, Error 0.022822\n",
      "Epoch : 114/2000, Train Loss : 15.151704, Valid Loss 16.642402, Error 0.022911\n",
      "Epoch : 115/2000, Train Loss : 16.668798, Valid Loss 17.154168, Error 0.024107\n",
      "Epoch : 116/2000, Train Loss : 15.669579, Valid Loss 16.409843, Error 0.022480\n",
      "Epoch : 117/2000, Train Loss : 15.110237, Valid Loss 19.216903, Error 0.024980\n",
      "Epoch : 118/2000, Train Loss : 15.322914, Valid Loss 16.188718, Error 0.023355\n",
      "Epoch : 119/2000, Train Loss : 15.127932, Valid Loss 17.253812, Error 0.023894\n",
      "Epoch : 120/2000, Train Loss : 15.331670, Valid Loss 18.026510, Error 0.025080\n",
      "Epoch : 121/2000, Train Loss : 15.616267, Valid Loss 16.919228, Error 0.023823\n",
      "Epoch : 122/2000, Train Loss : 14.751344, Valid Loss 17.061991, Error 0.023455\n",
      "Epoch : 123/2000, Train Loss : 15.135407, Valid Loss 16.279701, Error 0.023508\n",
      "Epoch : 124/2000, Train Loss : 15.757039, Valid Loss 16.695213, Error 0.022963\n",
      "Epoch : 125/2000, Train Loss : 14.565277, Valid Loss 15.993795, Error 0.022703\n",
      "Epoch : 126/2000, Train Loss : 14.279736, Valid Loss 17.890407, Error 0.024722\n",
      "Epoch : 127/2000, Train Loss : 15.457915, Valid Loss 16.785359, Error 0.023763\n",
      "Epoch : 128/2000, Train Loss : 14.554918, Valid Loss 15.739833, Error 0.022477\n",
      "Epoch : 129/2000, Train Loss : 16.620396, Valid Loss 17.652886, Error 0.024603\n",
      "Epoch : 130/2000, Train Loss : 17.119233, Valid Loss 15.596418, Error 0.022798\n",
      "Epoch : 131/2000, Train Loss : 15.777952, Valid Loss 16.483868, Error 0.023389\n",
      "Epoch : 132/2000, Train Loss : 15.635896, Valid Loss 17.681403, Error 0.025017\n",
      "Epoch : 133/2000, Train Loss : 13.888127, Valid Loss 15.922627, Error 0.023323\n",
      "Epoch : 134/2000, Train Loss : 13.737313, Valid Loss 16.675728, Error 0.023595\n",
      "Epoch : 135/2000, Train Loss : 14.001831, Valid Loss 16.759601, Error 0.024129\n",
      "Epoch : 136/2000, Train Loss : 14.223570, Valid Loss 15.432080, Error 0.023494\n",
      "Epoch : 137/2000, Train Loss : 13.950149, Valid Loss 18.798962, Error 0.025156\n",
      "Epoch : 138/2000, Train Loss : 14.657693, Valid Loss 17.557986, Error 0.024129\n",
      "Epoch : 139/2000, Train Loss : 15.137798, Valid Loss 16.031716, Error 0.023053\n",
      "Epoch : 140/2000, Train Loss : 13.787189, Valid Loss 16.812127, Error 0.024541\n",
      "Epoch : 141/2000, Train Loss : 14.854703, Valid Loss 15.680044, Error 0.023351\n",
      "Epoch : 142/2000, Train Loss : 14.143747, Valid Loss 16.216557, Error 0.022736\n",
      "Epoch : 143/2000, Train Loss : 15.813749, Valid Loss 15.206968, Error 0.022154\n",
      "Epoch : 144/2000, Train Loss : 13.532925, Valid Loss 14.979256, Error 0.022640\n",
      "Epoch : 145/2000, Train Loss : 13.323354, Valid Loss 14.827536, Error 0.021670\n",
      "Epoch : 146/2000, Train Loss : 13.216369, Valid Loss 15.216122, Error 0.022354\n",
      "Epoch : 147/2000, Train Loss : 13.425589, Valid Loss 17.440919, Error 0.023943\n",
      "Epoch : 148/2000, Train Loss : 15.645339, Valid Loss 15.635047, Error 0.022697\n",
      "Epoch : 149/2000, Train Loss : 14.609543, Valid Loss 15.139165, Error 0.022882\n",
      "Epoch : 150/2000, Train Loss : 13.031153, Valid Loss 15.704482, Error 0.022076\n",
      "Epoch : 151/2000, Train Loss : 13.329669, Valid Loss 14.491177, Error 0.021980\n",
      "Epoch : 152/2000, Train Loss : 13.242407, Valid Loss 14.783349, Error 0.022385\n",
      "Epoch : 153/2000, Train Loss : 12.920271, Valid Loss 16.659588, Error 0.023108\n",
      "Epoch : 154/2000, Train Loss : 14.019866, Valid Loss 15.929471, Error 0.023480\n",
      "Epoch : 155/2000, Train Loss : 13.193384, Valid Loss 18.100763, Error 0.025140\n",
      "Epoch : 156/2000, Train Loss : 14.302159, Valid Loss 19.860036, Error 0.026930\n",
      "Epoch : 157/2000, Train Loss : 14.048058, Valid Loss 14.866783, Error 0.023377\n",
      "Epoch : 158/2000, Train Loss : 13.898058, Valid Loss 17.010718, Error 0.023581\n",
      "Epoch : 159/2000, Train Loss : 13.362877, Valid Loss 14.228408, Error 0.022315\n",
      "Epoch : 160/2000, Train Loss : 12.647187, Valid Loss 16.403510, Error 0.023381\n",
      "Epoch : 161/2000, Train Loss : 13.289107, Valid Loss 15.687453, Error 0.022508\n",
      "Epoch : 162/2000, Train Loss : 12.474895, Valid Loss 19.347897, Error 0.026060\n",
      "Epoch : 163/2000, Train Loss : 13.603176, Valid Loss 15.228276, Error 0.022414\n",
      "Epoch : 164/2000, Train Loss : 12.725355, Valid Loss 14.805349, Error 0.022648\n",
      "Epoch : 165/2000, Train Loss : 12.538205, Valid Loss 15.356894, Error 0.023162\n",
      "Epoch : 166/2000, Train Loss : 12.124081, Valid Loss 14.533255, Error 0.021978\n",
      "Epoch : 167/2000, Train Loss : 12.614660, Valid Loss 15.173474, Error 0.022962\n",
      "Epoch : 168/2000, Train Loss : 12.317833, Valid Loss 14.586211, Error 0.022543\n",
      "Epoch : 169/2000, Train Loss : 12.313669, Valid Loss 16.221711, Error 0.024466\n",
      "Epoch : 170/2000, Train Loss : 12.866306, Valid Loss 15.223872, Error 0.021912\n",
      "Epoch : 171/2000, Train Loss : 13.273710, Valid Loss 14.857666, Error 0.022929\n",
      "Epoch : 172/2000, Train Loss : 12.706718, Valid Loss 14.232907, Error 0.022657\n",
      "Epoch : 173/2000, Train Loss : 12.522770, Valid Loss 14.312564, Error 0.021471\n",
      "Epoch : 174/2000, Train Loss : 11.790019, Valid Loss 14.297392, Error 0.021989\n",
      "Epoch : 175/2000, Train Loss : 11.957468, Valid Loss 14.112153, Error 0.022234\n",
      "Epoch : 176/2000, Train Loss : 13.726741, Valid Loss 14.011361, Error 0.021975\n",
      "Epoch : 177/2000, Train Loss : 11.836272, Valid Loss 14.686705, Error 0.023226\n",
      "Epoch : 178/2000, Train Loss : 11.896774, Valid Loss 15.319928, Error 0.022708\n",
      "Epoch : 179/2000, Train Loss : 11.789518, Valid Loss 14.571828, Error 0.022542\n",
      "Epoch : 180/2000, Train Loss : 12.682329, Valid Loss 14.646289, Error 0.022815\n",
      "Epoch : 181/2000, Train Loss : 12.495691, Valid Loss 14.995644, Error 0.022334\n",
      "Epoch : 182/2000, Train Loss : 12.409244, Valid Loss 14.593208, Error 0.022226\n",
      "Epoch : 183/2000, Train Loss : 12.300367, Valid Loss 13.733084, Error 0.021579\n",
      "Epoch : 184/2000, Train Loss : 11.779835, Valid Loss 13.495690, Error 0.021488\n",
      "Epoch : 185/2000, Train Loss : 13.137535, Valid Loss 13.678357, Error 0.021143\n",
      "Epoch : 186/2000, Train Loss : 12.075946, Valid Loss 16.273148, Error 0.023036\n",
      "Epoch : 187/2000, Train Loss : 12.732440, Valid Loss 15.311656, Error 0.022965\n",
      "Epoch : 188/2000, Train Loss : 12.263411, Valid Loss 13.605248, Error 0.021903\n",
      "Epoch : 189/2000, Train Loss : 11.538155, Valid Loss 13.554310, Error 0.020706\n",
      "Epoch : 190/2000, Train Loss : 12.400243, Valid Loss 14.061210, Error 0.021782\n",
      "Epoch : 191/2000, Train Loss : 12.454375, Valid Loss 13.606939, Error 0.021122\n",
      "Epoch : 192/2000, Train Loss : 13.401533, Valid Loss 13.501137, Error 0.021570\n",
      "Epoch : 193/2000, Train Loss : 12.320254, Valid Loss 14.756367, Error 0.022134\n",
      "Epoch : 194/2000, Train Loss : 11.556222, Valid Loss 14.244482, Error 0.021748\n",
      "Epoch : 195/2000, Train Loss : 11.333191, Valid Loss 14.244691, Error 0.022436\n",
      "Epoch : 196/2000, Train Loss : 11.255745, Valid Loss 13.717304, Error 0.021810\n",
      "Epoch : 197/2000, Train Loss : 11.225446, Valid Loss 13.847986, Error 0.021288\n",
      "Epoch : 198/2000, Train Loss : 11.529629, Valid Loss 15.154387, Error 0.022721\n",
      "Epoch : 199/2000, Train Loss : 11.716140, Valid Loss 15.441979, Error 0.023399\n",
      "Epoch : 200/2000, Train Loss : 11.235301, Valid Loss 13.134509, Error 0.021324\n",
      "Epoch : 201/2000, Train Loss : 12.085870, Valid Loss 13.035253, Error 0.020526\n",
      "Epoch : 202/2000, Train Loss : 11.334279, Valid Loss 13.438747, Error 0.021225\n",
      "Epoch : 203/2000, Train Loss : 11.599812, Valid Loss 12.943549, Error 0.021015\n",
      "Epoch : 204/2000, Train Loss : 10.895675, Valid Loss 13.310054, Error 0.021673\n",
      "Epoch : 205/2000, Train Loss : 10.955489, Valid Loss 13.336187, Error 0.021016\n",
      "Epoch : 206/2000, Train Loss : 11.145270, Valid Loss 13.016750, Error 0.020716\n",
      "Epoch : 207/2000, Train Loss : 11.343640, Valid Loss 15.628100, Error 0.023281\n",
      "Epoch : 208/2000, Train Loss : 12.280028, Valid Loss 15.049814, Error 0.023252\n",
      "Epoch : 209/2000, Train Loss : 10.688243, Valid Loss 13.578625, Error 0.021281\n",
      "Epoch : 210/2000, Train Loss : 10.896347, Valid Loss 13.251416, Error 0.021268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 211/2000, Train Loss : 11.757466, Valid Loss 19.493862, Error 0.025797\n",
      "Epoch : 212/2000, Train Loss : 11.808342, Valid Loss 15.093043, Error 0.022005\n",
      "Epoch : 213/2000, Train Loss : 10.690721, Valid Loss 13.177488, Error 0.021580\n",
      "Epoch : 214/2000, Train Loss : 10.857700, Valid Loss 13.250388, Error 0.021363\n",
      "Epoch : 215/2000, Train Loss : 10.883720, Valid Loss 13.001090, Error 0.020392\n",
      "Epoch : 216/2000, Train Loss : 11.196219, Valid Loss 13.777397, Error 0.022328\n",
      "Epoch : 217/2000, Train Loss : 10.523507, Valid Loss 13.140752, Error 0.021107\n",
      "Epoch : 218/2000, Train Loss : 10.892065, Valid Loss 12.742526, Error 0.020511\n",
      "Epoch : 219/2000, Train Loss : 10.833103, Valid Loss 15.854297, Error 0.023459\n",
      "Epoch : 220/2000, Train Loss : 12.844118, Valid Loss 14.138239, Error 0.021798\n",
      "Epoch : 221/2000, Train Loss : 10.867956, Valid Loss 13.381538, Error 0.021448\n",
      "Epoch : 222/2000, Train Loss : 11.086153, Valid Loss 13.406042, Error 0.020780\n",
      "Epoch : 223/2000, Train Loss : 11.139555, Valid Loss 14.816206, Error 0.022505\n",
      "Epoch : 224/2000, Train Loss : 11.897699, Valid Loss 14.936887, Error 0.022839\n",
      "Epoch : 225/2000, Train Loss : 10.752494, Valid Loss 13.438727, Error 0.020829\n",
      "Epoch : 226/2000, Train Loss : 11.689111, Valid Loss 13.546145, Error 0.021516\n",
      "Epoch : 227/2000, Train Loss : 11.400018, Valid Loss 16.656316, Error 0.023566\n",
      "Epoch : 228/2000, Train Loss : 10.980618, Valid Loss 12.781430, Error 0.020102\n",
      "Epoch : 229/2000, Train Loss : 10.839134, Valid Loss 15.163446, Error 0.022295\n",
      "Epoch : 230/2000, Train Loss : 10.304361, Valid Loss 13.107068, Error 0.021683\n",
      "Epoch : 231/2000, Train Loss : 10.139284, Valid Loss 12.405888, Error 0.020184\n",
      "Epoch : 232/2000, Train Loss : 10.863931, Valid Loss 17.271752, Error 0.025032\n",
      "Epoch : 233/2000, Train Loss : 10.501426, Valid Loss 12.885791, Error 0.021605\n",
      "Epoch : 234/2000, Train Loss : 9.628040, Valid Loss 12.482892, Error 0.021479\n",
      "Epoch : 235/2000, Train Loss : 10.088462, Valid Loss 12.503995, Error 0.020546\n",
      "Epoch : 236/2000, Train Loss : 10.041465, Valid Loss 14.146691, Error 0.022592\n",
      "Epoch : 237/2000, Train Loss : 9.843682, Valid Loss 13.224093, Error 0.020998\n",
      "Epoch : 238/2000, Train Loss : 10.125430, Valid Loss 12.803759, Error 0.020759\n",
      "Epoch : 239/2000, Train Loss : 10.212096, Valid Loss 12.371543, Error 0.020543\n",
      "Epoch : 240/2000, Train Loss : 10.104919, Valid Loss 12.187213, Error 0.020181\n",
      "Epoch : 241/2000, Train Loss : 9.701396, Valid Loss 12.371353, Error 0.019921\n",
      "Epoch : 242/2000, Train Loss : 10.706463, Valid Loss 14.295787, Error 0.021936\n",
      "Epoch : 243/2000, Train Loss : 10.050871, Valid Loss 12.873439, Error 0.021333\n",
      "Epoch : 244/2000, Train Loss : 10.679859, Valid Loss 13.852346, Error 0.022525\n",
      "Epoch : 245/2000, Train Loss : 11.173747, Valid Loss 12.446826, Error 0.020377\n",
      "Epoch : 246/2000, Train Loss : 11.591860, Valid Loss 12.528781, Error 0.020599\n",
      "Epoch : 247/2000, Train Loss : 9.969472, Valid Loss 12.006316, Error 0.020325\n",
      "Epoch : 248/2000, Train Loss : 9.525228, Valid Loss 12.064458, Error 0.020568\n",
      "Epoch : 249/2000, Train Loss : 9.184869, Valid Loss 12.520063, Error 0.020035\n",
      "Epoch : 250/2000, Train Loss : 10.007896, Valid Loss 14.132474, Error 0.022247\n",
      "Epoch : 251/2000, Train Loss : 10.243764, Valid Loss 12.618790, Error 0.020822\n",
      "Epoch : 252/2000, Train Loss : 9.916901, Valid Loss 13.698276, Error 0.021283\n",
      "Epoch : 253/2000, Train Loss : 9.988504, Valid Loss 12.924202, Error 0.020897\n",
      "Epoch : 254/2000, Train Loss : 9.506617, Valid Loss 12.379949, Error 0.020817\n",
      "Epoch : 255/2000, Train Loss : 9.876442, Valid Loss 11.966573, Error 0.019644\n",
      "Epoch : 256/2000, Train Loss : 10.472654, Valid Loss 12.077810, Error 0.019135\n",
      "Epoch : 257/2000, Train Loss : 10.387184, Valid Loss 15.147493, Error 0.023856\n",
      "Epoch : 258/2000, Train Loss : 11.000570, Valid Loss 12.452631, Error 0.020885\n",
      "Epoch : 259/2000, Train Loss : 10.389881, Valid Loss 15.455338, Error 0.023272\n",
      "Epoch : 260/2000, Train Loss : 10.384823, Valid Loss 13.142311, Error 0.021749\n",
      "Epoch : 261/2000, Train Loss : 9.503790, Valid Loss 12.576062, Error 0.020805\n",
      "Epoch : 262/2000, Train Loss : 10.341649, Valid Loss 14.639509, Error 0.022594\n",
      "Epoch : 263/2000, Train Loss : 9.749091, Valid Loss 11.872638, Error 0.020268\n",
      "Epoch : 264/2000, Train Loss : 9.031100, Valid Loss 12.699616, Error 0.020914\n",
      "Epoch : 265/2000, Train Loss : 9.392595, Valid Loss 11.761766, Error 0.019202\n",
      "Epoch : 266/2000, Train Loss : 9.502984, Valid Loss 12.162870, Error 0.021071\n",
      "Epoch : 267/2000, Train Loss : 9.012956, Valid Loss 11.513519, Error 0.020035\n",
      "Epoch : 268/2000, Train Loss : 8.989597, Valid Loss 12.449083, Error 0.020821\n",
      "Epoch : 269/2000, Train Loss : 9.525098, Valid Loss 13.118968, Error 0.022101\n",
      "Epoch : 270/2000, Train Loss : 10.215536, Valid Loss 11.613433, Error 0.019618\n",
      "Epoch : 271/2000, Train Loss : 9.009546, Valid Loss 12.028490, Error 0.019910\n",
      "Epoch : 272/2000, Train Loss : 9.566855, Valid Loss 12.352205, Error 0.020639\n",
      "Epoch : 273/2000, Train Loss : 10.990998, Valid Loss 19.157872, Error 0.026555\n",
      "Epoch : 274/2000, Train Loss : 11.282441, Valid Loss 15.883158, Error 0.023450\n",
      "Epoch : 275/2000, Train Loss : 9.432067, Valid Loss 13.715742, Error 0.021244\n",
      "Epoch : 276/2000, Train Loss : 9.880774, Valid Loss 13.300105, Error 0.021303\n",
      "Epoch : 277/2000, Train Loss : 9.663641, Valid Loss 12.336324, Error 0.020731\n",
      "Epoch : 278/2000, Train Loss : 9.153115, Valid Loss 12.389852, Error 0.020559\n",
      "Epoch : 279/2000, Train Loss : 9.236832, Valid Loss 12.339843, Error 0.020622\n",
      "Epoch : 280/2000, Train Loss : 8.753091, Valid Loss 12.258338, Error 0.020418\n",
      "Epoch : 281/2000, Train Loss : 9.423499, Valid Loss 14.326628, Error 0.021615\n",
      "Epoch : 282/2000, Train Loss : 9.573631, Valid Loss 15.826229, Error 0.022981\n",
      "Epoch : 283/2000, Train Loss : 9.095230, Valid Loss 11.737444, Error 0.019347\n",
      "Epoch : 284/2000, Train Loss : 9.014971, Valid Loss 11.449366, Error 0.019821\n",
      "Epoch : 285/2000, Train Loss : 8.500749, Valid Loss 12.016580, Error 0.019477\n",
      "Epoch : 286/2000, Train Loss : 8.403595, Valid Loss 12.777356, Error 0.020701\n",
      "Epoch : 287/2000, Train Loss : 8.597865, Valid Loss 13.470273, Error 0.021359\n",
      "Epoch : 288/2000, Train Loss : 8.863307, Valid Loss 12.551867, Error 0.021033\n",
      "Epoch : 289/2000, Train Loss : 8.367262, Valid Loss 11.549209, Error 0.019403\n",
      "Epoch : 290/2000, Train Loss : 8.738129, Valid Loss 11.471714, Error 0.019366\n",
      "Epoch : 291/2000, Train Loss : 8.720853, Valid Loss 11.897545, Error 0.020494\n",
      "Epoch : 292/2000, Train Loss : 8.930366, Valid Loss 13.205225, Error 0.020582\n",
      "Epoch : 293/2000, Train Loss : 8.535187, Valid Loss 12.058057, Error 0.020167\n",
      "Epoch : 294/2000, Train Loss : 8.276054, Valid Loss 11.468551, Error 0.019412\n",
      "Epoch : 295/2000, Train Loss : 8.342562, Valid Loss 11.471847, Error 0.020175\n",
      "Epoch : 296/2000, Train Loss : 9.078309, Valid Loss 13.490878, Error 0.021913\n",
      "Epoch : 297/2000, Train Loss : 9.976617, Valid Loss 11.967025, Error 0.020218\n",
      "Epoch : 298/2000, Train Loss : 8.614910, Valid Loss 10.993048, Error 0.019183\n",
      "Epoch : 299/2000, Train Loss : 8.365560, Valid Loss 12.375111, Error 0.020031\n",
      "Epoch : 300/2000, Train Loss : 8.441839, Valid Loss 11.472815, Error 0.019783\n",
      "Epoch : 301/2000, Train Loss : 8.563362, Valid Loss 17.521033, Error 0.026243\n",
      "Epoch : 302/2000, Train Loss : 9.147151, Valid Loss 11.390021, Error 0.019616\n",
      "Epoch : 303/2000, Train Loss : 8.401083, Valid Loss 12.592488, Error 0.020229\n",
      "Epoch : 304/2000, Train Loss : 8.772740, Valid Loss 11.119659, Error 0.018159\n",
      "Epoch : 305/2000, Train Loss : 10.757817, Valid Loss 17.566758, Error 0.025571\n",
      "Epoch : 306/2000, Train Loss : 12.596341, Valid Loss 15.632336, Error 0.023229\n",
      "Epoch : 307/2000, Train Loss : 11.240604, Valid Loss 14.419315, Error 0.022378\n",
      "Epoch : 308/2000, Train Loss : 8.916700, Valid Loss 11.935783, Error 0.020066\n",
      "Epoch : 309/2000, Train Loss : 8.199076, Valid Loss 11.655985, Error 0.020272\n",
      "Epoch : 310/2000, Train Loss : 8.152240, Valid Loss 11.392888, Error 0.019288\n",
      "Epoch : 311/2000, Train Loss : 8.262908, Valid Loss 14.722477, Error 0.023287\n",
      "Epoch : 312/2000, Train Loss : 8.744720, Valid Loss 11.091289, Error 0.018377\n",
      "Epoch : 313/2000, Train Loss : 7.813502, Valid Loss 11.865423, Error 0.019687\n",
      "Epoch : 314/2000, Train Loss : 8.152817, Valid Loss 13.372526, Error 0.021720\n",
      "Epoch : 315/2000, Train Loss : 8.305641, Valid Loss 13.139332, Error 0.020713\n",
      "Epoch : 316/2000, Train Loss : 8.238806, Valid Loss 11.613570, Error 0.020017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 317/2000, Train Loss : 8.077453, Valid Loss 11.687476, Error 0.019892\n",
      "Epoch : 318/2000, Train Loss : 7.667007, Valid Loss 11.555269, Error 0.019508\n",
      "Epoch : 319/2000, Train Loss : 7.871622, Valid Loss 11.869888, Error 0.019518\n",
      "Early stopping\n",
      "Total Error Mean 0.042225\n"
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "total_error = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model_atn.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model_atn.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    error = torch.sum(torch.abs(outputs - targets) / targets) / len(targets)\n",
    "    total_error += error\n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, Error {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      error))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Total Error Mean {:4f}'.format(total_error/(epoch+1)))\n",
    "        break\n",
    "\n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7621716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85211f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86606fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cnn_acc = nn.Conv2d(in_channels=3, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait",
   "language": "python",
   "name": "gait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
