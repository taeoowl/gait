{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dc5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error # mse}\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from dataloader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f5a4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        print(h0.shape)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        _, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        _, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "    \n",
    "        inputs_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        out_lstm = self.reg_module1(inputs_concat)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83facf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "dataset = Gait_Dataset_Salted(file_path)\n",
    "val_percent = 0.2\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e009bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8209fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98bbbae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 1/2000, Train Loss : 17630.685140, Valid Loss 17414.996094, Error 0.989701\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 2/2000, Train Loss : 16989.953857, Valid Loss 16230.190592, Error 0.953428\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 3/2000, Train Loss : 14873.753784, Valid Loss 12830.360677, Error 0.841101\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 4/2000, Train Loss : 9960.685201, Valid Loss 6369.623535, Error 0.573284\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 5/2000, Train Loss : 3283.304199, Valid Loss 794.227356, Error 0.157922\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 6/2000, Train Loss : 389.027940, Valid Loss 359.618011, Error 0.131208\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 7/2000, Train Loss : 322.062281, Valid Loss 308.240402, Error 0.116832\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 84, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 128, 64])\n",
      "torch.Size([1, 116, 64])\n",
      "Epoch : 8/2000, Train Loss : 304.442803, Valid Loss 308.309077, Error 0.116632"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32336\\1982048126.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     72\u001b[0m     print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, Error {:.6f}\".format(epoch+1, n_epochs,\n\u001b[0;32m     73\u001b[0m                                                                                        \u001b[0mtrn_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m                                                                                       error))\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# Early Stopping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gait\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[1;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m                 \u001b[1;31m# mp.Pool cannot be trusted to flush promptly (or ever),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gait\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gait\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    545\u001b[0m                 )\n\u001b[0;32m    546\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gait\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "total_error = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    error = torch.sum(torch.abs(outputs - targets) / targets) / len(targets)\n",
    "    total_error += error\n",
    "    \n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, Error {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      error))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Total Error Mean {:4f}'.format(total_error/(epoch+1)))\n",
    "        break\n",
    "    \n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01162acc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c353f807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ec1eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a04e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b344b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce56a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81be170",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class LSTM_atn(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc975cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 64\n",
    "num_layers = 1\n",
    "\n",
    "model_atn = LSTM_atn(input_size, hidden_size, num_layers).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model_atn.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc9767a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 17746.155599, Valid Loss 17663.092773, Error 0.996257\n",
      "Epoch : 2/2000, Train Loss : 17380.334147, Valid Loss 16920.583984, Error 0.974798\n",
      "Epoch : 3/2000, Train Loss : 15908.585612, Valid Loss 14371.164388, Error 0.897196\n",
      "Epoch : 4/2000, Train Loss : 11838.384481, Valid Loss 8483.621582, Error 0.684904\n",
      "Epoch : 5/2000, Train Loss : 4909.334173, Valid Loss 1584.802490, Error 0.271992\n",
      "Epoch : 6/2000, Train Loss : 565.266740, Valid Loss 394.996328, Error 0.128506\n",
      "Epoch : 7/2000, Train Loss : 329.174536, Valid Loss 340.072881, Error 0.123538\n",
      "Epoch : 8/2000, Train Loss : 296.477468, Valid Loss 340.002772, Error 0.123510\n",
      "Epoch : 9/2000, Train Loss : 295.836207, Valid Loss 339.882629, Error 0.123446\n",
      "Epoch : 10/2000, Train Loss : 295.486719, Valid Loss 339.912120, Error 0.123472\n",
      "Epoch : 11/2000, Train Loss : 295.184500, Valid Loss 339.863561, Error 0.123433\n",
      "Epoch : 12/2000, Train Loss : 296.545444, Valid Loss 340.076136, Error 0.123540\n",
      "Epoch : 13/2000, Train Loss : 294.946229, Valid Loss 339.854431, Error 0.123438\n",
      "Epoch : 14/2000, Train Loss : 294.862552, Valid Loss 340.019643, Error 0.123523\n",
      "Epoch : 15/2000, Train Loss : 294.877044, Valid Loss 339.817434, Error 0.123418\n",
      "Epoch : 16/2000, Train Loss : 294.848628, Valid Loss 339.783452, Error 0.123420\n",
      "Epoch : 17/2000, Train Loss : 295.017262, Valid Loss 339.360936, Error 0.123323\n",
      "Epoch : 18/2000, Train Loss : 292.719739, Valid Loss 333.350235, Error 0.122192\n",
      "Epoch : 19/2000, Train Loss : 284.976927, Valid Loss 319.274663, Error 0.120121\n",
      "Epoch : 20/2000, Train Loss : 269.810402, Valid Loss 311.082342, Error 0.116594\n",
      "Epoch : 21/2000, Train Loss : 253.366926, Valid Loss 274.417819, Error 0.109991\n",
      "Epoch : 22/2000, Train Loss : 226.034573, Valid Loss 253.345248, Error 0.108301\n",
      "Epoch : 23/2000, Train Loss : 205.381980, Valid Loss 209.580866, Error 0.096801\n",
      "Epoch : 24/2000, Train Loss : 178.651913, Valid Loss 181.918142, Error 0.089978\n",
      "Epoch : 25/2000, Train Loss : 160.750230, Valid Loss 161.475731, Error 0.083765\n",
      "Epoch : 26/2000, Train Loss : 140.399963, Valid Loss 135.445318, Error 0.077339\n",
      "Epoch : 27/2000, Train Loss : 120.676627, Valid Loss 114.604837, Error 0.071518\n",
      "Epoch : 28/2000, Train Loss : 119.167442, Valid Loss 105.989690, Error 0.066888\n",
      "Epoch : 29/2000, Train Loss : 105.127706, Valid Loss 95.019975, Error 0.062763\n",
      "Epoch : 30/2000, Train Loss : 89.500161, Valid Loss 86.874620, Error 0.060904\n",
      "Epoch : 31/2000, Train Loss : 82.375968, Valid Loss 79.259506, Error 0.053925\n",
      "Epoch : 32/2000, Train Loss : 73.736136, Valid Loss 63.797724, Error 0.048389\n",
      "Epoch : 33/2000, Train Loss : 65.693051, Valid Loss 58.884479, Error 0.044746\n",
      "Epoch : 34/2000, Train Loss : 60.965403, Valid Loss 51.474033, Error 0.042420\n",
      "Epoch : 35/2000, Train Loss : 59.008686, Valid Loss 53.671929, Error 0.041714\n",
      "Epoch : 36/2000, Train Loss : 51.879035, Valid Loss 45.732187, Error 0.040263\n",
      "Epoch : 37/2000, Train Loss : 49.906691, Valid Loss 55.248451, Error 0.046250\n",
      "Epoch : 38/2000, Train Loss : 48.228613, Valid Loss 39.790150, Error 0.036455\n",
      "Epoch : 39/2000, Train Loss : 45.640045, Valid Loss 39.472632, Error 0.034993\n",
      "Epoch : 40/2000, Train Loss : 42.263751, Valid Loss 36.572996, Error 0.035190\n",
      "Epoch : 41/2000, Train Loss : 40.745937, Valid Loss 34.994902, Error 0.034676\n",
      "Epoch : 42/2000, Train Loss : 39.428334, Valid Loss 37.691392, Error 0.038224\n",
      "Epoch : 43/2000, Train Loss : 37.774772, Valid Loss 42.641071, Error 0.036270\n",
      "Epoch : 44/2000, Train Loss : 36.862978, Valid Loss 31.629064, Error 0.032656\n",
      "Epoch : 45/2000, Train Loss : 34.214095, Valid Loss 31.460793, Error 0.032372\n",
      "Epoch : 46/2000, Train Loss : 33.584019, Valid Loss 31.170683, Error 0.033978\n",
      "Epoch : 47/2000, Train Loss : 33.105454, Valid Loss 31.436364, Error 0.032098\n",
      "Epoch : 48/2000, Train Loss : 30.862766, Valid Loss 29.917188, Error 0.033005\n",
      "Epoch : 49/2000, Train Loss : 32.046125, Valid Loss 28.229021, Error 0.032779\n",
      "Epoch : 50/2000, Train Loss : 32.380711, Valid Loss 28.622046, Error 0.031022\n",
      "Epoch : 51/2000, Train Loss : 29.036500, Valid Loss 28.944723, Error 0.030971\n",
      "Epoch : 52/2000, Train Loss : 28.529365, Valid Loss 27.612034, Error 0.030611\n",
      "Epoch : 53/2000, Train Loss : 28.666894, Valid Loss 28.620946, Error 0.030449\n",
      "Epoch : 54/2000, Train Loss : 32.510382, Valid Loss 28.506569, Error 0.032454\n",
      "Epoch : 55/2000, Train Loss : 27.756232, Valid Loss 26.653507, Error 0.031307\n",
      "Epoch : 56/2000, Train Loss : 27.737658, Valid Loss 25.484940, Error 0.031198\n",
      "Epoch : 57/2000, Train Loss : 27.286329, Valid Loss 24.305438, Error 0.029072\n",
      "Epoch : 58/2000, Train Loss : 26.070024, Valid Loss 28.215765, Error 0.030063\n",
      "Epoch : 59/2000, Train Loss : 25.892390, Valid Loss 31.437661, Error 0.034912\n",
      "Epoch : 60/2000, Train Loss : 27.172318, Valid Loss 23.698163, Error 0.028461\n",
      "Epoch : 61/2000, Train Loss : 26.226291, Valid Loss 24.768416, Error 0.029213\n",
      "Epoch : 62/2000, Train Loss : 27.148716, Valid Loss 23.361794, Error 0.028963\n",
      "Epoch : 63/2000, Train Loss : 23.484789, Valid Loss 25.665076, Error 0.028903\n",
      "Epoch : 64/2000, Train Loss : 23.392354, Valid Loss 25.235213, Error 0.028635\n",
      "Epoch : 65/2000, Train Loss : 23.829504, Valid Loss 22.651375, Error 0.027501\n",
      "Epoch : 66/2000, Train Loss : 23.208577, Valid Loss 24.322181, Error 0.028362\n",
      "Epoch : 67/2000, Train Loss : 22.806966, Valid Loss 22.480411, Error 0.027995\n",
      "Epoch : 68/2000, Train Loss : 23.713401, Valid Loss 23.209682, Error 0.027365\n",
      "Epoch : 69/2000, Train Loss : 22.668682, Valid Loss 22.988604, Error 0.028683\n",
      "Epoch : 70/2000, Train Loss : 22.305296, Valid Loss 21.357707, Error 0.027051\n",
      "Epoch : 71/2000, Train Loss : 21.897731, Valid Loss 22.396448, Error 0.028017\n",
      "Epoch : 72/2000, Train Loss : 22.281193, Valid Loss 22.136411, Error 0.027142\n",
      "Epoch : 73/2000, Train Loss : 22.507941, Valid Loss 20.887829, Error 0.026670\n",
      "Epoch : 74/2000, Train Loss : 21.432510, Valid Loss 22.759131, Error 0.027335\n",
      "Epoch : 75/2000, Train Loss : 21.533336, Valid Loss 20.304359, Error 0.025855\n",
      "Epoch : 76/2000, Train Loss : 20.357301, Valid Loss 21.342022, Error 0.026787\n",
      "Epoch : 77/2000, Train Loss : 19.961457, Valid Loss 21.228907, Error 0.026372\n",
      "Epoch : 78/2000, Train Loss : 21.283344, Valid Loss 23.162016, Error 0.027679\n",
      "Epoch : 79/2000, Train Loss : 20.539526, Valid Loss 27.181420, Error 0.029945\n",
      "Epoch : 80/2000, Train Loss : 20.463899, Valid Loss 22.218242, Error 0.026194\n",
      "Epoch : 81/2000, Train Loss : 19.964327, Valid Loss 19.642453, Error 0.026516\n",
      "Epoch : 82/2000, Train Loss : 19.101043, Valid Loss 20.954659, Error 0.025623\n",
      "Epoch : 83/2000, Train Loss : 18.918130, Valid Loss 20.222287, Error 0.025625\n",
      "Epoch : 84/2000, Train Loss : 21.299348, Valid Loss 19.516720, Error 0.025228\n",
      "Epoch : 85/2000, Train Loss : 18.834789, Valid Loss 21.568552, Error 0.026563\n",
      "Epoch : 86/2000, Train Loss : 19.838210, Valid Loss 18.958966, Error 0.025005\n",
      "Epoch : 87/2000, Train Loss : 18.742500, Valid Loss 19.063059, Error 0.025103\n",
      "Epoch : 88/2000, Train Loss : 18.977407, Valid Loss 19.354151, Error 0.025388\n",
      "Epoch : 89/2000, Train Loss : 18.999027, Valid Loss 21.581952, Error 0.026551\n",
      "Epoch : 90/2000, Train Loss : 20.260914, Valid Loss 19.604070, Error 0.025300\n",
      "Epoch : 91/2000, Train Loss : 18.133022, Valid Loss 21.156068, Error 0.026201\n",
      "Epoch : 92/2000, Train Loss : 18.410487, Valid Loss 19.750679, Error 0.025167\n",
      "Epoch : 93/2000, Train Loss : 18.545669, Valid Loss 20.263473, Error 0.026370\n",
      "Epoch : 94/2000, Train Loss : 18.859083, Valid Loss 19.521643, Error 0.026508\n",
      "Epoch : 95/2000, Train Loss : 17.738393, Valid Loss 18.728705, Error 0.024656\n",
      "Epoch : 96/2000, Train Loss : 17.532592, Valid Loss 17.471943, Error 0.024050\n",
      "Epoch : 97/2000, Train Loss : 17.971662, Valid Loss 17.911412, Error 0.023967\n",
      "Epoch : 98/2000, Train Loss : 16.833942, Valid Loss 18.258144, Error 0.024346\n",
      "Epoch : 99/2000, Train Loss : 16.856601, Valid Loss 18.100742, Error 0.023566\n",
      "Epoch : 100/2000, Train Loss : 16.854650, Valid Loss 22.984829, Error 0.028729\n",
      "Epoch : 101/2000, Train Loss : 18.258764, Valid Loss 17.689452, Error 0.023562\n",
      "Epoch : 102/2000, Train Loss : 16.599231, Valid Loss 23.436054, Error 0.028163\n",
      "Epoch : 103/2000, Train Loss : 16.860558, Valid Loss 18.469821, Error 0.023948\n",
      "Epoch : 104/2000, Train Loss : 16.306420, Valid Loss 21.383687, Error 0.026154\n",
      "Epoch : 105/2000, Train Loss : 18.215283, Valid Loss 17.920603, Error 0.023236\n",
      "Epoch : 106/2000, Train Loss : 17.268888, Valid Loss 21.424931, Error 0.026351\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 107/2000, Train Loss : 16.216515, Valid Loss 23.194283, Error 0.027629\n",
      "Epoch : 108/2000, Train Loss : 17.692392, Valid Loss 17.909806, Error 0.024385\n",
      "Epoch : 109/2000, Train Loss : 16.120957, Valid Loss 16.687341, Error 0.022427\n",
      "Epoch : 110/2000, Train Loss : 15.845936, Valid Loss 17.254032, Error 0.023990\n",
      "Epoch : 111/2000, Train Loss : 15.293227, Valid Loss 16.690448, Error 0.023370\n",
      "Epoch : 112/2000, Train Loss : 15.404807, Valid Loss 17.810793, Error 0.024242\n",
      "Epoch : 113/2000, Train Loss : 16.718792, Valid Loss 16.615735, Error 0.022822\n",
      "Epoch : 114/2000, Train Loss : 15.151704, Valid Loss 16.642402, Error 0.022911\n",
      "Epoch : 115/2000, Train Loss : 16.668798, Valid Loss 17.154168, Error 0.024107\n",
      "Epoch : 116/2000, Train Loss : 15.669579, Valid Loss 16.409843, Error 0.022480\n",
      "Epoch : 117/2000, Train Loss : 15.110237, Valid Loss 19.216903, Error 0.024980\n",
      "Epoch : 118/2000, Train Loss : 15.322914, Valid Loss 16.188718, Error 0.023355\n",
      "Epoch : 119/2000, Train Loss : 15.127932, Valid Loss 17.253812, Error 0.023894\n",
      "Epoch : 120/2000, Train Loss : 15.331670, Valid Loss 18.026510, Error 0.025080\n",
      "Epoch : 121/2000, Train Loss : 15.616267, Valid Loss 16.919228, Error 0.023823\n",
      "Epoch : 122/2000, Train Loss : 14.751344, Valid Loss 17.061991, Error 0.023455\n",
      "Epoch : 123/2000, Train Loss : 15.135407, Valid Loss 16.279701, Error 0.023508\n",
      "Epoch : 124/2000, Train Loss : 15.757039, Valid Loss 16.695213, Error 0.022963\n",
      "Epoch : 125/2000, Train Loss : 14.565277, Valid Loss 15.993795, Error 0.022703\n",
      "Epoch : 126/2000, Train Loss : 14.279736, Valid Loss 17.890407, Error 0.024722\n",
      "Epoch : 127/2000, Train Loss : 15.457915, Valid Loss 16.785359, Error 0.023763\n",
      "Epoch : 128/2000, Train Loss : 14.554918, Valid Loss 15.739833, Error 0.022477\n",
      "Epoch : 129/2000, Train Loss : 16.620396, Valid Loss 17.652886, Error 0.024603\n",
      "Epoch : 130/2000, Train Loss : 17.119233, Valid Loss 15.596418, Error 0.022798\n",
      "Epoch : 131/2000, Train Loss : 15.777952, Valid Loss 16.483868, Error 0.023389\n",
      "Epoch : 132/2000, Train Loss : 15.635896, Valid Loss 17.681403, Error 0.025017\n",
      "Epoch : 133/2000, Train Loss : 13.888127, Valid Loss 15.922627, Error 0.023323\n",
      "Epoch : 134/2000, Train Loss : 13.737313, Valid Loss 16.675728, Error 0.023595\n",
      "Epoch : 135/2000, Train Loss : 14.001831, Valid Loss 16.759601, Error 0.024129\n",
      "Epoch : 136/2000, Train Loss : 14.223570, Valid Loss 15.432080, Error 0.023494\n",
      "Epoch : 137/2000, Train Loss : 13.950149, Valid Loss 18.798962, Error 0.025156\n",
      "Epoch : 138/2000, Train Loss : 14.657693, Valid Loss 17.557986, Error 0.024129\n",
      "Epoch : 139/2000, Train Loss : 15.137798, Valid Loss 16.031716, Error 0.023053\n",
      "Epoch : 140/2000, Train Loss : 13.787189, Valid Loss 16.812127, Error 0.024541\n",
      "Epoch : 141/2000, Train Loss : 14.854703, Valid Loss 15.680044, Error 0.023351\n",
      "Epoch : 142/2000, Train Loss : 14.143747, Valid Loss 16.216557, Error 0.022736\n",
      "Epoch : 143/2000, Train Loss : 15.813749, Valid Loss 15.206968, Error 0.022154\n",
      "Epoch : 144/2000, Train Loss : 13.532925, Valid Loss 14.979256, Error 0.022640\n",
      "Epoch : 145/2000, Train Loss : 13.323354, Valid Loss 14.827536, Error 0.021670\n",
      "Epoch : 146/2000, Train Loss : 13.216369, Valid Loss 15.216122, Error 0.022354\n",
      "Epoch : 147/2000, Train Loss : 13.425589, Valid Loss 17.440919, Error 0.023943\n",
      "Epoch : 148/2000, Train Loss : 15.645339, Valid Loss 15.635047, Error 0.022697\n",
      "Epoch : 149/2000, Train Loss : 14.609543, Valid Loss 15.139165, Error 0.022882\n",
      "Epoch : 150/2000, Train Loss : 13.031153, Valid Loss 15.704482, Error 0.022076\n",
      "Epoch : 151/2000, Train Loss : 13.329669, Valid Loss 14.491177, Error 0.021980\n",
      "Epoch : 152/2000, Train Loss : 13.242407, Valid Loss 14.783349, Error 0.022385\n",
      "Epoch : 153/2000, Train Loss : 12.920271, Valid Loss 16.659588, Error 0.023108\n",
      "Epoch : 154/2000, Train Loss : 14.019866, Valid Loss 15.929471, Error 0.023480\n",
      "Epoch : 155/2000, Train Loss : 13.193384, Valid Loss 18.100763, Error 0.025140\n",
      "Epoch : 156/2000, Train Loss : 14.302159, Valid Loss 19.860036, Error 0.026930\n",
      "Epoch : 157/2000, Train Loss : 14.048058, Valid Loss 14.866783, Error 0.023377\n",
      "Epoch : 158/2000, Train Loss : 13.898058, Valid Loss 17.010718, Error 0.023581\n",
      "Epoch : 159/2000, Train Loss : 13.362877, Valid Loss 14.228408, Error 0.022315\n",
      "Epoch : 160/2000, Train Loss : 12.647187, Valid Loss 16.403510, Error 0.023381\n",
      "Epoch : 161/2000, Train Loss : 13.289107, Valid Loss 15.687453, Error 0.022508\n",
      "Epoch : 162/2000, Train Loss : 12.474895, Valid Loss 19.347897, Error 0.026060\n",
      "Epoch : 163/2000, Train Loss : 13.603176, Valid Loss 15.228276, Error 0.022414\n",
      "Epoch : 164/2000, Train Loss : 12.725355, Valid Loss 14.805349, Error 0.022648\n",
      "Epoch : 165/2000, Train Loss : 12.538205, Valid Loss 15.356894, Error 0.023162\n",
      "Epoch : 166/2000, Train Loss : 12.124081, Valid Loss 14.533255, Error 0.021978\n",
      "Epoch : 167/2000, Train Loss : 12.614660, Valid Loss 15.173474, Error 0.022962\n",
      "Epoch : 168/2000, Train Loss : 12.317833, Valid Loss 14.586211, Error 0.022543\n",
      "Epoch : 169/2000, Train Loss : 12.313669, Valid Loss 16.221711, Error 0.024466\n",
      "Epoch : 170/2000, Train Loss : 12.866306, Valid Loss 15.223872, Error 0.021912\n",
      "Epoch : 171/2000, Train Loss : 13.273710, Valid Loss 14.857666, Error 0.022929\n",
      "Epoch : 172/2000, Train Loss : 12.706718, Valid Loss 14.232907, Error 0.022657\n",
      "Epoch : 173/2000, Train Loss : 12.522770, Valid Loss 14.312564, Error 0.021471\n",
      "Epoch : 174/2000, Train Loss : 11.790019, Valid Loss 14.297392, Error 0.021989\n",
      "Epoch : 175/2000, Train Loss : 11.957468, Valid Loss 14.112153, Error 0.022234\n",
      "Epoch : 176/2000, Train Loss : 13.726741, Valid Loss 14.011361, Error 0.021975\n",
      "Epoch : 177/2000, Train Loss : 11.836272, Valid Loss 14.686705, Error 0.023226\n",
      "Epoch : 178/2000, Train Loss : 11.896774, Valid Loss 15.319928, Error 0.022708\n",
      "Epoch : 179/2000, Train Loss : 11.789518, Valid Loss 14.571828, Error 0.022542\n",
      "Epoch : 180/2000, Train Loss : 12.682329, Valid Loss 14.646289, Error 0.022815\n",
      "Epoch : 181/2000, Train Loss : 12.495691, Valid Loss 14.995644, Error 0.022334\n",
      "Epoch : 182/2000, Train Loss : 12.409244, Valid Loss 14.593208, Error 0.022226\n",
      "Epoch : 183/2000, Train Loss : 12.300367, Valid Loss 13.733084, Error 0.021579\n",
      "Epoch : 184/2000, Train Loss : 11.779835, Valid Loss 13.495690, Error 0.021488\n",
      "Epoch : 185/2000, Train Loss : 13.137535, Valid Loss 13.678357, Error 0.021143\n",
      "Epoch : 186/2000, Train Loss : 12.075946, Valid Loss 16.273148, Error 0.023036\n",
      "Epoch : 187/2000, Train Loss : 12.732440, Valid Loss 15.311656, Error 0.022965\n",
      "Epoch : 188/2000, Train Loss : 12.263411, Valid Loss 13.605248, Error 0.021903\n",
      "Epoch : 189/2000, Train Loss : 11.538155, Valid Loss 13.554310, Error 0.020706\n",
      "Epoch : 190/2000, Train Loss : 12.400243, Valid Loss 14.061210, Error 0.021782\n",
      "Epoch : 191/2000, Train Loss : 12.454375, Valid Loss 13.606939, Error 0.021122\n",
      "Epoch : 192/2000, Train Loss : 13.401533, Valid Loss 13.501137, Error 0.021570\n",
      "Epoch : 193/2000, Train Loss : 12.320254, Valid Loss 14.756367, Error 0.022134\n",
      "Epoch : 194/2000, Train Loss : 11.556222, Valid Loss 14.244482, Error 0.021748\n",
      "Epoch : 195/2000, Train Loss : 11.333191, Valid Loss 14.244691, Error 0.022436\n",
      "Epoch : 196/2000, Train Loss : 11.255745, Valid Loss 13.717304, Error 0.021810\n",
      "Epoch : 197/2000, Train Loss : 11.225446, Valid Loss 13.847986, Error 0.021288\n",
      "Epoch : 198/2000, Train Loss : 11.529629, Valid Loss 15.154387, Error 0.022721\n",
      "Epoch : 199/2000, Train Loss : 11.716140, Valid Loss 15.441979, Error 0.023399\n",
      "Epoch : 200/2000, Train Loss : 11.235301, Valid Loss 13.134509, Error 0.021324\n",
      "Epoch : 201/2000, Train Loss : 12.085870, Valid Loss 13.035253, Error 0.020526\n",
      "Epoch : 202/2000, Train Loss : 11.334279, Valid Loss 13.438747, Error 0.021225\n",
      "Epoch : 203/2000, Train Loss : 11.599812, Valid Loss 12.943549, Error 0.021015\n",
      "Epoch : 204/2000, Train Loss : 10.895675, Valid Loss 13.310054, Error 0.021673\n",
      "Epoch : 205/2000, Train Loss : 10.955489, Valid Loss 13.336187, Error 0.021016\n",
      "Epoch : 206/2000, Train Loss : 11.145270, Valid Loss 13.016750, Error 0.020716\n",
      "Epoch : 207/2000, Train Loss : 11.343640, Valid Loss 15.628100, Error 0.023281\n",
      "Epoch : 208/2000, Train Loss : 12.280028, Valid Loss 15.049814, Error 0.023252\n",
      "Epoch : 209/2000, Train Loss : 10.688243, Valid Loss 13.578625, Error 0.021281\n",
      "Epoch : 210/2000, Train Loss : 10.896347, Valid Loss 13.251416, Error 0.021268\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 211/2000, Train Loss : 11.757466, Valid Loss 19.493862, Error 0.025797\n",
      "Epoch : 212/2000, Train Loss : 11.808342, Valid Loss 15.093043, Error 0.022005\n",
      "Epoch : 213/2000, Train Loss : 10.690721, Valid Loss 13.177488, Error 0.021580\n",
      "Epoch : 214/2000, Train Loss : 10.857700, Valid Loss 13.250388, Error 0.021363\n",
      "Epoch : 215/2000, Train Loss : 10.883720, Valid Loss 13.001090, Error 0.020392\n",
      "Epoch : 216/2000, Train Loss : 11.196219, Valid Loss 13.777397, Error 0.022328\n",
      "Epoch : 217/2000, Train Loss : 10.523507, Valid Loss 13.140752, Error 0.021107\n",
      "Epoch : 218/2000, Train Loss : 10.892065, Valid Loss 12.742526, Error 0.020511\n",
      "Epoch : 219/2000, Train Loss : 10.833103, Valid Loss 15.854297, Error 0.023459\n",
      "Epoch : 220/2000, Train Loss : 12.844118, Valid Loss 14.138239, Error 0.021798\n",
      "Epoch : 221/2000, Train Loss : 10.867956, Valid Loss 13.381538, Error 0.021448\n",
      "Epoch : 222/2000, Train Loss : 11.086153, Valid Loss 13.406042, Error 0.020780\n",
      "Epoch : 223/2000, Train Loss : 11.139555, Valid Loss 14.816206, Error 0.022505\n",
      "Epoch : 224/2000, Train Loss : 11.897699, Valid Loss 14.936887, Error 0.022839\n",
      "Epoch : 225/2000, Train Loss : 10.752494, Valid Loss 13.438727, Error 0.020829\n",
      "Epoch : 226/2000, Train Loss : 11.689111, Valid Loss 13.546145, Error 0.021516\n",
      "Epoch : 227/2000, Train Loss : 11.400018, Valid Loss 16.656316, Error 0.023566\n",
      "Epoch : 228/2000, Train Loss : 10.980618, Valid Loss 12.781430, Error 0.020102\n",
      "Epoch : 229/2000, Train Loss : 10.839134, Valid Loss 15.163446, Error 0.022295\n",
      "Epoch : 230/2000, Train Loss : 10.304361, Valid Loss 13.107068, Error 0.021683\n",
      "Epoch : 231/2000, Train Loss : 10.139284, Valid Loss 12.405888, Error 0.020184\n",
      "Epoch : 232/2000, Train Loss : 10.863931, Valid Loss 17.271752, Error 0.025032\n",
      "Epoch : 233/2000, Train Loss : 10.501426, Valid Loss 12.885791, Error 0.021605\n",
      "Epoch : 234/2000, Train Loss : 9.628040, Valid Loss 12.482892, Error 0.021479\n",
      "Epoch : 235/2000, Train Loss : 10.088462, Valid Loss 12.503995, Error 0.020546\n",
      "Epoch : 236/2000, Train Loss : 10.041465, Valid Loss 14.146691, Error 0.022592\n",
      "Epoch : 237/2000, Train Loss : 9.843682, Valid Loss 13.224093, Error 0.020998\n",
      "Epoch : 238/2000, Train Loss : 10.125430, Valid Loss 12.803759, Error 0.020759\n",
      "Epoch : 239/2000, Train Loss : 10.212096, Valid Loss 12.371543, Error 0.020543\n",
      "Epoch : 240/2000, Train Loss : 10.104919, Valid Loss 12.187213, Error 0.020181\n",
      "Epoch : 241/2000, Train Loss : 9.701396, Valid Loss 12.371353, Error 0.019921\n",
      "Epoch : 242/2000, Train Loss : 10.706463, Valid Loss 14.295787, Error 0.021936\n",
      "Epoch : 243/2000, Train Loss : 10.050871, Valid Loss 12.873439, Error 0.021333\n",
      "Epoch : 244/2000, Train Loss : 10.679859, Valid Loss 13.852346, Error 0.022525\n",
      "Epoch : 245/2000, Train Loss : 11.173747, Valid Loss 12.446826, Error 0.020377\n",
      "Epoch : 246/2000, Train Loss : 11.591860, Valid Loss 12.528781, Error 0.020599\n",
      "Epoch : 247/2000, Train Loss : 9.969472, Valid Loss 12.006316, Error 0.020325\n",
      "Epoch : 248/2000, Train Loss : 9.525228, Valid Loss 12.064458, Error 0.020568\n",
      "Epoch : 249/2000, Train Loss : 9.184869, Valid Loss 12.520063, Error 0.020035\n",
      "Epoch : 250/2000, Train Loss : 10.007896, Valid Loss 14.132474, Error 0.022247\n",
      "Epoch : 251/2000, Train Loss : 10.243764, Valid Loss 12.618790, Error 0.020822\n",
      "Epoch : 252/2000, Train Loss : 9.916901, Valid Loss 13.698276, Error 0.021283\n",
      "Epoch : 253/2000, Train Loss : 9.988504, Valid Loss 12.924202, Error 0.020897\n",
      "Epoch : 254/2000, Train Loss : 9.506617, Valid Loss 12.379949, Error 0.020817\n",
      "Epoch : 255/2000, Train Loss : 9.876442, Valid Loss 11.966573, Error 0.019644\n",
      "Epoch : 256/2000, Train Loss : 10.472654, Valid Loss 12.077810, Error 0.019135\n",
      "Epoch : 257/2000, Train Loss : 10.387184, Valid Loss 15.147493, Error 0.023856\n",
      "Epoch : 258/2000, Train Loss : 11.000570, Valid Loss 12.452631, Error 0.020885\n",
      "Epoch : 259/2000, Train Loss : 10.389881, Valid Loss 15.455338, Error 0.023272\n",
      "Epoch : 260/2000, Train Loss : 10.384823, Valid Loss 13.142311, Error 0.021749\n",
      "Epoch : 261/2000, Train Loss : 9.503790, Valid Loss 12.576062, Error 0.020805\n",
      "Epoch : 262/2000, Train Loss : 10.341649, Valid Loss 14.639509, Error 0.022594\n",
      "Epoch : 263/2000, Train Loss : 9.749091, Valid Loss 11.872638, Error 0.020268\n",
      "Epoch : 264/2000, Train Loss : 9.031100, Valid Loss 12.699616, Error 0.020914\n",
      "Epoch : 265/2000, Train Loss : 9.392595, Valid Loss 11.761766, Error 0.019202\n",
      "Epoch : 266/2000, Train Loss : 9.502984, Valid Loss 12.162870, Error 0.021071\n",
      "Epoch : 267/2000, Train Loss : 9.012956, Valid Loss 11.513519, Error 0.020035\n",
      "Epoch : 268/2000, Train Loss : 8.989597, Valid Loss 12.449083, Error 0.020821\n",
      "Epoch : 269/2000, Train Loss : 9.525098, Valid Loss 13.118968, Error 0.022101\n",
      "Epoch : 270/2000, Train Loss : 10.215536, Valid Loss 11.613433, Error 0.019618\n",
      "Epoch : 271/2000, Train Loss : 9.009546, Valid Loss 12.028490, Error 0.019910\n",
      "Epoch : 272/2000, Train Loss : 9.566855, Valid Loss 12.352205, Error 0.020639\n",
      "Epoch : 273/2000, Train Loss : 10.990998, Valid Loss 19.157872, Error 0.026555\n",
      "Epoch : 274/2000, Train Loss : 11.282441, Valid Loss 15.883158, Error 0.023450\n",
      "Epoch : 275/2000, Train Loss : 9.432067, Valid Loss 13.715742, Error 0.021244\n",
      "Epoch : 276/2000, Train Loss : 9.880774, Valid Loss 13.300105, Error 0.021303\n",
      "Epoch : 277/2000, Train Loss : 9.663641, Valid Loss 12.336324, Error 0.020731\n",
      "Epoch : 278/2000, Train Loss : 9.153115, Valid Loss 12.389852, Error 0.020559\n",
      "Epoch : 279/2000, Train Loss : 9.236832, Valid Loss 12.339843, Error 0.020622\n",
      "Epoch : 280/2000, Train Loss : 8.753091, Valid Loss 12.258338, Error 0.020418\n",
      "Epoch : 281/2000, Train Loss : 9.423499, Valid Loss 14.326628, Error 0.021615\n",
      "Epoch : 282/2000, Train Loss : 9.573631, Valid Loss 15.826229, Error 0.022981\n",
      "Epoch : 283/2000, Train Loss : 9.095230, Valid Loss 11.737444, Error 0.019347\n",
      "Epoch : 284/2000, Train Loss : 9.014971, Valid Loss 11.449366, Error 0.019821\n",
      "Epoch : 285/2000, Train Loss : 8.500749, Valid Loss 12.016580, Error 0.019477\n",
      "Epoch : 286/2000, Train Loss : 8.403595, Valid Loss 12.777356, Error 0.020701\n",
      "Epoch : 287/2000, Train Loss : 8.597865, Valid Loss 13.470273, Error 0.021359\n",
      "Epoch : 288/2000, Train Loss : 8.863307, Valid Loss 12.551867, Error 0.021033\n",
      "Epoch : 289/2000, Train Loss : 8.367262, Valid Loss 11.549209, Error 0.019403\n",
      "Epoch : 290/2000, Train Loss : 8.738129, Valid Loss 11.471714, Error 0.019366\n",
      "Epoch : 291/2000, Train Loss : 8.720853, Valid Loss 11.897545, Error 0.020494\n",
      "Epoch : 292/2000, Train Loss : 8.930366, Valid Loss 13.205225, Error 0.020582\n",
      "Epoch : 293/2000, Train Loss : 8.535187, Valid Loss 12.058057, Error 0.020167\n",
      "Epoch : 294/2000, Train Loss : 8.276054, Valid Loss 11.468551, Error 0.019412\n",
      "Epoch : 295/2000, Train Loss : 8.342562, Valid Loss 11.471847, Error 0.020175\n",
      "Epoch : 296/2000, Train Loss : 9.078309, Valid Loss 13.490878, Error 0.021913\n",
      "Epoch : 297/2000, Train Loss : 9.976617, Valid Loss 11.967025, Error 0.020218\n",
      "Epoch : 298/2000, Train Loss : 8.614910, Valid Loss 10.993048, Error 0.019183\n",
      "Epoch : 299/2000, Train Loss : 8.365560, Valid Loss 12.375111, Error 0.020031\n",
      "Epoch : 300/2000, Train Loss : 8.441839, Valid Loss 11.472815, Error 0.019783\n",
      "Epoch : 301/2000, Train Loss : 8.563362, Valid Loss 17.521033, Error 0.026243\n",
      "Epoch : 302/2000, Train Loss : 9.147151, Valid Loss 11.390021, Error 0.019616\n",
      "Epoch : 303/2000, Train Loss : 8.401083, Valid Loss 12.592488, Error 0.020229\n",
      "Epoch : 304/2000, Train Loss : 8.772740, Valid Loss 11.119659, Error 0.018159\n",
      "Epoch : 305/2000, Train Loss : 10.757817, Valid Loss 17.566758, Error 0.025571\n",
      "Epoch : 306/2000, Train Loss : 12.596341, Valid Loss 15.632336, Error 0.023229\n",
      "Epoch : 307/2000, Train Loss : 11.240604, Valid Loss 14.419315, Error 0.022378\n",
      "Epoch : 308/2000, Train Loss : 8.916700, Valid Loss 11.935783, Error 0.020066\n",
      "Epoch : 309/2000, Train Loss : 8.199076, Valid Loss 11.655985, Error 0.020272\n",
      "Epoch : 310/2000, Train Loss : 8.152240, Valid Loss 11.392888, Error 0.019288\n",
      "Epoch : 311/2000, Train Loss : 8.262908, Valid Loss 14.722477, Error 0.023287\n",
      "Epoch : 312/2000, Train Loss : 8.744720, Valid Loss 11.091289, Error 0.018377\n",
      "Epoch : 313/2000, Train Loss : 7.813502, Valid Loss 11.865423, Error 0.019687\n",
      "Epoch : 314/2000, Train Loss : 8.152817, Valid Loss 13.372526, Error 0.021720\n",
      "Epoch : 315/2000, Train Loss : 8.305641, Valid Loss 13.139332, Error 0.020713\n",
      "Epoch : 316/2000, Train Loss : 8.238806, Valid Loss 11.613570, Error 0.020017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 317/2000, Train Loss : 8.077453, Valid Loss 11.687476, Error 0.019892\n",
      "Epoch : 318/2000, Train Loss : 7.667007, Valid Loss 11.555269, Error 0.019508\n",
      "Epoch : 319/2000, Train Loss : 7.871622, Valid Loss 11.869888, Error 0.019518\n",
      "Early stopping\n",
      "Total Error Mean 0.042225\n"
     ]
    }
   ],
   "source": [
    "# Set fixed random number seed\n",
    "torch.manual_seed(7777)\n",
    "\n",
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "total_error = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model_atn.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, targets = data\n",
    "        inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "        inputs_acc = inputs_acc.to(device)\n",
    "        inputs_gyr = inputs_gyr.to(device)\n",
    "        targets = targets.reshape(-1, 1)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = loss_function(outputs, targets)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model_atn.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, targets = data\n",
    "            inputs_acc, inputs_gyr, targets = inputs_acc.float(), inputs_gyr.float(), targets.float()\n",
    "            inputs_acc = inputs_acc.to(device)\n",
    "            inputs_gyr = inputs_gyr.to(device)\n",
    "            targets = targets.reshape(-1, 1)\n",
    "            targets = targets.to(device)\n",
    "                        \n",
    "            # 순전파 \n",
    "            outputs = model_atn(inputs_acc, inputs_gyr)\n",
    "            \n",
    "            # Batch 별 Loss 계산\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    error = torch.sum(torch.abs(outputs - targets) / targets) / len(targets)\n",
    "    total_error += error\n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, Error {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      error))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Total Error Mean {:4f}'.format(total_error/(epoch+1)))\n",
    "        break\n",
    "\n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7621716",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85211f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86606fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# LSTM Module\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(LSTM_atn, self).__init__() # 상속한 nn.Module에서 RNN에 해당하는 init 실행\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.cnn_acc = nn.Conv2d(in_channels=3, hidden_size, num_layers, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        self.reg_module1 = nn.Sequential(\n",
    "            nn.Linear(hidden_size*2, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "    \n",
    "    def attention(self, lstm_output, final_state):\n",
    "#         merged_state = torch.cat([s for s in final_state], 1)\n",
    "        merged_state = final_state.squeeze(0).unsqueeze(2)\n",
    "        weights = torch.bmm(lstm_output, merged_state)\n",
    "        weights = F.softmax(weights.squeeze(2), dim=1).unsqueeze(2)\n",
    "        return torch.bmm(torch.transpose(lstm_output, 1, 2), weights).squeeze(2)\n",
    "\n",
    "    def forward(self, acc, gyr): \n",
    "        \n",
    "        # 다음 학습에 영향을 주지 않기 위해 초기 h_0과 c_0 초기화\n",
    "        h0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, acc.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        o_acc, (h_acc, _) = self.lstm_acc(acc, (h0, c0))\n",
    "        o_gyr, (h_gyr, _) = self.lstm_gyr(gyr, (h0, c0))\n",
    "        \n",
    "        h_concat = torch.cat((h_acc.view(-1, hidden_size), h_gyr.view(-1, hidden_size)), dim=1)\n",
    "        o_concat = torch.cat((o_acc, o_gyr), dim=2)\n",
    "        \n",
    "        attn_outputs = self.attention(o_concat, h_concat)\n",
    "    \n",
    "        out_lstm = self.reg_module1(attn_outputs)\n",
    "        \n",
    "        return out_lstm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait",
   "language": "python",
   "name": "gait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
