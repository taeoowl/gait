{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64447c74",
   "metadata": {},
   "source": [
    "# 모듈 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dc5bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import scipy\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os \n",
    "import glob\n",
    "import cv2\n",
    "import itertools\n",
    "\n",
    "from dataloader_distance import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36b8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78ed85",
   "metadata": {},
   "source": [
    "# Linear Regression : Only Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c5d586",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8172050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "_, _, stride_length = get_sensor_salted(file_path)\n",
    "inputs_pst = get_position_salted(file_path, distance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da8dfdd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R_DIS_X</th>\n",
       "      <th>R_DIS_Y</th>\n",
       "      <th>R_DIS_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.325056</td>\n",
       "      <td>0.295638</td>\n",
       "      <td>0.591209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408169</td>\n",
       "      <td>0.226372</td>\n",
       "      <td>0.594593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.385037</td>\n",
       "      <td>0.185862</td>\n",
       "      <td>0.569153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.432047</td>\n",
       "      <td>0.206133</td>\n",
       "      <td>0.605114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.483833</td>\n",
       "      <td>0.175300</td>\n",
       "      <td>0.620395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>0.635216</td>\n",
       "      <td>0.341886</td>\n",
       "      <td>0.721252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>0.541239</td>\n",
       "      <td>0.295841</td>\n",
       "      <td>0.731596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3781</th>\n",
       "      <td>0.654111</td>\n",
       "      <td>0.232625</td>\n",
       "      <td>0.748407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3782</th>\n",
       "      <td>0.682866</td>\n",
       "      <td>0.343563</td>\n",
       "      <td>0.722284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3783</th>\n",
       "      <td>0.637258</td>\n",
       "      <td>0.336259</td>\n",
       "      <td>0.723490</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3784 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       R_DIS_X   R_DIS_Y   R_DIS_Z\n",
       "0     0.325056  0.295638  0.591209\n",
       "1     0.408169  0.226372  0.594593\n",
       "2     0.385037  0.185862  0.569153\n",
       "3     0.432047  0.206133  0.605114\n",
       "4     0.483833  0.175300  0.620395\n",
       "...        ...       ...       ...\n",
       "3779  0.635216  0.341886  0.721252\n",
       "3780  0.541239  0.295841  0.731596\n",
       "3781  0.654111  0.232625  0.748407\n",
       "3782  0.682866  0.343563  0.722284\n",
       "3783  0.637258  0.336259  0.723490\n",
       "\n",
       "[3784 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame(inputs_pst)\n",
    "train.columns =  ['R_DIS_X', 'R_DIS_Y', 'R_DIS_Z']\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5697c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train, stride_length, train_size=0.8, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b926f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "061def29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.003078276181142"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d174a3",
   "metadata": {},
   "source": [
    "# Encoder-based Model : Acc, Gyro 각각 입력, 최종 노드 1개\n",
    "- Encoder로 Conv1d와 LSTM을 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc0a015",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57327b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "dataset = Gait_Dataset_Salted(file_path)\n",
    "val_percent = 0.2\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e59538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True,\n",
    "                                           worker_init_fn=np.random.seed(42))\n",
    "val_loader = torch.utils.data.DataLoader(val,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae6f3a",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "345e58a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        \n",
    "        self.conv1d_acc = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim1, 30),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(hidden_dim1, hidden_dim2, 30),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.conv1d_gyr = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim1, 30),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(hidden_dim1, hidden_dim2, 30),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(hidden_dim2, 64, 1, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(hidden_dim2, 64, 1, batch_first=True)\n",
    "            \n",
    "#         self.encoder_pres = nn.Sequential(\n",
    "#             nn.Conv1d(input_dim, hidden_dim1, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv1d(hidden_dim1, hidden_dim2, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv1d(hidden_dim2, hidden_dim3, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Flatten()\n",
    "#         )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(64*2, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, inputs_acc, inputs_gyr): \n",
    "        \n",
    "        h0 = torch.zeros(1, inputs_acc.size(0), 64).to(device)\n",
    "        c0 = torch.zeros(1, inputs_acc.size(0), 64).to(device)\n",
    "        \n",
    "        conv1d_output_acc = self.conv1d_acc(inputs_acc).transpose(1, 2)\n",
    "        conv1d_output_gyr = self.conv1d_gyr(inputs_gyr).transpose(1, 2)\n",
    "        \n",
    "        _, (enc_output_acc, _) = self.lstm_acc(conv1d_output_acc)\n",
    "        _, (enc_output_gyr, _) = self.lstm_gyr(conv1d_output_gyr)\n",
    "        \n",
    "        enc_output = torch.concat((enc_output_acc[-1], enc_output_gyr[-1]), 1)\n",
    "        dense_output = self.dense(enc_output)\n",
    "        \n",
    "        return dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3332e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "model = Encoder(input_dim, 16, 32).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "n_epochs = 2000\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8adb5373",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 17542.586100, Valid Loss 17085.446289, MAE 131.177490\n",
      "Epoch : 2/2000, Train Loss : 15620.851278, Valid Loss 13621.143717, MAE 117.035294\n",
      "Epoch : 3/2000, Train Loss : 10575.935445, Valid Loss 6959.515788, MAE 83.219910\n",
      "Epoch : 4/2000, Train Loss : 3717.038096, Valid Loss 1065.681305, MAE 29.732834\n",
      "Epoch : 5/2000, Train Loss : 464.796411, Valid Loss 334.866206, MAE 12.965923\n",
      "Epoch : 6/2000, Train Loss : 323.879810, Valid Loss 302.285436, MAE 13.297342\n",
      "Epoch : 7/2000, Train Loss : 304.477130, Valid Loss 303.095825, MAE 13.463957\n",
      "Epoch : 8/2000, Train Loss : 305.403108, Valid Loss 302.250753, MAE 13.354850\n",
      "Epoch : 9/2000, Train Loss : 304.198984, Valid Loss 302.005809, MAE 13.335560\n",
      "Epoch : 10/2000, Train Loss : 305.020031, Valid Loss 302.501658, MAE 13.402302\n",
      "Epoch : 11/2000, Train Loss : 303.952150, Valid Loss 301.922536, MAE 13.369467\n",
      "Epoch : 12/2000, Train Loss : 304.337308, Valid Loss 297.912824, MAE 13.345143\n",
      "Epoch : 13/2000, Train Loss : 254.788677, Valid Loss 224.164065, MAE 10.806504\n",
      "Epoch : 14/2000, Train Loss : 225.139645, Valid Loss 234.052577, MAE 11.303823\n",
      "Epoch : 15/2000, Train Loss : 223.858840, Valid Loss 220.923848, MAE 10.957229\n",
      "Epoch : 16/2000, Train Loss : 217.937737, Valid Loss 213.534047, MAE 10.556479\n",
      "Epoch : 17/2000, Train Loss : 215.073126, Valid Loss 211.931089, MAE 10.537365\n",
      "Epoch : 18/2000, Train Loss : 214.942631, Valid Loss 224.003794, MAE 10.820782\n",
      "Epoch : 19/2000, Train Loss : 212.285354, Valid Loss 225.374952, MAE 11.293670\n",
      "Epoch : 20/2000, Train Loss : 210.737783, Valid Loss 209.040329, MAE 10.557554\n",
      "Epoch : 21/2000, Train Loss : 211.892588, Valid Loss 207.727936, MAE 10.383580\n",
      "Epoch : 22/2000, Train Loss : 205.492778, Valid Loss 204.877780, MAE 10.303931\n",
      "Epoch : 23/2000, Train Loss : 205.223609, Valid Loss 207.519157, MAE 10.326397\n",
      "Epoch : 24/2000, Train Loss : 201.883535, Valid Loss 202.091197, MAE 10.388634\n",
      "Epoch : 25/2000, Train Loss : 202.304056, Valid Loss 202.729757, MAE 10.500301\n",
      "Epoch : 26/2000, Train Loss : 201.066381, Valid Loss 200.367577, MAE 10.288453\n",
      "Epoch : 27/2000, Train Loss : 197.691367, Valid Loss 200.984873, MAE 10.451628\n",
      "Epoch : 28/2000, Train Loss : 197.482255, Valid Loss 194.568748, MAE 10.105185\n",
      "Epoch : 29/2000, Train Loss : 202.116386, Valid Loss 197.422221, MAE 10.347012\n",
      "Epoch : 30/2000, Train Loss : 200.843267, Valid Loss 201.131917, MAE 10.148706\n",
      "Epoch : 31/2000, Train Loss : 195.954765, Valid Loss 198.690526, MAE 10.475407\n",
      "Epoch : 32/2000, Train Loss : 189.769615, Valid Loss 199.498009, MAE 10.463187\n",
      "Epoch : 33/2000, Train Loss : 190.711830, Valid Loss 194.236186, MAE 10.322626\n",
      "Epoch : 34/2000, Train Loss : 193.899916, Valid Loss 188.121841, MAE 10.167341\n",
      "Epoch : 35/2000, Train Loss : 187.290109, Valid Loss 182.297625, MAE 9.723524\n",
      "Epoch : 36/2000, Train Loss : 185.218130, Valid Loss 178.526606, MAE 9.530189\n",
      "Epoch : 37/2000, Train Loss : 177.095348, Valid Loss 169.606593, MAE 9.350251\n",
      "Epoch : 38/2000, Train Loss : 169.187386, Valid Loss 166.564034, MAE 9.502401\n",
      "Epoch : 39/2000, Train Loss : 175.259898, Valid Loss 220.514064, MAE 11.444790\n",
      "Epoch : 40/2000, Train Loss : 188.656733, Valid Loss 212.566363, MAE 11.632906\n",
      "Epoch : 41/2000, Train Loss : 164.476301, Valid Loss 148.815379, MAE 9.204929\n",
      "Epoch : 42/2000, Train Loss : 127.225374, Valid Loss 120.413751, MAE 7.722648\n",
      "Epoch : 43/2000, Train Loss : 113.160714, Valid Loss 106.945000, MAE 7.085697\n",
      "Epoch : 44/2000, Train Loss : 103.487055, Valid Loss 98.562651, MAE 6.826834\n",
      "Epoch : 45/2000, Train Loss : 95.216160, Valid Loss 94.329117, MAE 6.754093\n",
      "Epoch : 46/2000, Train Loss : 87.978639, Valid Loss 87.177535, MAE 6.219422\n",
      "Epoch : 47/2000, Train Loss : 85.025366, Valid Loss 90.943250, MAE 6.858340\n",
      "Epoch : 48/2000, Train Loss : 79.635631, Valid Loss 80.327034, MAE 6.078061\n",
      "Epoch : 49/2000, Train Loss : 78.711156, Valid Loss 74.428560, MAE 5.848332\n",
      "Epoch : 50/2000, Train Loss : 76.741021, Valid Loss 72.859649, MAE 5.894567\n",
      "Epoch : 51/2000, Train Loss : 69.905082, Valid Loss 73.411475, MAE 5.961823\n",
      "Epoch : 52/2000, Train Loss : 67.765475, Valid Loss 75.154797, MAE 5.673977\n",
      "Epoch : 53/2000, Train Loss : 69.566431, Valid Loss 67.653418, MAE 5.495910\n",
      "Epoch : 54/2000, Train Loss : 61.366218, Valid Loss 63.616901, MAE 5.446703\n",
      "Epoch : 55/2000, Train Loss : 59.755478, Valid Loss 56.459561, MAE 4.888927\n",
      "Epoch : 56/2000, Train Loss : 58.684820, Valid Loss 65.784220, MAE 5.553102\n",
      "Epoch : 57/2000, Train Loss : 60.763505, Valid Loss 68.978921, MAE 6.019285\n",
      "Epoch : 58/2000, Train Loss : 58.785580, Valid Loss 53.412299, MAE 5.040851\n",
      "Epoch : 59/2000, Train Loss : 52.208637, Valid Loss 49.525358, MAE 4.587542\n",
      "Epoch : 60/2000, Train Loss : 49.583485, Valid Loss 47.778453, MAE 4.943063\n",
      "Epoch : 61/2000, Train Loss : 47.523212, Valid Loss 47.432546, MAE 4.949614\n",
      "Epoch : 62/2000, Train Loss : 46.173124, Valid Loss 59.385295, MAE 4.729955\n",
      "Epoch : 63/2000, Train Loss : 49.095714, Valid Loss 50.229688, MAE 5.035975\n",
      "Epoch : 64/2000, Train Loss : 44.941780, Valid Loss 43.725151, MAE 4.263659\n",
      "Epoch : 65/2000, Train Loss : 43.870809, Valid Loss 54.890994, MAE 5.050108\n",
      "Epoch : 66/2000, Train Loss : 41.114324, Valid Loss 41.543308, MAE 4.143855\n",
      "Epoch : 67/2000, Train Loss : 42.728490, Valid Loss 59.923348, MAE 5.204696\n",
      "Epoch : 68/2000, Train Loss : 44.663995, Valid Loss 40.718747, MAE 4.283062\n",
      "Epoch : 69/2000, Train Loss : 41.449672, Valid Loss 43.439952, MAE 4.215082\n",
      "Epoch : 70/2000, Train Loss : 46.963574, Valid Loss 45.520367, MAE 4.289887\n",
      "Epoch : 71/2000, Train Loss : 41.102894, Valid Loss 52.690259, MAE 5.544240\n",
      "Epoch : 72/2000, Train Loss : 43.434740, Valid Loss 40.526206, MAE 4.492657\n",
      "Epoch : 73/2000, Train Loss : 46.095354, Valid Loss 63.620232, MAE 5.577344\n",
      "Epoch : 74/2000, Train Loss : 48.860784, Valid Loss 43.639093, MAE 4.438492\n",
      "Epoch : 75/2000, Train Loss : 39.142135, Valid Loss 43.217429, MAE 4.287258\n",
      "Epoch : 76/2000, Train Loss : 35.686236, Valid Loss 54.376036, MAE 5.380644\n",
      "Epoch : 77/2000, Train Loss : 37.757283, Valid Loss 39.398012, MAE 4.185296\n",
      "Epoch : 78/2000, Train Loss : 34.954322, Valid Loss 42.815576, MAE 4.251574\n",
      "Epoch : 79/2000, Train Loss : 39.815548, Valid Loss 36.313518, MAE 4.209321\n",
      "Epoch : 80/2000, Train Loss : 37.416725, Valid Loss 42.546450, MAE 4.246463\n",
      "Epoch : 81/2000, Train Loss : 35.351556, Valid Loss 34.631503, MAE 3.922070\n",
      "Epoch : 82/2000, Train Loss : 35.381634, Valid Loss 36.773424, MAE 4.083994\n",
      "Epoch : 83/2000, Train Loss : 33.641670, Valid Loss 38.189706, MAE 3.980580\n",
      "Epoch : 84/2000, Train Loss : 36.792993, Valid Loss 48.140996, MAE 4.444336\n",
      "Epoch : 85/2000, Train Loss : 36.481676, Valid Loss 34.792598, MAE 3.869477\n",
      "Epoch : 86/2000, Train Loss : 33.316592, Valid Loss 33.910067, MAE 4.109698\n",
      "Epoch : 87/2000, Train Loss : 32.859723, Valid Loss 33.150638, MAE 3.903947\n",
      "Epoch : 88/2000, Train Loss : 30.879272, Valid Loss 33.556578, MAE 3.742645\n",
      "Epoch : 89/2000, Train Loss : 30.930997, Valid Loss 34.973034, MAE 3.872277\n",
      "Epoch : 90/2000, Train Loss : 31.934988, Valid Loss 38.419169, MAE 4.275887\n",
      "Epoch : 91/2000, Train Loss : 34.644739, Valid Loss 62.513343, MAE 6.219557\n",
      "Epoch : 92/2000, Train Loss : 46.047463, Valid Loss 44.856993, MAE 4.506760\n",
      "Epoch : 93/2000, Train Loss : 36.408206, Valid Loss 35.729899, MAE 4.120203\n",
      "Epoch : 94/2000, Train Loss : 30.999719, Valid Loss 35.820488, MAE 4.042677\n",
      "Epoch : 95/2000, Train Loss : 29.741655, Valid Loss 33.908506, MAE 3.749741\n",
      "Epoch : 96/2000, Train Loss : 31.183233, Valid Loss 45.869713, MAE 4.470015\n",
      "Epoch : 97/2000, Train Loss : 40.528432, Valid Loss 46.142170, MAE 4.313745\n",
      "Epoch : 98/2000, Train Loss : 33.599027, Valid Loss 35.112542, MAE 3.870750\n",
      "Epoch : 99/2000, Train Loss : 27.921679, Valid Loss 30.768493, MAE 3.623386\n",
      "Epoch : 100/2000, Train Loss : 29.130772, Valid Loss 34.179611, MAE 4.072523\n",
      "Epoch : 101/2000, Train Loss : 33.409696, Valid Loss 37.827735, MAE 4.032608\n",
      "Epoch : 102/2000, Train Loss : 33.717655, Valid Loss 32.162550, MAE 3.747040\n",
      "Epoch : 103/2000, Train Loss : 28.401162, Valid Loss 31.230216, MAE 3.656177\n",
      "Epoch : 104/2000, Train Loss : 28.073810, Valid Loss 34.648033, MAE 4.209841\n",
      "Epoch : 105/2000, Train Loss : 30.511911, Valid Loss 35.323609, MAE 4.429790\n",
      "Epoch : 106/2000, Train Loss : 29.200855, Valid Loss 30.190612, MAE 3.735413\n",
      "Epoch : 107/2000, Train Loss : 30.306302, Valid Loss 35.178092, MAE 4.556936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 108/2000, Train Loss : 28.176003, Valid Loss 30.384656, MAE 3.932522\n",
      "Epoch : 109/2000, Train Loss : 27.686847, Valid Loss 32.099671, MAE 4.001892\n",
      "Epoch : 110/2000, Train Loss : 27.285361, Valid Loss 29.457425, MAE 3.655983\n",
      "Epoch : 111/2000, Train Loss : 29.069866, Valid Loss 34.113635, MAE 3.691536\n",
      "Epoch : 112/2000, Train Loss : 30.864595, Valid Loss 35.534314, MAE 4.523419\n",
      "Epoch : 113/2000, Train Loss : 27.641877, Valid Loss 30.323187, MAE 3.777603\n",
      "Epoch : 114/2000, Train Loss : 26.488358, Valid Loss 30.438152, MAE 3.904057\n",
      "Epoch : 115/2000, Train Loss : 25.449046, Valid Loss 31.213621, MAE 3.561618\n",
      "Epoch : 116/2000, Train Loss : 27.563608, Valid Loss 29.465072, MAE 3.847836\n",
      "Epoch : 117/2000, Train Loss : 26.569814, Valid Loss 29.272609, MAE 3.770973\n",
      "Epoch : 118/2000, Train Loss : 25.994125, Valid Loss 29.692456, MAE 3.477934\n",
      "Epoch : 119/2000, Train Loss : 28.413421, Valid Loss 57.565406, MAE 5.120290\n",
      "Epoch : 120/2000, Train Loss : 30.506082, Valid Loss 32.979669, MAE 3.990085\n",
      "Epoch : 121/2000, Train Loss : 31.832319, Valid Loss 31.303982, MAE 3.844591\n",
      "Epoch : 122/2000, Train Loss : 25.812153, Valid Loss 29.155258, MAE 3.326807\n",
      "Epoch : 123/2000, Train Loss : 24.145848, Valid Loss 33.444019, MAE 4.251318\n",
      "Epoch : 124/2000, Train Loss : 24.904609, Valid Loss 28.836221, MAE 3.768106\n",
      "Epoch : 125/2000, Train Loss : 26.877017, Valid Loss 32.362015, MAE 4.045040\n",
      "Epoch : 126/2000, Train Loss : 25.782222, Valid Loss 29.430180, MAE 3.959180\n",
      "Epoch : 127/2000, Train Loss : 22.224275, Valid Loss 28.288928, MAE 3.472080\n",
      "Epoch : 128/2000, Train Loss : 22.402847, Valid Loss 27.965451, MAE 3.323848\n",
      "Epoch : 129/2000, Train Loss : 25.484901, Valid Loss 29.355385, MAE 3.498043\n",
      "Epoch : 130/2000, Train Loss : 24.471528, Valid Loss 29.141683, MAE 3.347532\n",
      "Epoch : 131/2000, Train Loss : 24.115247, Valid Loss 29.032302, MAE 3.638069\n",
      "Epoch : 132/2000, Train Loss : 24.519348, Valid Loss 25.717525, MAE 3.652877\n",
      "Epoch : 133/2000, Train Loss : 24.835440, Valid Loss 32.502888, MAE 4.003405\n",
      "Epoch : 134/2000, Train Loss : 23.259755, Valid Loss 25.425946, MAE 3.083384\n",
      "Epoch : 135/2000, Train Loss : 21.929881, Valid Loss 26.917047, MAE 3.269838\n",
      "Epoch : 136/2000, Train Loss : 20.555186, Valid Loss 26.967997, MAE 3.823910\n",
      "Epoch : 137/2000, Train Loss : 24.928410, Valid Loss 27.666000, MAE 3.574283\n",
      "Epoch : 138/2000, Train Loss : 26.244171, Valid Loss 33.060257, MAE 3.487145\n",
      "Epoch : 139/2000, Train Loss : 24.650698, Valid Loss 25.352897, MAE 3.205453\n",
      "Epoch : 140/2000, Train Loss : 22.245431, Valid Loss 24.811065, MAE 3.258605\n",
      "Epoch : 141/2000, Train Loss : 22.742101, Valid Loss 28.744663, MAE 4.022064\n",
      "Epoch : 142/2000, Train Loss : 22.017261, Valid Loss 27.674594, MAE 3.339214\n",
      "Epoch : 143/2000, Train Loss : 21.361926, Valid Loss 26.963906, MAE 3.486304\n",
      "Epoch : 144/2000, Train Loss : 21.042644, Valid Loss 24.112716, MAE 3.250033\n",
      "Epoch : 145/2000, Train Loss : 20.249090, Valid Loss 25.054209, MAE 3.216608\n",
      "Epoch : 146/2000, Train Loss : 18.792703, Valid Loss 24.171002, MAE 3.149111\n",
      "Epoch : 147/2000, Train Loss : 19.335632, Valid Loss 23.355274, MAE 3.298450\n",
      "Epoch : 148/2000, Train Loss : 22.971499, Valid Loss 33.721903, MAE 3.872453\n",
      "Epoch : 149/2000, Train Loss : 23.938074, Valid Loss 24.763876, MAE 2.993434\n",
      "Epoch : 150/2000, Train Loss : 20.447191, Valid Loss 24.770709, MAE 3.466894\n",
      "Epoch : 151/2000, Train Loss : 19.774891, Valid Loss 21.996984, MAE 3.259393\n",
      "Epoch : 152/2000, Train Loss : 18.638523, Valid Loss 22.940820, MAE 3.173025\n",
      "Epoch : 153/2000, Train Loss : 19.866104, Valid Loss 23.089501, MAE 3.165786\n",
      "Epoch : 154/2000, Train Loss : 19.962978, Valid Loss 25.864658, MAE 3.267501\n",
      "Epoch : 155/2000, Train Loss : 22.348275, Valid Loss 28.816379, MAE 4.008956\n",
      "Epoch : 156/2000, Train Loss : 21.308334, Valid Loss 26.992074, MAE 3.532127\n",
      "Epoch : 157/2000, Train Loss : 34.453945, Valid Loss 54.703484, MAE 5.884500\n",
      "Epoch : 158/2000, Train Loss : 39.805631, Valid Loss 32.355595, MAE 3.923521\n",
      "Epoch : 159/2000, Train Loss : 28.674523, Valid Loss 26.989183, MAE 3.303125\n",
      "Epoch : 160/2000, Train Loss : 21.072504, Valid Loss 23.964320, MAE 3.536579\n",
      "Epoch : 161/2000, Train Loss : 20.173241, Valid Loss 24.959122, MAE 3.190603\n",
      "Epoch : 162/2000, Train Loss : 24.395597, Valid Loss 37.605689, MAE 4.199635\n",
      "Epoch : 163/2000, Train Loss : 23.614411, Valid Loss 23.691819, MAE 3.253477\n",
      "Epoch : 164/2000, Train Loss : 19.821974, Valid Loss 24.949559, MAE 3.292867\n",
      "Epoch : 165/2000, Train Loss : 20.026330, Valid Loss 22.788611, MAE 3.344780\n",
      "Epoch : 166/2000, Train Loss : 17.543111, Valid Loss 22.519892, MAE 3.078989\n",
      "Epoch : 167/2000, Train Loss : 17.608366, Valid Loss 25.575816, MAE 3.563805\n",
      "Epoch : 168/2000, Train Loss : 16.950504, Valid Loss 21.540054, MAE 3.043972\n",
      "Epoch : 169/2000, Train Loss : 16.902210, Valid Loss 21.164007, MAE 3.064615\n",
      "Epoch : 170/2000, Train Loss : 16.888492, Valid Loss 21.048208, MAE 2.946230\n",
      "Epoch : 171/2000, Train Loss : 17.157439, Valid Loss 20.858177, MAE 3.107908\n",
      "Epoch : 172/2000, Train Loss : 17.630451, Valid Loss 21.272207, MAE 3.398714\n",
      "Epoch : 173/2000, Train Loss : 17.790608, Valid Loss 22.965703, MAE 3.289822\n",
      "Epoch : 174/2000, Train Loss : 18.572968, Valid Loss 22.725266, MAE 3.039196\n",
      "Epoch : 175/2000, Train Loss : 21.834520, Valid Loss 24.029765, MAE 3.355154\n",
      "Epoch : 176/2000, Train Loss : 17.565637, Valid Loss 22.025819, MAE 3.171712\n",
      "Epoch : 177/2000, Train Loss : 17.212074, Valid Loss 19.455841, MAE 2.967838\n",
      "Epoch : 178/2000, Train Loss : 16.440079, Valid Loss 22.059928, MAE 3.376256\n",
      "Epoch : 179/2000, Train Loss : 17.222768, Valid Loss 19.372377, MAE 3.031901\n",
      "Epoch : 180/2000, Train Loss : 16.031679, Valid Loss 22.156518, MAE 3.242980\n",
      "Epoch : 181/2000, Train Loss : 17.172451, Valid Loss 21.373151, MAE 3.082657\n",
      "Epoch : 182/2000, Train Loss : 18.497923, Valid Loss 22.628676, MAE 3.184240\n",
      "Epoch : 183/2000, Train Loss : 16.366695, Valid Loss 20.549711, MAE 2.990251\n",
      "Epoch : 184/2000, Train Loss : 18.550336, Valid Loss 21.805856, MAE 3.079215\n",
      "Epoch : 185/2000, Train Loss : 17.415288, Valid Loss 28.563914, MAE 3.219274\n",
      "Epoch : 186/2000, Train Loss : 19.052450, Valid Loss 24.998534, MAE 3.219781\n",
      "Epoch : 187/2000, Train Loss : 19.312803, Valid Loss 20.069393, MAE 2.880456\n",
      "Epoch : 188/2000, Train Loss : 18.100383, Valid Loss 20.275357, MAE 2.961086\n",
      "Epoch : 189/2000, Train Loss : 15.684182, Valid Loss 20.870313, MAE 2.803319\n",
      "Epoch : 190/2000, Train Loss : 15.621270, Valid Loss 21.979949, MAE 2.945953\n",
      "Epoch : 191/2000, Train Loss : 18.303311, Valid Loss 25.675461, MAE 3.684980\n",
      "Epoch : 192/2000, Train Loss : 25.265199, Valid Loss 30.674074, MAE 3.632930\n",
      "Epoch : 193/2000, Train Loss : 23.153399, Valid Loss 24.068075, MAE 3.159354\n",
      "Epoch : 194/2000, Train Loss : 20.532598, Valid Loss 21.575334, MAE 3.000057\n",
      "Epoch : 195/2000, Train Loss : 17.415437, Valid Loss 18.705335, MAE 2.906697\n",
      "Epoch : 196/2000, Train Loss : 16.213180, Valid Loss 18.450675, MAE 2.781310\n",
      "Epoch : 197/2000, Train Loss : 16.711329, Valid Loss 20.456156, MAE 3.099627\n",
      "Epoch : 198/2000, Train Loss : 15.532245, Valid Loss 18.995797, MAE 2.767292\n",
      "Epoch : 199/2000, Train Loss : 15.675034, Valid Loss 19.570319, MAE 3.220719\n",
      "Epoch : 200/2000, Train Loss : 14.993647, Valid Loss 21.673659, MAE 2.760683\n",
      "Epoch : 201/2000, Train Loss : 15.469381, Valid Loss 17.759807, MAE 2.693929\n",
      "Epoch : 202/2000, Train Loss : 14.448987, Valid Loss 21.237980, MAE 3.236976\n",
      "Epoch : 203/2000, Train Loss : 15.817790, Valid Loss 17.918694, MAE 2.820925\n",
      "Epoch : 204/2000, Train Loss : 18.790030, Valid Loss 29.800402, MAE 3.531274\n",
      "Epoch : 205/2000, Train Loss : 17.206399, Valid Loss 20.088022, MAE 2.842587\n",
      "Epoch : 206/2000, Train Loss : 16.799767, Valid Loss 23.361841, MAE 3.378045\n",
      "Epoch : 207/2000, Train Loss : 23.270841, Valid Loss 32.046080, MAE 3.637658\n",
      "Epoch : 208/2000, Train Loss : 27.074914, Valid Loss 22.261370, MAE 3.324697\n",
      "Epoch : 209/2000, Train Loss : 22.460098, Valid Loss 25.508082, MAE 3.343205\n",
      "Epoch : 210/2000, Train Loss : 16.677787, Valid Loss 20.556561, MAE 3.000180\n",
      "Epoch : 211/2000, Train Loss : 14.279862, Valid Loss 19.725067, MAE 2.910180\n",
      "Epoch : 212/2000, Train Loss : 13.511756, Valid Loss 18.695518, MAE 2.907896\n",
      "Epoch : 213/2000, Train Loss : 13.871406, Valid Loss 18.960175, MAE 3.028370\n",
      "Epoch : 214/2000, Train Loss : 15.912693, Valid Loss 32.093579, MAE 4.621135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 215/2000, Train Loss : 33.181884, Valid Loss 28.556925, MAE 4.073813\n",
      "Epoch : 216/2000, Train Loss : 20.522239, Valid Loss 21.184007, MAE 2.906629\n",
      "Epoch : 217/2000, Train Loss : 14.643721, Valid Loss 21.852645, MAE 3.552240\n",
      "Epoch : 218/2000, Train Loss : 14.966059, Valid Loss 24.367970, MAE 3.828964\n",
      "Epoch : 219/2000, Train Loss : 14.491746, Valid Loss 18.584537, MAE 2.818892\n",
      "Epoch : 220/2000, Train Loss : 13.988866, Valid Loss 17.581133, MAE 2.653990\n",
      "Epoch : 221/2000, Train Loss : 13.812726, Valid Loss 20.129803, MAE 2.728753\n",
      "Epoch : 222/2000, Train Loss : 15.133395, Valid Loss 19.131472, MAE 2.853875\n",
      "Epoch : 223/2000, Train Loss : 13.854306, Valid Loss 17.693787, MAE 2.604475\n",
      "Epoch : 224/2000, Train Loss : 13.821399, Valid Loss 18.144876, MAE 2.987607\n",
      "Epoch : 225/2000, Train Loss : 13.605088, Valid Loss 16.447041, MAE 2.647150\n",
      "Epoch : 226/2000, Train Loss : 14.172397, Valid Loss 21.343350, MAE 3.018484\n",
      "Epoch : 227/2000, Train Loss : 13.688819, Valid Loss 16.050761, MAE 2.637112\n",
      "Epoch : 228/2000, Train Loss : 13.537507, Valid Loss 16.802348, MAE 2.677704\n",
      "Epoch : 229/2000, Train Loss : 12.552276, Valid Loss 16.431638, MAE 2.646776\n",
      "Epoch : 230/2000, Train Loss : 12.802017, Valid Loss 18.103906, MAE 2.689449\n",
      "Epoch : 231/2000, Train Loss : 13.465919, Valid Loss 18.776779, MAE 2.828169\n",
      "Epoch : 232/2000, Train Loss : 12.850540, Valid Loss 17.076394, MAE 2.646700\n",
      "Epoch : 233/2000, Train Loss : 12.395544, Valid Loss 18.066406, MAE 2.638272\n",
      "Epoch : 234/2000, Train Loss : 12.615049, Valid Loss 20.345781, MAE 3.159715\n",
      "Epoch : 235/2000, Train Loss : 13.289833, Valid Loss 18.011208, MAE 2.589220\n",
      "Epoch : 236/2000, Train Loss : 17.244745, Valid Loss 20.670080, MAE 2.977423\n",
      "Epoch : 237/2000, Train Loss : 21.301917, Valid Loss 21.238794, MAE 2.789550\n",
      "Epoch : 238/2000, Train Loss : 23.318325, Valid Loss 25.819960, MAE 3.391181\n",
      "Epoch : 239/2000, Train Loss : 20.819592, Valid Loss 22.947430, MAE 3.285733\n",
      "Epoch : 240/2000, Train Loss : 19.348659, Valid Loss 19.675830, MAE 2.851915\n",
      "Epoch : 241/2000, Train Loss : 15.317290, Valid Loss 18.654791, MAE 2.797251\n",
      "Epoch : 242/2000, Train Loss : 13.714305, Valid Loss 18.559893, MAE 2.836916\n",
      "Epoch : 243/2000, Train Loss : 13.528533, Valid Loss 15.742845, MAE 2.733247\n",
      "Epoch : 244/2000, Train Loss : 13.380203, Valid Loss 19.893611, MAE 2.956292\n",
      "Epoch : 245/2000, Train Loss : 14.760860, Valid Loss 25.439947, MAE 2.878711\n",
      "Epoch : 246/2000, Train Loss : 20.371769, Valid Loss 21.828687, MAE 3.030055\n",
      "Epoch : 247/2000, Train Loss : 14.697502, Valid Loss 16.460970, MAE 2.762595\n",
      "Epoch : 248/2000, Train Loss : 14.415922, Valid Loss 20.351336, MAE 2.961293\n",
      "Epoch : 249/2000, Train Loss : 15.957729, Valid Loss 20.288181, MAE 2.702376\n",
      "Epoch : 250/2000, Train Loss : 15.302706, Valid Loss 29.706402, MAE 3.715537\n",
      "Epoch : 251/2000, Train Loss : 16.627694, Valid Loss 16.081489, MAE 2.669939\n",
      "Epoch : 252/2000, Train Loss : 17.854349, Valid Loss 22.401447, MAE 3.502540\n",
      "Epoch : 253/2000, Train Loss : 13.022286, Valid Loss 15.672107, MAE 2.686918\n",
      "Epoch : 254/2000, Train Loss : 12.396933, Valid Loss 15.953568, MAE 2.721826\n",
      "Epoch : 255/2000, Train Loss : 11.947405, Valid Loss 15.975915, MAE 2.580170\n",
      "Epoch : 256/2000, Train Loss : 11.550791, Valid Loss 16.788132, MAE 2.489054\n",
      "Epoch : 257/2000, Train Loss : 12.541291, Valid Loss 16.995886, MAE 2.989311\n",
      "Epoch : 258/2000, Train Loss : 12.272844, Valid Loss 16.966062, MAE 2.582540\n",
      "Epoch : 259/2000, Train Loss : 12.032540, Valid Loss 18.907874, MAE 2.682962\n",
      "Epoch : 260/2000, Train Loss : 14.555921, Valid Loss 17.869852, MAE 2.888733\n",
      "Epoch : 261/2000, Train Loss : 32.217837, Valid Loss 33.452158, MAE 3.728506\n",
      "Epoch : 262/2000, Train Loss : 29.747155, Valid Loss 26.985327, MAE 3.552335\n",
      "Epoch : 263/2000, Train Loss : 16.325390, Valid Loss 21.508339, MAE 3.009987\n",
      "Epoch : 264/2000, Train Loss : 13.706238, Valid Loss 17.332232, MAE 2.725606\n",
      "Epoch : 265/2000, Train Loss : 13.388203, Valid Loss 19.138638, MAE 2.750641\n",
      "Epoch : 266/2000, Train Loss : 13.771238, Valid Loss 20.188588, MAE 3.124734\n",
      "Epoch : 267/2000, Train Loss : 12.749135, Valid Loss 17.946650, MAE 2.830475\n",
      "Epoch : 268/2000, Train Loss : 12.198150, Valid Loss 16.273717, MAE 2.721355\n",
      "Epoch : 269/2000, Train Loss : 13.846388, Valid Loss 17.632671, MAE 2.488570\n",
      "Epoch : 270/2000, Train Loss : 11.542204, Valid Loss 16.760057, MAE 2.560513\n",
      "Epoch : 271/2000, Train Loss : 11.776181, Valid Loss 16.823010, MAE 2.660541\n",
      "Epoch : 272/2000, Train Loss : 11.570548, Valid Loss 18.362203, MAE 2.435453\n",
      "Epoch : 273/2000, Train Loss : 12.023671, Valid Loss 15.982672, MAE 2.485499\n",
      "Epoch : 274/2000, Train Loss : 11.673867, Valid Loss 18.837902, MAE 2.512666\n",
      "Early stopping\n",
      "Best Result : Epoch 253, Valid Loss 15.672107, MAE 2.686918\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "total_MAE = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, stride_length, inputs_pst = data\n",
    "        inputs_acc, inputs_gyr, stride_length, inputs_pst = inputs_acc.float(), inputs_gyr.float(), stride_length.float(), inputs_pst.float()\n",
    "        inputs_acc, inputs_gyr, inputs_pst = inputs_acc.to(device), inputs_gyr.to(device), inputs_pst.to(device)\n",
    "        stride_length = stride_length.reshape(-1, 1)\n",
    "        stride_length = stride_length.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        outputs = model(inputs_acc, inputs_gyr)\n",
    "#         outputs = torch.sum(dense_output*inputs_pst, axis=1)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = criterion(outputs, stride_length)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step()\n",
    "#         scheduler.step()\n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, stride_length, inputs_pst = data\n",
    "            inputs_acc, inputs_gyr, stride_length, inputs_pst = inputs_acc.float(), inputs_gyr.float(), stride_length.float(), inputs_pst.float()\n",
    "            inputs_acc, inputs_gyr, inputs_pst = inputs_acc.to(device), inputs_gyr.to(device), inputs_pst.to(device)\n",
    "            stride_length = stride_length.reshape(-1, 1)\n",
    "            stride_length = stride_length.to(device)\n",
    "#             print(inputs_pst[0])\n",
    "\n",
    "            # 순전파 \n",
    "            outputs = model(inputs_acc, inputs_gyr)\n",
    "#             outputs = torch.sum(dense_output*inputs_pst, axis=1)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = criterion(outputs, stride_length)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    MAE = torch.sum(torch.abs(outputs - stride_length)) / len(stride_length)\n",
    "    total_MAE += MAE\n",
    "    \n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, MAE {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      MAE))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        best_MAE = MAE\n",
    "        best_epoch = epoch+1\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Best Result : Epoch {}, Valid Loss {:4f}, MAE {:4f}'.format(best_epoch, best, best_MAE))\n",
    "        break\n",
    "    \n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b9ded",
   "metadata": {},
   "source": [
    "# Encoder-based Model : Acc, Gyro 각각 입력, 최종 노드 3개\n",
    "- Encoder로 Conv1d와 LSTM을 활용\n",
    "- 최종적으로 출력된 노드 3개를 distance와 곱"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6169a",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "991de16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "dataset = Gait_Dataset_Salted(file_path)\n",
    "val_percent = 0.2\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e63145f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True,\n",
    "                                           worker_init_fn=np.random.seed(42))\n",
    "val_loader = torch.utils.data.DataLoader(val,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3644dac8",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "251cc838",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "class Encoder_dist(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Encoder_dist, self).__init__()\n",
    "        \n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        \n",
    "        self.conv1d_acc = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim1, 30),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(hidden_dim1, hidden_dim2, 30),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.conv1d_gyr = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim1, 30),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(hidden_dim1, hidden_dim2, 30),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.lstm_acc = nn.LSTM(hidden_dim2, 64, 1, batch_first=True)\n",
    "        self.lstm_gyr = nn.LSTM(hidden_dim2, 64, 1, batch_first=True)\n",
    "            \n",
    "#         self.encoder_pres = nn.Sequential(\n",
    "#             nn.Conv1d(input_dim, hidden_dim1, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv1d(hidden_dim1, hidden_dim2, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv1d(hidden_dim2, hidden_dim3, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Flatten()\n",
    "#         )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(64*2, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 3)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, inputs_acc, inputs_gyr): \n",
    "        \n",
    "        h0 = torch.zeros(1, inputs_acc.size(0), 64).to(device)\n",
    "        c0 = torch.zeros(1, inputs_acc.size(0), 64).to(device)\n",
    "        \n",
    "        conv1d_output_acc = self.conv1d_acc(inputs_acc).transpose(1, 2)\n",
    "        conv1d_output_gyr = self.conv1d_gyr(inputs_gyr).transpose(1, 2)\n",
    "        \n",
    "        _, (enc_output_acc, _) = self.lstm_acc(conv1d_output_acc)\n",
    "        _, (enc_output_gyr, _) = self.lstm_gyr(conv1d_output_gyr)\n",
    "        \n",
    "        enc_output = torch.concat((enc_output_acc[-1], enc_output_gyr[-1]), 1)\n",
    "        dense_output = self.dense(enc_output)\n",
    "        \n",
    "        return dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f724f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "model = Encoder_dist(input_dim, 16, 32).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96ef528c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 17460.751465, Valid Loss 16719.876139, MAE 129.162582\n",
      "Epoch : 2/2000, Train Loss : 14221.272827, Valid Loss 10564.693522, MAE 102.307770\n",
      "Epoch : 3/2000, Train Loss : 5865.228017, Valid Loss 1599.944621, MAE 35.245960\n",
      "Epoch : 4/2000, Train Loss : 900.131142, Valid Loss 898.654449, MAE 26.698446\n",
      "Epoch : 5/2000, Train Loss : 761.088069, Valid Loss 799.394135, MAE 23.304424\n",
      "Epoch : 6/2000, Train Loss : 742.503118, Valid Loss 787.794739, MAE 23.745398\n",
      "Epoch : 7/2000, Train Loss : 734.902962, Valid Loss 788.038706, MAE 23.461910\n",
      "Epoch : 8/2000, Train Loss : 735.844762, Valid Loss 787.044505, MAE 23.668493\n",
      "Epoch : 9/2000, Train Loss : 736.294393, Valid Loss 786.714335, MAE 23.573292\n",
      "Epoch : 10/2000, Train Loss : 738.738070, Valid Loss 786.578227, MAE 23.664757\n",
      "Epoch : 11/2000, Train Loss : 735.070211, Valid Loss 786.644226, MAE 23.488855\n",
      "Epoch : 12/2000, Train Loss : 736.761248, Valid Loss 785.866313, MAE 23.548433\n",
      "Epoch : 13/2000, Train Loss : 734.669731, Valid Loss 786.162028, MAE 23.451153\n",
      "Epoch : 14/2000, Train Loss : 731.729757, Valid Loss 774.495656, MAE 22.833368\n",
      "Epoch : 15/2000, Train Loss : 691.766479, Valid Loss 701.641693, MAE 21.191902\n",
      "Epoch : 16/2000, Train Loss : 670.796137, Valid Loss 647.211884, MAE 20.206379\n",
      "Epoch : 17/2000, Train Loss : 597.786463, Valid Loss 611.975403, MAE 20.371464\n",
      "Epoch : 18/2000, Train Loss : 535.718732, Valid Loss 521.195643, MAE 18.455238\n",
      "Epoch : 19/2000, Train Loss : 506.686540, Valid Loss 475.028163, MAE 17.327751\n",
      "Epoch : 20/2000, Train Loss : 462.537865, Valid Loss 440.609421, MAE 16.752876\n",
      "Epoch : 21/2000, Train Loss : 436.029839, Valid Loss 425.344472, MAE 16.331799\n",
      "Epoch : 22/2000, Train Loss : 402.973866, Valid Loss 404.727702, MAE 16.280313\n",
      "Epoch : 23/2000, Train Loss : 374.805691, Valid Loss 403.220601, MAE 16.415010\n",
      "Epoch : 24/2000, Train Loss : 371.464718, Valid Loss 390.715556, MAE 15.637862\n",
      "Epoch : 25/2000, Train Loss : 329.971166, Valid Loss 329.256475, MAE 14.566054\n",
      "Epoch : 26/2000, Train Loss : 300.590680, Valid Loss 315.578588, MAE 13.554489\n",
      "Epoch : 27/2000, Train Loss : 289.919125, Valid Loss 285.502436, MAE 13.090606\n",
      "Epoch : 28/2000, Train Loss : 270.380919, Valid Loss 275.883092, MAE 12.747248\n",
      "Epoch : 29/2000, Train Loss : 251.139861, Valid Loss 264.884206, MAE 12.601851\n",
      "Epoch : 30/2000, Train Loss : 256.613721, Valid Loss 258.310364, MAE 12.242535\n",
      "Epoch : 31/2000, Train Loss : 233.670345, Valid Loss 248.060852, MAE 11.328637\n",
      "Epoch : 32/2000, Train Loss : 233.673466, Valid Loss 247.941017, MAE 12.231174\n",
      "Epoch : 33/2000, Train Loss : 221.706210, Valid Loss 295.196314, MAE 13.545273\n",
      "Epoch : 34/2000, Train Loss : 237.538065, Valid Loss 230.627942, MAE 11.316173\n",
      "Epoch : 35/2000, Train Loss : 229.939566, Valid Loss 242.644160, MAE 11.535278\n",
      "Epoch : 36/2000, Train Loss : 218.672974, Valid Loss 242.681577, MAE 10.962940\n",
      "Epoch : 37/2000, Train Loss : 221.179593, Valid Loss 235.501785, MAE 11.280511\n",
      "Epoch : 38/2000, Train Loss : 215.129651, Valid Loss 261.086095, MAE 12.835952\n",
      "Epoch : 39/2000, Train Loss : 229.082094, Valid Loss 295.290807, MAE 12.684678\n",
      "Epoch : 40/2000, Train Loss : 239.430001, Valid Loss 234.258733, MAE 10.725030\n",
      "Epoch : 41/2000, Train Loss : 200.391778, Valid Loss 209.564072, MAE 10.294007\n",
      "Epoch : 42/2000, Train Loss : 196.950967, Valid Loss 225.319392, MAE 10.751311\n",
      "Epoch : 43/2000, Train Loss : 241.589560, Valid Loss 267.918432, MAE 13.617471\n",
      "Epoch : 44/2000, Train Loss : 220.447257, Valid Loss 221.743955, MAE 10.360587\n",
      "Epoch : 45/2000, Train Loss : 203.403321, Valid Loss 220.370962, MAE 10.526458\n",
      "Epoch : 46/2000, Train Loss : 194.112334, Valid Loss 221.842570, MAE 11.218406\n",
      "Epoch : 47/2000, Train Loss : 184.620275, Valid Loss 201.881694, MAE 10.364933\n",
      "Epoch : 48/2000, Train Loss : 183.659825, Valid Loss 201.742887, MAE 10.409226\n",
      "Epoch : 49/2000, Train Loss : 185.576077, Valid Loss 200.126104, MAE 10.390362\n",
      "Epoch : 50/2000, Train Loss : 174.339999, Valid Loss 199.181173, MAE 10.738260\n",
      "Epoch : 51/2000, Train Loss : 174.544551, Valid Loss 206.470963, MAE 10.549130\n",
      "Epoch : 52/2000, Train Loss : 173.724428, Valid Loss 203.528460, MAE 10.494397\n",
      "Epoch : 53/2000, Train Loss : 181.163091, Valid Loss 193.120150, MAE 10.113007\n",
      "Epoch : 54/2000, Train Loss : 187.771749, Valid Loss 215.553083, MAE 10.789412\n",
      "Epoch : 55/2000, Train Loss : 174.560963, Valid Loss 202.514982, MAE 10.400371\n",
      "Epoch : 56/2000, Train Loss : 175.702744, Valid Loss 229.060155, MAE 11.257793\n",
      "Epoch : 57/2000, Train Loss : 180.947684, Valid Loss 208.951037, MAE 10.241634\n",
      "Epoch : 58/2000, Train Loss : 184.930246, Valid Loss 193.624863, MAE 10.080265\n",
      "Epoch : 59/2000, Train Loss : 168.938674, Valid Loss 186.513753, MAE 9.400246\n",
      "Epoch : 60/2000, Train Loss : 172.625928, Valid Loss 194.211644, MAE 9.945485\n",
      "Epoch : 61/2000, Train Loss : 175.630032, Valid Loss 193.486542, MAE 10.165221\n",
      "Epoch : 62/2000, Train Loss : 165.545917, Valid Loss 200.539088, MAE 10.579750\n",
      "Epoch : 63/2000, Train Loss : 167.449616, Valid Loss 192.159854, MAE 9.948427\n",
      "Epoch : 64/2000, Train Loss : 169.132697, Valid Loss 193.038963, MAE 9.855893\n",
      "Epoch : 65/2000, Train Loss : 171.252030, Valid Loss 202.754524, MAE 10.698666\n",
      "Epoch : 66/2000, Train Loss : 170.090750, Valid Loss 186.838282, MAE 10.065205\n",
      "Epoch : 67/2000, Train Loss : 169.936093, Valid Loss 201.259265, MAE 10.581098\n",
      "Epoch : 68/2000, Train Loss : 166.679236, Valid Loss 213.815893, MAE 10.690510\n",
      "Epoch : 69/2000, Train Loss : 163.726091, Valid Loss 200.026622, MAE 10.288762\n",
      "Epoch : 70/2000, Train Loss : 175.305864, Valid Loss 193.230118, MAE 10.503557\n",
      "Epoch : 71/2000, Train Loss : 169.473680, Valid Loss 184.858480, MAE 9.156158\n",
      "Epoch : 72/2000, Train Loss : 161.370674, Valid Loss 184.880343, MAE 9.718504\n",
      "Epoch : 73/2000, Train Loss : 162.086407, Valid Loss 190.878349, MAE 9.986703\n",
      "Epoch : 74/2000, Train Loss : 158.775202, Valid Loss 179.799909, MAE 9.471840\n",
      "Epoch : 75/2000, Train Loss : 160.635085, Valid Loss 183.948395, MAE 9.422115\n",
      "Epoch : 76/2000, Train Loss : 164.796139, Valid Loss 191.215645, MAE 10.456352\n",
      "Epoch : 77/2000, Train Loss : 160.860753, Valid Loss 181.987938, MAE 9.432279\n",
      "Epoch : 78/2000, Train Loss : 154.853512, Valid Loss 180.671389, MAE 9.555044\n",
      "Epoch : 79/2000, Train Loss : 154.297427, Valid Loss 176.844676, MAE 9.330918\n",
      "Epoch : 80/2000, Train Loss : 155.269201, Valid Loss 180.448781, MAE 9.294262\n",
      "Epoch : 81/2000, Train Loss : 153.297891, Valid Loss 207.917651, MAE 10.756944\n",
      "Epoch : 82/2000, Train Loss : 168.204717, Valid Loss 199.822950, MAE 10.057251\n",
      "Epoch : 83/2000, Train Loss : 158.700208, Valid Loss 189.348648, MAE 9.968335\n",
      "Epoch : 84/2000, Train Loss : 164.234151, Valid Loss 174.925339, MAE 9.460575\n",
      "Epoch : 85/2000, Train Loss : 155.470407, Valid Loss 171.617704, MAE 9.308561\n",
      "Epoch : 86/2000, Train Loss : 161.282914, Valid Loss 199.910029, MAE 9.755988\n",
      "Epoch : 87/2000, Train Loss : 156.758433, Valid Loss 172.139074, MAE 9.094210\n",
      "Epoch : 88/2000, Train Loss : 152.175282, Valid Loss 176.454647, MAE 9.304105\n",
      "Epoch : 89/2000, Train Loss : 156.595338, Valid Loss 190.270337, MAE 9.598911\n",
      "Epoch : 90/2000, Train Loss : 163.690504, Valid Loss 174.133420, MAE 9.339474\n",
      "Epoch : 91/2000, Train Loss : 154.954250, Valid Loss 183.314107, MAE 9.388285\n",
      "Epoch : 92/2000, Train Loss : 162.610469, Valid Loss 179.904882, MAE 9.538553\n",
      "Epoch : 93/2000, Train Loss : 162.055483, Valid Loss 175.415540, MAE 9.168960\n",
      "Epoch : 94/2000, Train Loss : 157.027786, Valid Loss 183.992439, MAE 9.179412\n",
      "Epoch : 95/2000, Train Loss : 159.312179, Valid Loss 172.296191, MAE 8.708658\n",
      "Epoch : 96/2000, Train Loss : 154.189396, Valid Loss 197.695203, MAE 10.591475\n",
      "Epoch : 97/2000, Train Loss : 164.117565, Valid Loss 173.034299, MAE 9.091542\n",
      "Epoch : 98/2000, Train Loss : 158.374590, Valid Loss 183.304548, MAE 9.448215\n",
      "Epoch : 99/2000, Train Loss : 150.640745, Valid Loss 178.386411, MAE 9.233988\n",
      "Epoch : 100/2000, Train Loss : 149.942782, Valid Loss 178.941744, MAE 9.395531\n",
      "Epoch : 101/2000, Train Loss : 151.088045, Valid Loss 168.223409, MAE 9.174977\n",
      "Epoch : 102/2000, Train Loss : 152.162409, Valid Loss 168.977587, MAE 8.505770\n",
      "Epoch : 103/2000, Train Loss : 167.428039, Valid Loss 208.851705, MAE 10.974604\n",
      "Epoch : 104/2000, Train Loss : 159.788929, Valid Loss 175.977402, MAE 9.206365\n",
      "Epoch : 105/2000, Train Loss : 154.823641, Valid Loss 190.794973, MAE 9.455198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 106/2000, Train Loss : 156.679056, Valid Loss 169.978280, MAE 8.709022\n",
      "Epoch : 107/2000, Train Loss : 147.607361, Valid Loss 175.639844, MAE 9.211037\n",
      "Epoch : 108/2000, Train Loss : 149.510487, Valid Loss 162.048916, MAE 8.495798\n",
      "Epoch : 109/2000, Train Loss : 144.543265, Valid Loss 166.866267, MAE 8.859984\n",
      "Epoch : 110/2000, Train Loss : 153.521392, Valid Loss 182.915988, MAE 9.652942\n",
      "Epoch : 111/2000, Train Loss : 152.356752, Valid Loss 175.490908, MAE 9.091082\n",
      "Epoch : 112/2000, Train Loss : 144.759773, Valid Loss 169.872718, MAE 8.930136\n",
      "Epoch : 113/2000, Train Loss : 148.672163, Valid Loss 193.587769, MAE 9.904086\n",
      "Epoch : 114/2000, Train Loss : 150.392755, Valid Loss 161.121435, MAE 8.685586\n",
      "Epoch : 115/2000, Train Loss : 146.763241, Valid Loss 164.047361, MAE 8.533797\n",
      "Epoch : 116/2000, Train Loss : 144.944880, Valid Loss 182.940127, MAE 9.008857\n",
      "Epoch : 117/2000, Train Loss : 145.613076, Valid Loss 165.693398, MAE 8.613988\n",
      "Epoch : 118/2000, Train Loss : 145.971555, Valid Loss 177.437991, MAE 8.840920\n",
      "Epoch : 119/2000, Train Loss : 143.924331, Valid Loss 160.475138, MAE 8.639440\n",
      "Epoch : 120/2000, Train Loss : 151.830161, Valid Loss 174.206373, MAE 9.098278\n",
      "Epoch : 121/2000, Train Loss : 144.760099, Valid Loss 167.273605, MAE 8.978625\n",
      "Epoch : 122/2000, Train Loss : 148.141541, Valid Loss 161.663254, MAE 8.440760\n",
      "Epoch : 123/2000, Train Loss : 143.544225, Valid Loss 169.710615, MAE 8.982708\n",
      "Epoch : 124/2000, Train Loss : 143.714498, Valid Loss 175.200170, MAE 9.396157\n",
      "Epoch : 125/2000, Train Loss : 146.184832, Valid Loss 182.251905, MAE 8.967326\n",
      "Epoch : 126/2000, Train Loss : 190.396467, Valid Loss 189.169673, MAE 9.982738\n",
      "Epoch : 127/2000, Train Loss : 158.553549, Valid Loss 167.149251, MAE 8.603437\n",
      "Epoch : 128/2000, Train Loss : 148.860386, Valid Loss 180.279549, MAE 9.617338\n",
      "Epoch : 129/2000, Train Loss : 145.167687, Valid Loss 157.418260, MAE 8.242700\n",
      "Epoch : 130/2000, Train Loss : 142.317522, Valid Loss 163.115701, MAE 8.578434\n",
      "Epoch : 131/2000, Train Loss : 136.200505, Valid Loss 171.123314, MAE 9.182117\n",
      "Epoch : 132/2000, Train Loss : 136.392795, Valid Loss 160.099360, MAE 8.408396\n",
      "Epoch : 133/2000, Train Loss : 140.960056, Valid Loss 164.519384, MAE 8.837619\n",
      "Epoch : 134/2000, Train Loss : 133.459514, Valid Loss 158.826942, MAE 8.342384\n",
      "Epoch : 135/2000, Train Loss : 133.381552, Valid Loss 157.410409, MAE 8.258224\n",
      "Epoch : 136/2000, Train Loss : 134.581622, Valid Loss 155.221883, MAE 8.367458\n",
      "Epoch : 137/2000, Train Loss : 133.938369, Valid Loss 165.439082, MAE 8.575574\n",
      "Epoch : 138/2000, Train Loss : 143.289509, Valid Loss 182.360118, MAE 10.219714\n",
      "Epoch : 139/2000, Train Loss : 132.081599, Valid Loss 169.224085, MAE 8.281780\n",
      "Epoch : 140/2000, Train Loss : 138.865411, Valid Loss 161.027865, MAE 8.733639\n",
      "Epoch : 141/2000, Train Loss : 142.655294, Valid Loss 158.073589, MAE 8.784956\n",
      "Epoch : 142/2000, Train Loss : 135.820884, Valid Loss 158.563527, MAE 8.939309\n",
      "Epoch : 143/2000, Train Loss : 136.608482, Valid Loss 154.585350, MAE 7.933147\n",
      "Epoch : 144/2000, Train Loss : 142.098053, Valid Loss 188.719515, MAE 8.930881\n",
      "Epoch : 145/2000, Train Loss : 138.595756, Valid Loss 155.409517, MAE 8.673482\n",
      "Epoch : 146/2000, Train Loss : 130.891011, Valid Loss 146.544961, MAE 7.888692\n",
      "Epoch : 147/2000, Train Loss : 121.642459, Valid Loss 156.514778, MAE 8.685895\n",
      "Epoch : 148/2000, Train Loss : 135.241268, Valid Loss 165.619382, MAE 8.853691\n",
      "Epoch : 149/2000, Train Loss : 136.283150, Valid Loss 187.491254, MAE 9.351877\n",
      "Epoch : 150/2000, Train Loss : 155.507197, Valid Loss 160.673203, MAE 8.221625\n",
      "Epoch : 151/2000, Train Loss : 137.663912, Valid Loss 179.819865, MAE 9.745296\n",
      "Epoch : 152/2000, Train Loss : 139.083282, Valid Loss 158.928131, MAE 8.469002\n",
      "Epoch : 153/2000, Train Loss : 128.968186, Valid Loss 160.454456, MAE 8.842619\n",
      "Epoch : 154/2000, Train Loss : 122.956469, Valid Loss 159.296975, MAE 8.791097\n",
      "Epoch : 155/2000, Train Loss : 121.991834, Valid Loss 147.849391, MAE 8.385583\n",
      "Epoch : 156/2000, Train Loss : 125.743371, Valid Loss 158.441027, MAE 8.749459\n",
      "Epoch : 157/2000, Train Loss : 123.027620, Valid Loss 143.212391, MAE 7.623175\n",
      "Epoch : 158/2000, Train Loss : 121.821921, Valid Loss 153.779235, MAE 8.442679\n",
      "Epoch : 159/2000, Train Loss : 121.804712, Valid Loss 153.833682, MAE 8.605245\n",
      "Epoch : 160/2000, Train Loss : 119.952218, Valid Loss 148.349789, MAE 8.480639\n",
      "Epoch : 161/2000, Train Loss : 123.293314, Valid Loss 149.003904, MAE 8.302997\n",
      "Epoch : 162/2000, Train Loss : 120.742230, Valid Loss 166.688161, MAE 8.975267\n",
      "Epoch : 163/2000, Train Loss : 123.744895, Valid Loss 156.900838, MAE 8.947721\n",
      "Epoch : 164/2000, Train Loss : 120.082102, Valid Loss 152.715378, MAE 8.848376\n",
      "Epoch : 165/2000, Train Loss : 119.535138, Valid Loss 147.079196, MAE 8.247128\n",
      "Epoch : 166/2000, Train Loss : 126.194555, Valid Loss 160.637365, MAE 8.995162\n",
      "Epoch : 167/2000, Train Loss : 124.582237, Valid Loss 141.440008, MAE 7.758291\n",
      "Epoch : 168/2000, Train Loss : 118.237008, Valid Loss 139.353230, MAE 7.775611\n",
      "Epoch : 169/2000, Train Loss : 112.268722, Valid Loss 143.415367, MAE 8.015035\n",
      "Epoch : 170/2000, Train Loss : 116.938260, Valid Loss 152.279434, MAE 8.757764\n",
      "Epoch : 171/2000, Train Loss : 117.237341, Valid Loss 141.792280, MAE 7.772770\n",
      "Epoch : 172/2000, Train Loss : 114.377854, Valid Loss 145.700174, MAE 8.298170\n",
      "Epoch : 173/2000, Train Loss : 117.639578, Valid Loss 154.174273, MAE 8.759280\n",
      "Epoch : 174/2000, Train Loss : 119.309096, Valid Loss 142.055794, MAE 8.269547\n",
      "Epoch : 175/2000, Train Loss : 114.805961, Valid Loss 144.046190, MAE 7.436688\n",
      "Epoch : 176/2000, Train Loss : 130.970926, Valid Loss 151.193548, MAE 8.290923\n",
      "Epoch : 177/2000, Train Loss : 126.410481, Valid Loss 200.851674, MAE 10.137654\n",
      "Epoch : 178/2000, Train Loss : 157.400868, Valid Loss 160.871707, MAE 9.364100\n",
      "Epoch : 179/2000, Train Loss : 142.445603, Valid Loss 160.395805, MAE 8.947417\n",
      "Epoch : 180/2000, Train Loss : 135.503420, Valid Loss 155.399307, MAE 8.843641\n",
      "Epoch : 181/2000, Train Loss : 130.034496, Valid Loss 154.080752, MAE 8.465432\n",
      "Epoch : 182/2000, Train Loss : 126.094445, Valid Loss 153.007153, MAE 8.115666\n",
      "Epoch : 183/2000, Train Loss : 119.901661, Valid Loss 171.448888, MAE 9.523778\n",
      "Epoch : 184/2000, Train Loss : 123.350742, Valid Loss 151.116845, MAE 8.242043\n",
      "Epoch : 185/2000, Train Loss : 129.251236, Valid Loss 157.390111, MAE 8.529049\n",
      "Epoch : 186/2000, Train Loss : 123.425873, Valid Loss 154.407537, MAE 8.564196\n",
      "Epoch : 187/2000, Train Loss : 128.321017, Valid Loss 142.326914, MAE 7.893101\n",
      "Epoch : 188/2000, Train Loss : 112.502817, Valid Loss 148.988199, MAE 7.932375\n",
      "Epoch : 189/2000, Train Loss : 117.119656, Valid Loss 139.858078, MAE 7.849949\n",
      "Early stopping\n",
      "Best Result : Epoch 168, Valid Loss 139.353230, MAE 7.775611\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "best_MAE = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_acc, inputs_gyr, stride_length, inputs_pst = data\n",
    "        inputs_acc, inputs_gyr, stride_length, inputs_pst = inputs_acc.float(), inputs_gyr.float(), stride_length.float(), inputs_pst.float()\n",
    "        inputs_acc, inputs_gyr, inputs_pst = inputs_acc.to(device), inputs_gyr.to(device), inputs_pst.to(device)\n",
    "        stride_length = stride_length.reshape(-1, 1)\n",
    "        stride_length = stride_length.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        dense_outputs = model(inputs_acc, inputs_gyr)\n",
    "        outputs = torch.unsqueeze(torch.sum(dense_outputs*inputs_pst, axis=1), 1)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = criterion(outputs, stride_length)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_acc, inputs_gyr, stride_length, inputs_pst = data\n",
    "            inputs_acc, inputs_gyr, stride_length, inputs_pst = inputs_acc.float(), inputs_gyr.float(), stride_length.float(), inputs_pst.float()\n",
    "            inputs_acc, inputs_gyr, inputs_pst = inputs_acc.to(device), inputs_gyr.to(device), inputs_pst.to(device)\n",
    "            stride_length = stride_length.reshape(-1, 1)\n",
    "            stride_length = stride_length.to(device)\n",
    "\n",
    "\n",
    "            dense_outputs = model(inputs_acc, inputs_gyr)\n",
    "            outputs = torch.unsqueeze(torch.sum(dense_outputs*inputs_pst, axis=1), 1)\n",
    "\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = criterion(outputs, stride_length)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    MAE = torch.sum(torch.abs(outputs - stride_length)) / len(stride_length)\n",
    "    \n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, MAE {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      MAE))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        best_MAE = MAE\n",
    "        best_epoch = epoch+1\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Best Result : Epoch {}, Valid Loss {:4f}, MAE {:4f}'.format(best_epoch, best, best_MAE))\n",
    "        break\n",
    "    \n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3593a90b",
   "metadata": {},
   "source": [
    "# Encoder-based Model : Acc, Gyro 축별 입력\n",
    "- 각 축 Acc에서 얻어진 축별 distance를 곱해주기 때문에 축별로 데이터를 입력\n",
    "    - 축별 Acc/Gyro의 정보가 알맞은 축의 distance와 곱해져야 한다는 생각에 시도\n",
    "- 3개의 인코더에는 각각 (Acc_x, Gyro_x) / (Acc_y, Gyro_y) / (Acc_z, Gyro_z)가 입력으로 들어감\n",
    "    - 인코더의 각 output을 concat한 뒤 FC-Layer에 넣었을 때, 축별 정보가 순서대로 보존될 수 있을지는 의문 부호 \n",
    "- Pressure는 축이 따로 없으므로 고려하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b771bc2",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9fd69e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"D:\\gait_dataset/salted/*\"\n",
    "dataset = Gait_Dataset_Axis_Salted(file_path)\n",
    "val_percent = 0.2\n",
    "n_val = int(len(dataset) * val_percent)\n",
    "n_train = len(dataset) - n_val\n",
    "train, val = random_split(dataset, [n_train, n_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fbf8c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train,\n",
    "                                           batch_size=128,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val,\n",
    "                                         batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b7a840",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4f5a4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "class Encoder_axis(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim1, hidden_dim2):\n",
    "        super(Encoder_axis, self).__init__()\n",
    "        \n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        \n",
    "        self.conv1d_x = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim1, 30),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(hidden_dim1, hidden_dim2, 30),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_y = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim1, 30),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(hidden_dim1, hidden_dim2, 30),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.conv1d_z = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, hidden_dim1, 30),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(hidden_dim1, hidden_dim2, 30),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.lstm_x = nn.LSTM(hidden_dim2, 64, 1, batch_first=True)\n",
    "        self.lstm_y = nn.LSTM(hidden_dim2, 64, 1, batch_first=True)\n",
    "        self.lstm_z = nn.LSTM(hidden_dim2, 64, 1, batch_first=True)\n",
    "            \n",
    "#         self.encoder_pres = nn.Sequential(\n",
    "#             nn.Conv1d(input_dim, hidden_dim1, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv1d(hidden_dim1, hidden_dim2, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Conv1d(hidden_dim2, hidden_dim3, 7),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Flatten()\n",
    "#         )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(64*3, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 3)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, inputs_x, inputs_y, inputs_z): \n",
    "        \n",
    "        h0 = torch.zeros(1, inputs_x.size(0), 64).to(device)\n",
    "        c0 = torch.zeros(1, inputs_x.size(0), 64).to(device)\n",
    "        \n",
    "        conv1d_output_x = self.conv1d_x(inputs_x).transpose(1, 2)\n",
    "        conv1d_output_y = self.conv1d_y(inputs_y).transpose(1, 2)\n",
    "        conv1d_output_z = self.conv1d_z(inputs_z).transpose(1, 2)\n",
    "        \n",
    "        _, (enc_output_x, _) = self.lstm_x(conv1d_output_x)\n",
    "        _, (enc_output_y, _) = self.lstm_y(conv1d_output_y)\n",
    "        _, (enc_output_z, _) = self.lstm_z(conv1d_output_z)\n",
    "        \n",
    "        enc_output = torch.concat((enc_output_x[-1], enc_output_y[-1], enc_output_z[-1]), 1)\n",
    "        dense_output = self.dense(enc_output)\n",
    "        \n",
    "        return dense_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8209fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2\n",
    "model = Encoder_axis(input_dim, 16, 32).to(device)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "n_epochs = 2000\n",
    "\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98bbbae7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1/2000, Train Loss : 4131.602580, Valid Loss 741.794952, MAE 23.011929\n",
      "Epoch : 2/2000, Train Loss : 887.732760, Valid Loss 725.486023, MAE 23.126055\n",
      "Epoch : 3/2000, Train Loss : 773.051751, Valid Loss 725.536906, MAE 22.928444\n",
      "Epoch : 4/2000, Train Loss : 766.594653, Valid Loss 723.411326, MAE 22.973690\n",
      "Epoch : 5/2000, Train Loss : 762.594986, Valid Loss 723.033732, MAE 22.952337\n",
      "Epoch : 6/2000, Train Loss : 762.331067, Valid Loss 721.919210, MAE 22.980139\n",
      "Epoch : 7/2000, Train Loss : 761.208069, Valid Loss 721.050598, MAE 22.843424\n",
      "Epoch : 8/2000, Train Loss : 757.525235, Valid Loss 717.445404, MAE 22.873594\n",
      "Epoch : 9/2000, Train Loss : 754.095818, Valid Loss 713.444438, MAE 22.760241\n",
      "Epoch : 10/2000, Train Loss : 745.656751, Valid Loss 705.566956, MAE 22.476133\n",
      "Epoch : 11/2000, Train Loss : 718.493685, Valid Loss 632.381968, MAE 20.897116\n",
      "Epoch : 12/2000, Train Loss : 628.830975, Valid Loss 560.911957, MAE 19.358414\n",
      "Epoch : 13/2000, Train Loss : 591.140091, Valid Loss 545.822561, MAE 19.141661\n",
      "Epoch : 14/2000, Train Loss : 573.997571, Valid Loss 533.986928, MAE 19.077751\n",
      "Epoch : 15/2000, Train Loss : 560.160114, Valid Loss 516.135030, MAE 18.791765\n",
      "Epoch : 16/2000, Train Loss : 545.105420, Valid Loss 522.001500, MAE 19.073931\n",
      "Epoch : 17/2000, Train Loss : 533.199595, Valid Loss 498.173874, MAE 18.763247\n",
      "Epoch : 18/2000, Train Loss : 511.462070, Valid Loss 470.724218, MAE 18.203796\n",
      "Epoch : 19/2000, Train Loss : 502.442074, Valid Loss 463.039836, MAE 17.807646\n",
      "Epoch : 20/2000, Train Loss : 494.436284, Valid Loss 453.088074, MAE 17.735880\n",
      "Epoch : 21/2000, Train Loss : 464.996616, Valid Loss 440.811213, MAE 17.502821\n",
      "Epoch : 22/2000, Train Loss : 455.337093, Valid Loss 426.240341, MAE 17.090014\n",
      "Epoch : 23/2000, Train Loss : 433.465037, Valid Loss 397.126780, MAE 16.190063\n",
      "Epoch : 24/2000, Train Loss : 401.754308, Valid Loss 369.774277, MAE 15.550318\n",
      "Epoch : 25/2000, Train Loss : 358.245625, Valid Loss 332.907242, MAE 14.473349\n",
      "Epoch : 26/2000, Train Loss : 329.592194, Valid Loss 326.745488, MAE 13.947560\n",
      "Epoch : 27/2000, Train Loss : 325.183490, Valid Loss 316.166128, MAE 13.929037\n",
      "Epoch : 28/2000, Train Loss : 320.334333, Valid Loss 306.125982, MAE 13.600462\n",
      "Epoch : 29/2000, Train Loss : 283.102305, Valid Loss 291.341527, MAE 13.000865\n",
      "Epoch : 30/2000, Train Loss : 285.799832, Valid Loss 266.696236, MAE 12.736745\n",
      "Epoch : 31/2000, Train Loss : 261.954776, Valid Loss 295.357712, MAE 13.246864\n",
      "Epoch : 32/2000, Train Loss : 267.836897, Valid Loss 254.632851, MAE 11.915864\n",
      "Epoch : 33/2000, Train Loss : 252.781775, Valid Loss 278.975800, MAE 12.040856\n",
      "Epoch : 34/2000, Train Loss : 240.290565, Valid Loss 239.930923, MAE 11.271182\n",
      "Epoch : 35/2000, Train Loss : 226.645832, Valid Loss 237.715670, MAE 11.552093\n",
      "Epoch : 36/2000, Train Loss : 214.077539, Valid Loss 223.448583, MAE 10.977788\n",
      "Epoch : 37/2000, Train Loss : 208.125270, Valid Loss 235.357681, MAE 11.098869\n",
      "Epoch : 38/2000, Train Loss : 215.114234, Valid Loss 229.348488, MAE 10.974526\n",
      "Epoch : 39/2000, Train Loss : 216.794022, Valid Loss 271.284004, MAE 12.771977\n",
      "Epoch : 40/2000, Train Loss : 224.411565, Valid Loss 235.255030, MAE 11.134134\n",
      "Epoch : 41/2000, Train Loss : 206.541171, Valid Loss 212.453672, MAE 10.613477\n",
      "Epoch : 42/2000, Train Loss : 197.280170, Valid Loss 215.232559, MAE 10.672149\n",
      "Epoch : 43/2000, Train Loss : 199.340418, Valid Loss 210.233281, MAE 10.492390\n",
      "Epoch : 44/2000, Train Loss : 199.770296, Valid Loss 205.839533, MAE 10.361078\n",
      "Epoch : 45/2000, Train Loss : 193.387389, Valid Loss 203.687576, MAE 10.104948\n",
      "Epoch : 46/2000, Train Loss : 195.354699, Valid Loss 211.835686, MAE 10.311093\n",
      "Epoch : 47/2000, Train Loss : 190.804905, Valid Loss 204.920504, MAE 10.251626\n",
      "Epoch : 48/2000, Train Loss : 184.427003, Valid Loss 218.694461, MAE 10.108003\n",
      "Epoch : 49/2000, Train Loss : 185.644531, Valid Loss 200.731341, MAE 10.233494\n",
      "Epoch : 50/2000, Train Loss : 185.212308, Valid Loss 203.202405, MAE 9.702999\n",
      "Epoch : 51/2000, Train Loss : 177.228007, Valid Loss 195.765795, MAE 9.826078\n",
      "Epoch : 52/2000, Train Loss : 175.901869, Valid Loss 198.615990, MAE 9.671559\n",
      "Epoch : 53/2000, Train Loss : 175.702037, Valid Loss 190.936694, MAE 9.402801\n",
      "Epoch : 54/2000, Train Loss : 178.938359, Valid Loss 198.250038, MAE 9.841783\n",
      "Epoch : 55/2000, Train Loss : 175.548910, Valid Loss 191.058235, MAE 9.501348\n",
      "Epoch : 56/2000, Train Loss : 173.786911, Valid Loss 200.300527, MAE 9.696366\n",
      "Epoch : 57/2000, Train Loss : 177.509054, Valid Loss 198.207494, MAE 10.246001\n",
      "Epoch : 58/2000, Train Loss : 174.676118, Valid Loss 208.684731, MAE 9.980884\n",
      "Epoch : 59/2000, Train Loss : 187.740085, Valid Loss 201.711456, MAE 9.490733\n",
      "Epoch : 60/2000, Train Loss : 189.976069, Valid Loss 196.284551, MAE 9.720406\n",
      "Epoch : 61/2000, Train Loss : 181.169766, Valid Loss 204.883837, MAE 10.696767\n",
      "Epoch : 62/2000, Train Loss : 173.816325, Valid Loss 193.064046, MAE 10.012875\n",
      "Epoch : 63/2000, Train Loss : 185.731800, Valid Loss 191.468430, MAE 9.542652\n",
      "Epoch : 64/2000, Train Loss : 184.151222, Valid Loss 195.423548, MAE 9.520368\n",
      "Epoch : 65/2000, Train Loss : 168.765003, Valid Loss 192.321416, MAE 9.799798\n",
      "Epoch : 66/2000, Train Loss : 168.280084, Valid Loss 207.857315, MAE 9.958804\n",
      "Epoch : 67/2000, Train Loss : 165.988450, Valid Loss 189.989171, MAE 9.634377\n",
      "Epoch : 68/2000, Train Loss : 166.388936, Valid Loss 186.586377, MAE 9.362663\n",
      "Epoch : 69/2000, Train Loss : 161.661865, Valid Loss 183.238764, MAE 9.279223\n",
      "Epoch : 70/2000, Train Loss : 166.322388, Valid Loss 194.502879, MAE 10.047777\n",
      "Epoch : 71/2000, Train Loss : 162.113792, Valid Loss 185.241684, MAE 9.877527\n",
      "Epoch : 72/2000, Train Loss : 160.102876, Valid Loss 185.469404, MAE 9.281293\n",
      "Epoch : 73/2000, Train Loss : 166.173229, Valid Loss 193.699331, MAE 9.846877\n",
      "Epoch : 74/2000, Train Loss : 161.684515, Valid Loss 198.678624, MAE 10.481052\n",
      "Epoch : 75/2000, Train Loss : 163.782990, Valid Loss 194.162196, MAE 9.616310\n",
      "Epoch : 76/2000, Train Loss : 163.015339, Valid Loss 198.150912, MAE 9.991770\n",
      "Epoch : 77/2000, Train Loss : 191.060867, Valid Loss 210.506533, MAE 10.080523\n",
      "Epoch : 78/2000, Train Loss : 165.820625, Valid Loss 189.451886, MAE 9.440991\n",
      "Epoch : 79/2000, Train Loss : 158.724266, Valid Loss 192.898771, MAE 9.318307\n",
      "Epoch : 80/2000, Train Loss : 158.977748, Valid Loss 193.507258, MAE 9.434259\n",
      "Epoch : 81/2000, Train Loss : 157.035392, Valid Loss 199.624914, MAE 9.961713\n",
      "Epoch : 82/2000, Train Loss : 159.689500, Valid Loss 190.077782, MAE 9.169029\n",
      "Epoch : 83/2000, Train Loss : 159.104936, Valid Loss 183.243726, MAE 9.549264\n",
      "Epoch : 84/2000, Train Loss : 155.999195, Valid Loss 188.006310, MAE 9.227431\n",
      "Epoch : 85/2000, Train Loss : 156.261587, Valid Loss 206.942645, MAE 10.094322\n",
      "Epoch : 86/2000, Train Loss : 158.770096, Valid Loss 191.366903, MAE 9.760234\n",
      "Epoch : 87/2000, Train Loss : 156.901179, Valid Loss 208.465296, MAE 10.442854\n",
      "Epoch : 88/2000, Train Loss : 169.438921, Valid Loss 189.145716, MAE 9.720044\n",
      "Epoch : 89/2000, Train Loss : 157.888394, Valid Loss 181.986829, MAE 9.400134\n",
      "Epoch : 90/2000, Train Loss : 157.244849, Valid Loss 189.871974, MAE 9.460645\n",
      "Epoch : 91/2000, Train Loss : 159.044195, Valid Loss 182.873728, MAE 9.393564\n",
      "Epoch : 92/2000, Train Loss : 153.650534, Valid Loss 187.034386, MAE 9.219815\n",
      "Epoch : 93/2000, Train Loss : 150.611129, Valid Loss 192.470988, MAE 9.796106\n",
      "Epoch : 94/2000, Train Loss : 156.380582, Valid Loss 177.825783, MAE 9.384226\n",
      "Epoch : 95/2000, Train Loss : 156.115055, Valid Loss 187.058205, MAE 9.292244\n",
      "Epoch : 96/2000, Train Loss : 152.138772, Valid Loss 175.642845, MAE 9.284015\n",
      "Epoch : 97/2000, Train Loss : 147.353924, Valid Loss 199.127050, MAE 9.348176\n",
      "Epoch : 98/2000, Train Loss : 156.449074, Valid Loss 179.253639, MAE 9.048783\n",
      "Epoch : 99/2000, Train Loss : 157.119543, Valid Loss 179.694857, MAE 8.941836\n",
      "Epoch : 100/2000, Train Loss : 154.533844, Valid Loss 180.652037, MAE 9.179641\n",
      "Epoch : 101/2000, Train Loss : 148.906507, Valid Loss 188.013034, MAE 9.360993\n",
      "Epoch : 102/2000, Train Loss : 147.106034, Valid Loss 188.481794, MAE 9.370914\n",
      "Epoch : 103/2000, Train Loss : 154.506652, Valid Loss 182.257973, MAE 9.188811\n",
      "Epoch : 104/2000, Train Loss : 158.366247, Valid Loss 180.893735, MAE 9.453322\n",
      "Epoch : 105/2000, Train Loss : 151.889187, Valid Loss 176.408150, MAE 8.922361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 106/2000, Train Loss : 148.757949, Valid Loss 190.447118, MAE 9.697701\n",
      "Epoch : 107/2000, Train Loss : 149.378489, Valid Loss 189.861509, MAE 9.328796\n",
      "Epoch : 108/2000, Train Loss : 148.244735, Valid Loss 175.736460, MAE 8.651345\n",
      "Epoch : 109/2000, Train Loss : 145.258740, Valid Loss 180.337639, MAE 9.264716\n",
      "Epoch : 110/2000, Train Loss : 155.514221, Valid Loss 193.945305, MAE 9.586026\n",
      "Epoch : 111/2000, Train Loss : 162.675944, Valid Loss 184.505712, MAE 9.419845\n",
      "Epoch : 112/2000, Train Loss : 157.697038, Valid Loss 189.047801, MAE 10.002328\n",
      "Epoch : 113/2000, Train Loss : 151.131913, Valid Loss 179.594976, MAE 9.392625\n",
      "Epoch : 114/2000, Train Loss : 147.598701, Valid Loss 176.820119, MAE 9.152346\n",
      "Epoch : 115/2000, Train Loss : 149.711472, Valid Loss 181.075414, MAE 9.425193\n",
      "Epoch : 116/2000, Train Loss : 145.712959, Valid Loss 184.234123, MAE 9.362118\n",
      "Epoch : 117/2000, Train Loss : 143.898503, Valid Loss 182.724854, MAE 8.973931\n",
      "Early stopping\n",
      "Best Result : Epoch 96, Valid Loss 175.642845, MAE 9.284015\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping을 위한 변수\n",
    "best = 1000\n",
    "converge_cnt = 0\n",
    "best_MAE = 0\n",
    "best_epoch = 0\n",
    "\n",
    "# Run Training loop\n",
    "for epoch in range(0, n_epochs) :\n",
    "    # Set current loss value \n",
    "    tot_trn_loss = 0.0\n",
    "    \n",
    "    # Train Mode\n",
    "    model.train()\n",
    "    \n",
    "    # Iterate over the DataLoader for training data \n",
    "    for i, data in enumerate(train_loader) :\n",
    "        inputs_x, inputs_y, inputs_z, stride_length, inputs_pst = data\n",
    "        inputs_x, inputs_y, inputs_z, stride_length, inputs_pst = inputs_x.float(), inputs_y.float(), inputs_z.float(), stride_length.float(), inputs_pst.float()\n",
    "        inputs_x, inputs_y, inputs_z, inputs_pst = inputs_x.to(device), inputs_y.to(device), inputs_z.to(device), inputs_pst.to(device)\n",
    "        stride_length = stride_length.reshape(-1, 1)\n",
    "        stride_length = stride_length.to(device)\n",
    "\n",
    "        # 순전파 \n",
    "        dense_outputs = model(inputs_x, inputs_y, inputs_z)\n",
    "        outputs = torch.unsqueeze(torch.sum(dense_outputs*inputs_pst, axis=1), 1)\n",
    "        \n",
    "        # Loss 계산\n",
    "        loss = criterion(outputs, stride_length)\n",
    "        \n",
    "        # Zero the gradients \n",
    "        optimizer.zero_grad()\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "        # Perform optimization \n",
    "        optimizer.step() \n",
    "        \n",
    "        # Print statistics\n",
    "        tot_trn_loss += loss.item()\n",
    "        \n",
    "    # Evaluation Mode\n",
    "    model.eval()\n",
    "    \n",
    "    tot_val_loss = 0\n",
    "    val_epoch_loss = []\n",
    "\n",
    "    with torch.no_grad() :\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs_x, inputs_y, inputs_z, stride_length, inputs_pst = data\n",
    "            inputs_x, inputs_y, inputs_z, stride_length, inputs_pst = inputs_x.float(), inputs_y.float(), inputs_z.float(), stride_length.float(), inputs_pst.float()\n",
    "            inputs_x, inputs_y, inputs_z, inputs_pst = inputs_x.to(device), inputs_y.to(device), inputs_z.to(device), inputs_pst.to(device)\n",
    "            stride_length = stride_length.reshape(-1, 1)\n",
    "            stride_length = stride_length.to(device)\n",
    "\n",
    "            # 순전파 \n",
    "            dense_outputs = model(inputs_x, inputs_y, inputs_z)\n",
    "            outputs = torch.unsqueeze(torch.sum(dense_outputs*inputs_pst, axis=1), 1)\n",
    "            \n",
    "            # Loss 계산\n",
    "            loss = criterion(outputs, stride_length)\n",
    "            tot_val_loss += loss.item()            \n",
    "            \n",
    "\n",
    "    # Epoch 별 Loss\n",
    "    trn_loss = tot_trn_loss / len(train_loader)\n",
    "    val_loss = tot_val_loss / len(val_loader)\n",
    "    MAE = torch.sum(torch.abs(outputs - stride_length)) / len(stride_length)\n",
    "    \n",
    "    \n",
    "    print(\"Epoch : {}/{}, Train Loss : {:.6f}, Valid Loss {:.6f}, MAE {:.6f}\".format(epoch+1, n_epochs,\n",
    "                                                                                       trn_loss, val_loss,\n",
    "                                                                                      MAE))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if val_loss < best:\n",
    "        best = np.mean(val_loss)\n",
    "        best_MAE = MAE\n",
    "        best_epoch = epoch+1\n",
    "        converge_cnt = 0\n",
    "    else:\n",
    "        converge_cnt += 1\n",
    "    \n",
    "    if converge_cnt > 20:\n",
    "        print('Early stopping')\n",
    "        print('Best Result : Epoch {}, Valid Loss {:4f}, MAE {:4f}'.format(best_epoch, best, best_MAE))\n",
    "        break\n",
    "    \n",
    "#     print(\"Epoch : {}/{} Epoch Loss : {:.6f}\".format(epoch+1, n_epochs, current_loss / len(trainloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea908b3",
   "metadata": {},
   "source": [
    "# Encoder-based Model : Acc, Gyro 축별 입력\n",
    "- 각 축 Acc에서 얻어진 축별 distance를 곱해주기 때문에 축별로 데이터를 입력\n",
    "    - 축별 Acc/Gyro의 정보가 알맞은 축의 distance와 곱해져야 한다는 생각에 시도\n",
    "- 3개의 인코더에는 각각 (Acc_x, Gyro_x) / (Acc_y, Gyro_y) / (Acc_z, Gyro_z)가 입력으로 들어감\n",
    "    - 인코더의 각 output을 concat한 뒤 FC-Layer에 넣었을 때, 축별 정보가 순서대로 보존될 수 있을지는 의문 부호 \n",
    "- Pressure는 축이 따로 없으므로 고려하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda52b2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gait",
   "language": "python",
   "name": "gait"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
